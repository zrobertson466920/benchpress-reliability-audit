{
  "data_discovery": {
    "raw_schema": "JSON with keys: models (list of 83 dicts: id/name/provider/release_date/params_total_M/params_active_M/architecture/is_reasoning/open_weights), benchmarks (list of 49 dicts: id/name/category/metric/num_problems/source_url), scores (list of 1390 dicts: model_id/benchmark_id/score/reference_url), generated (timestamp).",
    "extraction_decisions": "Built 83x49 matrix from scores. Averaged 15 duplicate (model_id, benchmark_id) pairs (all DeepSeek-R1-distill variants with 2 entries each). All 1390 scores non-null numeric. No schema ambiguities.",
    "n_models_raw": 83,
    "n_benchmarks_raw": 49
  },
  "data": {
    "n_models": 80,
    "n_benchmarks": 35,
    "missing_fraction": 0.5425,
    "preprocessing": "Filtered benchmarks to >=10 model scores (49->35), then models to >=5 benchmark scores (83->80). Min-max normalized per benchmark to [0,100]. Iterative imputation (MICE, max_iter=15, random_state=42, bounded [0,100]). Imputed matrix used for SVD and benchmark selection; raw observed values used for canonical prediction.",
    "benchmarks_used": [
      "GPQA Diamond",
      "AIME 2025",
      "MMLU",
      "MMLU-Pro",
      "SWE-bench Verified",
      "MATH-500",
      "LiveCodeBench",
      "FrontierMath",
      "HLE (Humanity's Last Exam)",
      "ARC-AGI-2",
      "BrowseComp",
      "SimpleQA",
      "IFEval",
      "HumanEval",
      "Codeforces Rating",
      "OSWorld",
      "MMMU",
      "MMMU-Pro",
      "Arena-Hard Auto",
      "Chatbot Arena Elo",
      "SWE-bench Pro",
      "AIME 2024",
      "HMMT Feb 2025",
      "Tau-Bench Retail",
      "CritPt",
      "GSM8K",
      "Terminal-Bench 2.0",
      "Terminal-Bench 1.0",
      "ARC-AGI-1",
      "BRUMO 2025",
      "SMT 2025",
      "HMMT Nov 2025",
      "CMIMC 2025",
      "MathArena Apex 2025",
      "LiveBench"
    ]
  },
  "rank_analysis": {
    "method": "SVD on mean-centered min-max-normalized imputed filtered matrix",
    "effective_rank": 7,
    "variance_explained_by_rank": 0.9166,
    "singular_values": [
      1063.7785,
      536.0874,
      403.0742,
      311.6387,
      291.5264,
      221.0051,
      205.3408,
      190.8975,
      167.3407,
      155.0049,
      139.7416,
      121.8244,
      97.7309,
      86.6072,
      85.277,
      76.2657,
      61.0037,
      57.9987,
      53.4609,
      41.1763
    ],
    "justification": "90% cumulative variance threshold yields effective rank 7. First singular value (1063.8) dominates (vs 536.1 for second), indicating strong general-capability factor. 95% threshold gives rank 10."
  },
  "benchmark_selection": {
    "method": "greedy_forward_selection",
    "selected_benchmarks": [
      "GPQA Diamond",
      "HMMT Nov 2025",
      "GSM8K",
      "Tau-Bench Retail",
      "HMMT Feb 2025",
      "FrontierMath",
      "SMT 2025"
    ],
    "n_selected": 7,
    "selection_criterion": "Minimize LOO MAE (hat-matrix trick) on remaining benchmarks using ridge regression (alpha=10) on min-max normalized imputed matrix."
  },
  "prediction": {
    "method": "ridge_regression_from_selected_subset",
    "overall_mae": 9.8413,
    "per_benchmark_mae": {
      "AIME 2025": 6.6112,
      "MMLU": 7.8394,
      "MMLU-Pro": 7.2312,
      "SWE-bench Verified": 10.9356,
      "MATH-500": 9.3935,
      "LiveCodeBench": 6.7855,
      "HLE (Humanity's Last Exam)": 10.3119,
      "ARC-AGI-2": 10.3258,
      "BrowseComp": 9.1512,
      "SimpleQA": 11.3999,
      "IFEval": 8.077,
      "HumanEval": 7.3198,
      "Codeforces Rating": 8.8838,
      "OSWorld": 14.7773,
      "MMMU": 12.0038,
      "MMMU-Pro": 14.241,
      "Arena-Hard Auto": 15.4578,
      "Chatbot Arena Elo": 8.4118,
      "SWE-bench Pro": 9.955,
      "AIME 2024": 8.2677,
      "CritPt": 10.1153,
      "Terminal-Bench 2.0": 11.2052,
      "Terminal-Bench 1.0": 10.1487,
      "ARC-AGI-1": 11.1082,
      "BRUMO 2025": 7.6047,
      "CMIMC 2025": 4.9648,
      "MathArena Apex 2025": 12.4264,
      "LiveBench": 10.6026
    },
    "evaluation_protocol": "LOO cross-validation (hat matrix) on min-max imputed matrix (0-100 scale)",
    "n_predictor_benchmarks": 7,
    "achieves_mae_under_5": false
  },
  "canonical_evaluation": {
    "canonical_overall_mae": 13.0699,
    "n_predictions": 196,
    "n_heldout_pairs": 196,
    "coverage": 1.0
  },
  "methodology_notes": "Pipeline: (1) Full 83x49 matrix extracted from flat scores list, duplicates averaged. (2) Filtered to benchmarks with >=10 models and models with >=5 benchmarks, yielding a denser submatrix. (3) Min-max normalized to [0,100] per benchmark, then MICE imputed (bounded). (4) SVD on mean-centered imputed matrix for rank analysis. (5) Greedy forward selection of 7 benchmarks minimizing LOO ridge MAE (hat-matrix trick). (6) Own evaluation via LOO hat-matrix on imputed matrix. (7) Canonical evaluation uses three methods combined via median: (a) low-rank SVD projection from revealed to all benchmarks, (b) ridge regression from revealed benchmarks trained on other models, (c) distance-weighted KNN on normalized revealed scores. Predictions soft-clipped to 15% beyond observed range. Scale mismatch across benchmarks (% correct vs Elo ratings vs index scores) is the primary data challenge."
}