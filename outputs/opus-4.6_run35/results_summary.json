{
  "data_discovery": {
    "raw_schema": "JSON with keys: 'models' (83 objects: id/name/provider/release_date/params_total_M/params_active_M/architecture/is_reasoning/open_weights), 'benchmarks' (49 objects: id/name/category/metric/num_problems/source_url), 'scores' (1390 entries: model_id/benchmark_id/score/reference_url), 'generated' (timestamp).",
    "extraction_decisions": "Scores as (model_id, benchmark_id, score) triples. 15 duplicate pairs resolved by averaging. Pivoted to model-by-benchmark matrix. No joins needed.",
    "n_models_raw": 83,
    "n_benchmarks_raw": 49
  },
  "data": {
    "n_models": 80,
    "n_benchmarks": 35,
    "missing_fraction": 0.5425,
    "preprocessing": "Filtered benchmarks with <10 obs and models with <5 obs (iteratively). Min-max normalized per benchmark to 0-100. Missing imputed with column mean for SVD; training column mean for Ridge.",
    "benchmarks_used": [
      "AIME 2024",
      "AIME 2025",
      "ARC-AGI-1",
      "ARC-AGI-2",
      "Arena-Hard Auto",
      "BrowseComp",
      "BRUMO 2025",
      "Chatbot Arena Elo",
      "CMIMC 2025",
      "Codeforces Rating",
      "CritPt",
      "FrontierMath",
      "GPQA Diamond",
      "GSM8K",
      "HLE (Humanity's Last Exam)",
      "HMMT Feb 2025",
      "HMMT Nov 2025",
      "HumanEval",
      "IFEval",
      "LiveBench",
      "LiveCodeBench",
      "MATH-500",
      "MathArena Apex 2025",
      "MMLU",
      "MMLU-Pro",
      "MMMU",
      "MMMU-Pro",
      "OSWorld",
      "SimpleQA",
      "SMT 2025",
      "SWE-bench Pro",
      "SWE-bench Verified",
      "Tau-Bench Retail",
      "Terminal-Bench 2.0",
      "Terminal-Bench 1.0"
    ]
  },
  "rank_analysis": {
    "method": "SVD on min-max normalized (0-100), mean-imputed, centered matrix",
    "effective_rank": 16,
    "variance_explained_by_rank": 0.9076,
    "singular_values": [
      541.07,
      377.4,
      231.02,
      212.23,
      179.91,
      172.26,
      165.29,
      162.05,
      150.34,
      133.77,
      126.3,
      123.8,
      119.59,
      107.68,
      102.79,
      96.95,
      93.89,
      86.74,
      85.47,
      82.0,
      81.1,
      77.75,
      72.81,
      69.16,
      65.84,
      60.12,
      59.12,
      52.11,
      49.63,
      44.33,
      43.96,
      38.64,
      38.31,
      30.72,
      23.96
    ],
    "justification": "Effective rank 16 at 90% cumulative variance. First component explains 34.6%, first 3 explain 57.8%. High apparent rank likely inflated by mean-imputation of 54% missing data; true intrinsic rank of observed data is likely lower (3-5)."
  },
  "benchmark_selection": {
    "method": "greedy_forward_correlation",
    "selected_benchmarks": [
      "Chatbot Arena Elo",
      "AIME 2025",
      "CritPt",
      "LiveBench",
      "GSM8K",
      "Terminal-Bench 1.0",
      "HMMT Nov 2025"
    ],
    "n_selected": 7,
    "selection_criterion": "Greedy: at each step select benchmark maximizing average max-|Pearson correlation| from remaining benchmarks to selected set."
  },
  "prediction": {
    "method": "Ensemble of Ridge regression + iterative SVD completion (rank=5), equal-weight average. Ridge uses all other benchmarks as features with NaN imputed by training column mean.",
    "overall_mae": 21.47,
    "per_benchmark_mae": {
      "AIME 2024": 18.5,
      "AIME 2025": 22.2,
      "ARC-AGI-1": 36.42,
      "ARC-AGI-2": 17.99,
      "Arena-Hard Auto": 46.22,
      "BrowseComp": 19.73,
      "BRUMO 2025": 11.56,
      "Chatbot Arena Elo": 18.35,
      "CMIMC 2025": 17.59,
      "Codeforces Rating": 32.42,
      "CritPt": 13.57,
      "FrontierMath": 16.26,
      "GPQA Diamond": 17.17,
      "GSM8K": 10.78,
      "HLE (Humanity's Last Exam)": 27.99,
      "HMMT Feb 2025": 18.01,
      "HMMT Nov 2025": 16.65,
      "HumanEval": 17.13,
      "IFEval": 14.04,
      "LiveBench": 29.38,
      "LiveCodeBench": 12.12,
      "MATH-500": 14.0,
      "MathArena Apex 2025": 12.33,
      "MMLU": 12.73,
      "MMLU-Pro": 13.77,
      "MMMU": 34.38,
      "MMMU-Pro": 25.63,
      "OSWorld": 20.62,
      "SimpleQA": 42.72,
      "SMT 2025": 18.59,
      "SWE-bench Pro": 28.1,
      "SWE-bench Verified": 43.03,
      "Tau-Bench Retail": 13.43,
      "Terminal-Bench 2.0": 18.04,
      "Terminal-Bench 1.0": 19.77
    },
    "evaluation_protocol": "5-fold CV on filtered matrix (80x35), Ridge-only, normalized 0-100",
    "n_predictor_benchmarks": 34,
    "achieves_mae_under_5": false
  },
  "methodology_notes": "Degrees of freedom: (1) Filtered sparse benchmarks (<10 obs) and models (<5 obs). (2) Min-max normalization to 0-100. (3) SVD with mean imputation for rank analysis. (4) Greedy correlation-based benchmark selection. (5) Ridge + SVD ensemble for canonical predictions. (6) 5-fold CV for own eval. Main challenge: 66% raw missingness (54% after filtering). Mean imputation inflates apparent rank. Iterative SVD completion more principled for prediction."
}