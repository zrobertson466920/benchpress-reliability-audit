{
  "method": "Ridge regression (alpha=10) per target benchmark, all other benchmarks as features, NaN imputed with training column mean",
  "overall_mae": 21.47,
  "per_benchmark_mae": {
    "AIME 2024": 18.5,
    "AIME 2025": 22.2,
    "ARC-AGI-1": 36.42,
    "ARC-AGI-2": 17.99,
    "Arena-Hard Auto": 46.22,
    "BrowseComp": 19.73,
    "BRUMO 2025": 11.56,
    "Chatbot Arena Elo": 18.35,
    "CMIMC 2025": 17.59,
    "Codeforces Rating": 32.42,
    "CritPt": 13.57,
    "FrontierMath": 16.26,
    "GPQA Diamond": 17.17,
    "GSM8K": 10.78,
    "HLE (Humanity's Last Exam)": 27.99,
    "HMMT Feb 2025": 18.01,
    "HMMT Nov 2025": 16.65,
    "HumanEval": 17.13,
    "IFEval": 14.04,
    "LiveBench": 29.38,
    "LiveCodeBench": 12.12,
    "MATH-500": 14.0,
    "MathArena Apex 2025": 12.33,
    "MMLU": 12.73,
    "MMLU-Pro": 13.77,
    "MMMU": 34.38,
    "MMMU-Pro": 25.63,
    "OSWorld": 20.62,
    "SimpleQA": 42.72,
    "SMT 2025": 18.59,
    "SWE-bench Pro": 28.1,
    "SWE-bench Verified": 43.03,
    "Tau-Bench Retail": 13.43,
    "Terminal-Bench 2.0": 18.04,
    "Terminal-Bench 1.0": 19.77
  },
  "evaluation_protocol": "5-fold cross-validation on filtered matrix (80x35), normalized 0-100",
  "n_predictor_benchmarks": 34,
  "achieves_mae_under_5": false,
  "n_predictions": 1281
}