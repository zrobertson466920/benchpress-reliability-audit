{
  "method": "Ridge regression from selected benchmarks to targets (alpha=1.0)",
  "overall_mae": 8.170071015659964,
  "per_benchmark_mae": {
    "AIME 2025": 13.149509607571796,
    "ARC-AGI-1": 9.79551982855185,
    "Arena-Hard Auto": 13.709308016954463,
    "BrowseComp": 7.168167702178499,
    "BRUMO 2025": 5.768541106220831,
    "Chatbot Arena Elo": 8.64692939990792,
    "CMIMC 2025": 6.7231359251591645,
    "Codeforces Rating": 12.610624796975603,
    "CritPt": 6.047215142647586,
    "FrontierMath": 9.043218597932448,
    "GPQA Diamond": 10.6882927741944,
    "GSM8K": 5.50335525969775,
    "HLE (Humanity's Last Exam)": 8.922194834587073,
    "HMMT Nov 2025": 3.325961910751885,
    "HumanEval": 9.539112511975636,
    "IFEval": 8.023963420927975,
    "LiveBench": 6.228194077503664,
    "LiveCodeBench": 10.502136152874908,
    "MATH-500": 6.701874580431641,
    "MathArena Apex 2025": 6.7660790455831,
    "MMLU": 7.5026594407531535,
    "MMMU": 10.537823980428051,
    "MMMU-Pro": 4.544586160477458,
    "OSWorld": 7.939552306388835,
    "SimpleQA": 11.70835021067314,
    "SMT 2025": 6.023260229534115,
    "SWE-bench Pro": 7.507696432527135,
    "Tau-Bench Retail": 4.300726253091058,
    "Terminal-Bench 2.0": 7.546946035245677,
    "Terminal-Bench 1.0": 8.627194728052125
  },
  "evaluation_protocol": "leave-one-model-out cross-validation on cleaned normalized matrix",
  "n_predictor_benchmarks": 5,
  "achieves_mae_under_5": false,
  "note": "MAE is on normalized 0-100 scale per benchmark"
}