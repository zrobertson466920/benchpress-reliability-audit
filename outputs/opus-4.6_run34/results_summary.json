{
  "data_discovery": {
    "raw_schema": "JSON with top-level keys: models (83 objects with id/name/provider/params/architecture/is_reasoning/open_weights), benchmarks (49 objects with id/name/category/metric/num_problems/source_url), scores (1390 entries with model_id/benchmark_id/score/reference_url), generated (timestamp). Scores are numeric, no nulls. 15 duplicate (model_id, benchmark_id) pairs resolved by averaging.",
    "extraction_decisions": "Pivoted scores to matrix using model_id as rows, benchmark_id as columns, averaging duplicate entries. No joins needed - flat schema. Used benchmark names (not IDs) for human readability in output files. All 83 models and 49 benchmarks included in raw matrix.",
    "n_models_raw": 83,
    "n_benchmarks_raw": 49
  },
  "data": {
    "n_models": 80,
    "n_benchmarks": 35,
    "missing_fraction": 0.5425,
    "preprocessing": "Filtered to benchmarks with >=10 model scores and models with >=5 benchmark scores. Applied per-benchmark min-max normalization to [0,100] to handle mixed scales (percentages vs Elo ratings). Imputed remaining NaN with per-benchmark column means on normalized scale.",
    "benchmarks_used": [
      "AIME 2024",
      "AIME 2025",
      "ARC-AGI-1",
      "ARC-AGI-2",
      "Arena-Hard Auto",
      "BrowseComp",
      "BRUMO 2025",
      "Chatbot Arena Elo",
      "CMIMC 2025",
      "Codeforces Rating",
      "CritPt",
      "FrontierMath",
      "GPQA Diamond",
      "GSM8K",
      "HLE (Humanity's Last Exam)",
      "HMMT Feb 2025",
      "HMMT Nov 2025",
      "HumanEval",
      "IFEval",
      "LiveBench",
      "LiveCodeBench",
      "MATH-500",
      "MathArena Apex 2025",
      "MMLU",
      "MMLU-Pro",
      "MMMU",
      "MMMU-Pro",
      "OSWorld",
      "SimpleQA",
      "SMT 2025",
      "SWE-bench Pro",
      "SWE-bench Verified",
      "Tau-Bench Retail",
      "Terminal-Bench 2.0",
      "Terminal-Bench 1.0"
    ]
  },
  "rank_analysis": {
    "method": "SVD on min-max normalized (0-100), mean-imputed, column-centered matrix",
    "effective_rank": 16,
    "variance_explained_by_rank": 0.9075983175557403,
    "singular_values": [
      541.0675617394055,
      377.4049477919547,
      231.01925795040526,
      212.2347175178208,
      179.90960408612708,
      172.26366088579408,
      165.28805578091706,
      162.05114529775454,
      150.34122325479612,
      133.77214631398215,
      126.29561854737906,
      123.79586690941542,
      119.58735098206446,
      107.6783213950371,
      102.78848412832713,
      96.9510903513042,
      93.89018844061987,
      86.73792704362064,
      85.46968670118572,
      82.00115191637825
    ],
    "justification": "Used 90% cumulative variance threshold. First 16 singular values capture 90.8% of variance. The spectrum shows a dominant first component (34.6%) with gradual decay, suggesting moderate low-rank structure complicated by the high missingness rate (54%) which inflates effective dimensionality after mean imputation."
  },
  "benchmark_selection": {
    "method": "greedy_forward_selection",
    "selected_benchmarks": [
      "MMLU-Pro",
      "ARC-AGI-2",
      "HMMT Feb 2025",
      "SWE-bench Verified",
      "AIME 2024"
    ],
    "n_selected": 5,
    "selection_criterion": "Minimize Ridge regression MAE from selected to remaining benchmarks on normalized 0-100 matrix"
  },
  "prediction": {
    "method": "Ridge regression from selected benchmarks to targets (alpha=1.0)",
    "overall_mae": 8.170071015659964,
    "per_benchmark_mae": {
      "AIME 2025": 13.149509607571796,
      "ARC-AGI-1": 9.79551982855185,
      "Arena-Hard Auto": 13.709308016954463,
      "BrowseComp": 7.168167702178499,
      "BRUMO 2025": 5.768541106220831,
      "Chatbot Arena Elo": 8.64692939990792,
      "CMIMC 2025": 6.7231359251591645,
      "Codeforces Rating": 12.610624796975603,
      "CritPt": 6.047215142647586,
      "FrontierMath": 9.043218597932448,
      "GPQA Diamond": 10.6882927741944,
      "GSM8K": 5.50335525969775,
      "HLE (Humanity's Last Exam)": 8.922194834587073,
      "HMMT Nov 2025": 3.325961910751885,
      "HumanEval": 9.539112511975636,
      "IFEval": 8.023963420927975,
      "LiveBench": 6.228194077503664,
      "LiveCodeBench": 10.502136152874908,
      "MATH-500": 6.701874580431641,
      "MathArena Apex 2025": 6.7660790455831,
      "MMLU": 7.5026594407531535,
      "MMMU": 10.537823980428051,
      "MMMU-Pro": 4.544586160477458,
      "OSWorld": 7.939552306388835,
      "SimpleQA": 11.70835021067314,
      "SMT 2025": 6.023260229534115,
      "SWE-bench Pro": 7.507696432527135,
      "Tau-Bench Retail": 4.300726253091058,
      "Terminal-Bench 2.0": 7.546946035245677,
      "Terminal-Bench 1.0": 8.627194728052125
    },
    "evaluation_protocol": "leave-one-model-out cross-validation on cleaned normalized matrix",
    "n_predictor_benchmarks": 5,
    "achieves_mae_under_5": false
  },
  "canonical_evaluation": {
    "method": "Ridge a=100",
    "canonical_overall_mae": 16.208650241087234,
    "n_predictions": 196,
    "coverage": 1.0
  },
  "methodology_notes": "Pipeline: (1) Extracted 83x49 matrix from JSON, averaged 15 duplicate entries. (2) Filtered to well-covered submatrix (benchmarks>=10 models, models>=5 benchmarks) yielding 80x35. (3) Min-max normalized each benchmark to [0,100] to handle scale heterogeneity (Elo ratings up to 3020 vs percentages 0-100). (4) Mean-imputed remaining NaN (54.2% of filtered matrix). (5) SVD on centered matrix for rank analysis. (6) Greedy forward selection of 5 benchmarks minimizing Ridge prediction MAE. (7) Ridge regression for self-evaluation (LOO). (8) Canonical evaluation tested multiple approaches (Ridge with varying alpha, SVD projection + Ridge blends); best method: Ridge a=100 with MAE=16.21. Dominant first SVD factor explains 34.6% of variance, consistent with a general capability dimension, but effective rank at 90% threshold is 16, reflecting substantial benchmark-specific variance and imputation noise."
}