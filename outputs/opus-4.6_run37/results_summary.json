{
  "data_discovery": {
    "raw_schema": "JSON with keys: models (list of 83 dicts: id, name, provider, release_date, params_total_M, params_active_M, architecture, is_reasoning, open_weights), benchmarks (list of 49 dicts: id, name, category, metric, num_problems, source_url), scores (list of 1390 dicts: model_id, benchmark_id, score, reference_url), generated (timestamp).",
    "extraction_decisions": "Mapped scores to (model_id, benchmark_id) pairs. Averaged 15 duplicate pairs (all deepseek-r1-distill variants with 2 entries each). All 1390 scores are numeric. Used sorted model_ids and benchmark_ids for deterministic ordering. Matrix is 83x49 with 1375 observed cells.",
    "n_models_raw": 83,
    "n_benchmarks_raw": 49
  },
  "data": {
    "n_models": 74,
    "n_benchmarks": 35,
    "missing_fraction": 0.5193,
    "preprocessing": "Filtered to benchmarks with >= 10 model observations (35/49 kept) and models with >= 8 benchmark observations post-filter (74/83 kept). Z-score normalized per benchmark (subtract mean, divide by std) for rank analysis. Min-max normalized to 0-100 per benchmark for prediction evaluation. Zero-imputation in z-score space (equivalent to mean imputation) for SVD.",
    "benchmarks_used": [
      "AIME 2024",
      "AIME 2025",
      "ARC-AGI-1",
      "ARC-AGI-2",
      "Arena-Hard Auto",
      "BrowseComp",
      "BRUMO 2025",
      "Chatbot Arena Elo",
      "CMIMC 2025",
      "Codeforces Rating",
      "CritPt",
      "FrontierMath",
      "GPQA Diamond",
      "GSM8K",
      "HLE (Humanity's Last Exam)",
      "HMMT Feb 2025",
      "HMMT Nov 2025",
      "HumanEval",
      "IFEval",
      "LiveBench",
      "LiveCodeBench",
      "MATH-500",
      "MathArena Apex 2025",
      "MMLU",
      "MMLU-Pro",
      "MMMU",
      "MMMU-Pro",
      "OSWorld",
      "SimpleQA",
      "SMT 2025",
      "SWE-bench Pro",
      "SWE-bench Verified",
      "Tau-Bench Retail",
      "Terminal-Bench 2.0",
      "Terminal-Bench 1.0"
    ]
  },
  "rank_analysis": {
    "method": "SVD on z-score normalized, zero-imputed filtered matrix (74 models x 35 benchmarks)",
    "effective_rank": 9,
    "variance_explained_by_rank": 0.8118,
    "singular_values": [
      21.9491,
      13.8376,
      8.7353,
      8.1545,
      7.0741,
      6.5417,
      6.2252,
      5.7842,
      5.4428,
      5.0649,
      4.707,
      4.3857,
      4.2976,
      3.9836,
      3.9308
    ],
    "justification": "Used 80% cumulative variance threshold on centered (z-scored) data. Top 9 components explain 81.2% of variance. The first component explains 38.7% and represents the dominant 'overall capability' axis. Ratio test gives rank 1. The spectrum confirms strong low-rank structure with rapid decay after the first few components."
  },
  "benchmark_selection": {
    "method": "greedy_forward_selection",
    "selected_benchmarks": [
      "HMMT Feb 2025",
      "ARC-AGI-2",
      "ARC-AGI-1",
      "LiveCodeBench",
      "SimpleQA"
    ],
    "n_selected": 5,
    "selection_criterion": "Minimize pairwise cross-validated Ridge MAE on min-max 0-100 scale, handling sparsity by using only jointly-observed predictor subsets per target benchmark"
  },
  "prediction": {
    "method": "Ridge regression from available selected benchmarks per model (own eval); ensemble of iterative SVD completion + Ridge from 5 revealed benchmarks (canonical eval)",
    "overall_mae": 13.5471,
    "per_benchmark_mae": {
      "AIME 2024": 10.4784,
      "AIME 2025": 9.8228,
      "Arena-Hard Auto": 21.8154,
      "BRUMO 2025": 11.909,
      "Chatbot Arena Elo": 15.3484,
      "Codeforces Rating": 11.0226,
      "FrontierMath": 18.0909,
      "GPQA Diamond": 8.8044,
      "HumanEval": 8.5821,
      "IFEval": 7.0887,
      "MATH-500": 6.2099,
      "MMLU": 8.1066,
      "MMLU-Pro": 8.4597,
      "MMMU": 16.6783,
      "OSWorld": 21.6774,
      "SMT 2025": 13.8253,
      "SWE-bench Pro": 19.7352,
      "SWE-bench Verified": 16.5127,
      "Tau-Bench Retail": 12.5507,
      "Terminal-Bench 1.0": 21.1458,
      "LiveBench": 38.2429,
      "Terminal-Bench 2.0": 16.2169,
      "CritPt": 10.1567,
      "HLE (Humanity's Last Exam)": 12.5891,
      "BrowseComp": 44.7632,
      "MMMU-Pro": 63.6979,
      "CMIMC 2025": 14.3907,
      "MathArena Apex 2025": 22.8078,
      "GSM8K": 15.3067,
      "HMMT Nov 2025": 27.1997
    },
    "evaluation_protocol": "Leave-one-model-out, Ridge from available selected benchmarks, min-max normalized 0-100 scale",
    "n_predictor_benchmarks": 5,
    "achieves_mae_under_5": false
  },
  "canonical_evaluation": {
    "canonical_overall_mae": 17.2269,
    "canonical_coverage": 1.0,
    "n_predictions": 196,
    "n_heldout_pairs": 196,
    "svd_rank_used": 3
  },
  "methodology_notes": "Pipeline: (1) Extract 83x49 raw matrix from JSON, averaging 15 duplicate entries. (2) Filter to models/benchmarks with sufficient coverage (>=10 bench obs, >=8 model obs) yielding 74x35 cleaned matrix. (3) Z-score normalize per benchmark for rank analysis (centers data so SVD captures covariance). (4) SVD on zero-imputed z-scored matrix; effective rank determined by 80% cumulative variance (rank 9), but rank 3 used for matrix completion to avoid overfitting with 66% missing data. (5) Greedy forward benchmark selection (5 benchmarks from high-coverage candidates only) minimizing Ridge cross-validated MAE, handling sparsity by using per-target available predictor subsets. (6) LOO-model Ridge prediction for own evaluation on min-max 0-100 scale. (7) Canonical eval uses ensemble of iterative rank-3 SVD matrix completion (40% weight) and Ridge from 5 revealed benchmarks with alpha=10 (60% weight). Key methodological choices: z-score rather than raw/min-max for decomposition; filtering sparse rows/columns rather than full-matrix imputation before analysis; high-coverage-only benchmark candidates for selection; conservative SVD rank for completion; Ridge for prediction robustness."
}