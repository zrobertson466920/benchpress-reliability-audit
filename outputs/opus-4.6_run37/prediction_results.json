{
  "method": "Ridge regression from available selected benchmarks per model",
  "overall_mae": 13.5471,
  "per_benchmark_mae": {
    "AIME 2024": 10.4784,
    "AIME 2025": 9.8228,
    "Arena-Hard Auto": 21.8154,
    "BRUMO 2025": 11.909,
    "Chatbot Arena Elo": 15.3484,
    "Codeforces Rating": 11.0226,
    "FrontierMath": 18.0909,
    "GPQA Diamond": 8.8044,
    "HumanEval": 8.5821,
    "IFEval": 7.0887,
    "MATH-500": 6.2099,
    "MMLU": 8.1066,
    "MMLU-Pro": 8.4597,
    "MMMU": 16.6783,
    "OSWorld": 21.6774,
    "SMT 2025": 13.8253,
    "SWE-bench Pro": 19.7352,
    "SWE-bench Verified": 16.5127,
    "Tau-Bench Retail": 12.5507,
    "Terminal-Bench 1.0": 21.1458,
    "LiveBench": 38.2429,
    "Terminal-Bench 2.0": 16.2169,
    "CritPt": 10.1567,
    "HLE (Humanity's Last Exam)": 12.5891,
    "BrowseComp": 44.7632,
    "MMMU-Pro": 63.6979,
    "CMIMC 2025": 14.3907,
    "MathArena Apex 2025": 22.8078,
    "GSM8K": 15.3067,
    "HMMT Nov 2025": 27.1997
  },
  "evaluation_protocol": "leave-one-model-out, min-max normalized 0-100 scale",
  "n_predictor_benchmarks": 5,
  "achieves_mae_under_5": false,
  "n_predictions": 1028
}