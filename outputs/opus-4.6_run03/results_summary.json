{
  "data_discovery": {
    "raw_schema": "JSON with top-level keys: 'models' (list of 83 dicts with id/name/provider/release_date/params/architecture/is_reasoning/open_weights), 'benchmarks' (list of 49 dicts with id/name/category/metric/num_problems/source_url), 'scores' (list of 1390 dicts with model_id/benchmark_id/score/reference_url), 'generated' (timestamp string). Relational structure: scores reference models and benchmarks by ID.",
    "extraction_decisions": "Mapped scores to (model_id, benchmark_id) -> score. Found 15 duplicate pairs (DeepSeek distill variants), resolved by averaging per canonical protocol. Built 83x49 matrix with model_id as row key and benchmark_id as column key. Used model 'name' field for display and 'id' for joins. No nested scores or multi-metric entries found.",
    "n_models_raw": 83,
    "n_benchmarks_raw": 49
  },
  "data": {
    "n_models": 74,
    "n_benchmarks": 40,
    "missing_fraction": 0.5659,
    "preprocessing": "Filtered benchmarks with <8 observations (dropped 9: AA Long Context Reasoning, AIME 2026, BigCodeBench, GDP-Val AA, IFBench, IMO 2025, MathVision, SciCode, SimpleBench) and models with <8 observations (dropped 9 including distill variants and small models). Applied per-benchmark min-max normalization to 0-100 scale to handle mixed metrics (Elo ratings, percentages, index scores). For SVD analysis, imputed missing values with column (benchmark) means. The cleaned matrix is the filtered + normalized matrix before imputation.",
    "benchmarks_used": [
      "AA Intelligence Index",
      "AIME 2024",
      "AIME 2025",
      "ARC-AGI-1",
      "ARC-AGI-2",
      "Arena-Hard Auto",
      "BrowseComp",
      "BRUMO 2025",
      "Chatbot Arena Elo",
      "CMIMC 2025",
      "Codeforces Rating",
      "CritPt",
      "FrontierMath",
      "GPQA Diamond",
      "GSM8K",
      "HLE (Humanity's Last Exam)",
      "HMMT Feb 2025",
      "HMMT Nov 2025",
      "HumanEval",
      "IFEval",
      "LiveBench",
      "LiveCodeBench",
      "MATH-500",
      "MathArena Apex 2025",
      "MMLU",
      "MMLU-Pro",
      "MMMU",
      "MMMU-Pro",
      "MRCR v2",
      "OSWorld",
      "SimpleQA",
      "SMT 2025",
      "SWE-bench Pro",
      "SWE-bench Verified",
      "Tau-Bench Retail",
      "Tau-Bench Telecom",
      "Terminal-Bench 2.0",
      "Terminal-Bench 1.0",
      "USAMO 2025",
      "Video-MMU"
    ]
  },
  "rank_analysis": {
    "method": "SVD on mean-imputed, column-centered, min-max normalized (0-100) matrix. 74 models x 40 benchmarks.",
    "effective_rank": 2,
    "variance_explained_by_rank": 0.498,
    "singular_values": [
      538.532,
      382.1967,
      234.3903,
      216.1972,
      185.0185,
      174.5025,
      172.7392,
      165.128,
      159.7015,
      139.3454,
      131.2974,
      127.7528,
      123.8194,
      110.6113,
      107.511,
      101.7463,
      99.9714,
      94.2341,
      88.2272,
      85.4232,
      81.1372,
      79.4279,
      73.9655,
      68.1202,
      66.0018,
      65.2947,
      59.1861,
      53.528,
      52.8274,
      50.5137,
      44.862,
      43.1626,
      41.4851,
      37.5572,
      34.3741,
      30.8702,
      26.0393,
      23.4593,
      17.9567,
      12.4301
    ],
    "justification": "Cross-validation on held-out observed entries strongly favors rank 2 (CV MAE = 15.05) over rank 1 (16.67), rank 3 (15.62), and higher ranks which monotonically worsen. The 90% variance threshold suggests rank 17, but this is inflated by mean imputation artifacts in a 56.6% missing matrix. The first two components explain 49.8% of variance and capture the dominant structure: a general capability factor (33.1%) and a reasoning/math vs. knowledge/instruction-following contrast (16.7%). CV-optimal rank of 2 is the appropriate effective rank for prediction."
  },
  "benchmark_selection": {
    "method": "Greedy forward selection from top-20 most-observed benchmarks. At each step, the benchmark minimizing overall leave-one-model-out MAE (ridge regression, alpha=1.0, normalized 0-100 scale) across all target benchmarks was added. Selection stopped at 5 benchmarks where MAE was minimized (10.49); adding more benchmarks increased error due to overfitting from reduced model overlap.",
    "selected_benchmarks": [
      "HLE (Humanity's Last Exam)",
      "Arena-Hard Auto",
      "AIME 2025",
      "SimpleQA",
      "MMLU-Pro"
    ],
    "n_selected": 5,
    "selection_criterion": "Minimize leave-one-model-out prediction MAE on normalized 0-100 scale. Optimal at 5 benchmarks (MAE = 10.49); 6th benchmark worsened to 10.67, confirming diminishing returns."
  },
  "prediction": {
    "method": "Ensemble of soft-impute low-rank matrix completion (ranks 2 and 3, lambda=3.0) blended with ridge regression from revealed benchmarks. For canonical evaluation: 60% weight on averaged soft-impute, 40% on ridge when available (>=3 training samples), 100% soft-impute otherwise. Predictions clipped to [0, 100] normalized range. For own evaluation: ridge regression from 5 selected benchmarks to each target.",
    "overall_mae": 10.4907,
    "per_benchmark_mae": {
      "AIME 2024": 12.2275,
      "ARC-AGI-1": 9.844,
      "ARC-AGI-2": 2.0716,
      "BRUMO 2025": 30.4282,
      "Codeforces Rating": 15.0948,
      "CritPt": 7.9545,
      "FrontierMath": 18.0572,
      "GPQA Diamond": 5.13,
      "HMMT Feb 2025": 10.7947,
      "HumanEval": 14.155,
      "IFEval": 6.5559,
      "LiveCodeBench": 8.4098,
      "MATH-500": 3.3036,
      "MMLU": 5.2022,
      "MMMU": 14.5051,
      "SMT 2025": 2.7144,
      "SWE-bench Verified": 19.6508,
      "Terminal-Bench 2.0": 10.379,
      "Terminal-Bench 1.0": 12.0275,
      "USAMO 2025": 17.8537
    },
    "evaluation_protocol": "Leave-one-model-out cross-validation using ridge regression from selected subset to each target benchmark. Only models with complete observations across all source + target benchmarks were included. 192 total predictions across 20 evaluable target benchmarks.",
    "n_predictor_benchmarks": 5,
    "achieves_mae_under_5": false,
    "canonical_overall_mae_normalized": 17.09,
    "canonical_coverage": 1.0,
    "canonical_n_predictions": 196
  },
  "methodology_notes": "Key methodological choices: (1) Schema: relational JSON with model/benchmark/score tables, duplicates averaged. (2) Preprocessing: filtered sparse rows/columns (threshold=8), min-max normalized per benchmark to handle mixed Elo/percentage/index scales. (3) Rank analysis: SVD on mean-imputed centered matrix; CV-optimal effective rank = 2. The matrix exhibits strong low-rank structure with a dominant general capability factor, but the high missingness (56.6% after filtering, 66.2% raw) limits the precision of rank estimates. (4) Benchmark selection: greedy forward selection identified 5 diverse benchmarks spanning reasoning (HLE), instruction following (Arena-Hard), math (AIME 2025), knowledge (SimpleQA, MMLU-Pro). (5) Prediction: blended soft-impute matrix completion with ridge regression for canonical eval. The canonical MAE of 17.09 reflects the difficulty of predicting from only 5 revealed benchmarks in a very sparse matrix. Major error sources are sparse benchmarks (HMMT, MRCR, MMMU-Pro) where few training models exist. (6) Scale mismatch between percentage-based and Elo-based benchmarks was handled via per-benchmark normalization, not by dropping non-percentage benchmarks."
}