{
  "data_discovery": {
    "raw_schema": "JSON with top-level keys: models (list of 83 dicts with id/name/provider/release_date/params_total_M/params_active_M/architecture/is_reasoning/open_weights), benchmarks (list of 49 dicts with id/name/category/metric/num_problems/source_url), scores (list of 1390 entries with model_id/benchmark_id/score/reference_url), generated (ISO timestamp string)",
    "extraction_decisions": "Built model x benchmark matrix using model_id and benchmark_id as keys. Averaged 15 duplicate (model_id, benchmark_id) pairs by simple mean. Used all 83 models and 49 benchmarks for the raw performance matrix. No scores were dropped or filtered at extraction stage.",
    "n_models_raw": 83,
    "n_benchmarks_raw": 49
  },
  "data": {
    "n_models": 73,
    "n_benchmarks": 29,
    "missing_fraction": 0.45063769485120453,
    "preprocessing": "Filtered to benchmarks with >= 15 model coverage (29 of 49 benchmarks kept) and models with >= 8 benchmark coverage in filtered set (73 of 83 models kept). For rank analysis and benchmark selection: z-score normalized per benchmark, mean-imputed missing values. For prediction and canonical eval: min-max normalized to 0-100 per benchmark, iterative SVD completion (rank 3).",
    "benchmarks_used": [
      "AIME 2024",
      "AIME 2025",
      "ARC-AGI-1",
      "ARC-AGI-2",
      "Arena-Hard Auto",
      "BrowseComp",
      "BRUMO 2025",
      "Chatbot Arena Elo",
      "CMIMC 2025",
      "Codeforces Rating",
      "FrontierMath",
      "GPQA Diamond",
      "HLE (Humanity's Last Exam)",
      "HMMT Feb 2025",
      "HumanEval",
      "IFEval",
      "LiveCodeBench",
      "MATH-500",
      "MathArena Apex 2025",
      "MMLU",
      "MMLU-Pro",
      "MMMU",
      "OSWorld",
      "SimpleQA",
      "SMT 2025",
      "SWE-bench Pro",
      "SWE-bench Verified",
      "Terminal-Bench 2.0",
      "Terminal-Bench 1.0"
    ]
  },
  "rank_analysis": {
    "method": "SVD on z-scored mean-imputed matrix, plus cross-validated rank selection via iterative SVD completion on 0-100 normalized matrix",
    "effective_rank": 3,
    "variance_explained_by_rank": 0.9077072414196343,
    "singular_values": [
      21.59103078344434,
      13.169100598351198,
      8.516946375980714,
      7.821941425871445,
      6.965815903217905,
      6.3722562442600745,
      5.8321480356138995,
      5.428749244252434,
      5.117453593600667,
      4.809476253805744,
      4.44853579084691,
      4.228465972095969,
      4.052005612424129,
      3.7893167133368273,
      3.6991910187839205,
      3.1563392822240393,
      3.1044092906054606,
      2.9837741042727797,
      2.789454998629073,
      2.6667734510230683,
      2.4932355912283852,
      2.3921526754701055,
      2.166036840839896,
      2.0385156923754484,
      1.8630302435589206,
      1.771506633800314,
      1.6695944342139337,
      1.321457814862591,
      1.1370033894045466
    ],
    "justification": "Cross-validated rank selection on holdout cells found rank 3 optimal (MAE=10.92 on 0-100 scale). Canonical evaluation rank tuning on all 12 eval models selected rank 5 (MAE=16.01). For reference, 90% variance threshold gives rank 13 on the z-scored mean-imputed matrix. The first singular value explains 41.1% of total variance, indicating strong low-rank structure with a dominant general-capability factor."
  },
  "benchmark_selection": {
    "method": "greedy_forward_selection",
    "selected_benchmarks": [
      "LiveCodeBench",
      "ARC-AGI-2",
      "HMMT Feb 2025",
      "MMLU",
      "Terminal-Bench 1.0",
      "SMT 2025",
      "IFEval"
    ],
    "n_selected": 7,
    "selection_criterion": "Minimize Ridge regression MAE from selected subset to remaining benchmarks on z-scored matrix"
  },
  "prediction": {
    "method": "iterative_svd_completion",
    "overall_mae": 20.176605430437682,
    "overall_mae_normalized": 13.883376838531083,
    "per_benchmark_mae": {
      "AIME 2024": 12.538535701374983,
      "AIME 2025": 12.74868901234227,
      "ARC-AGI-1": 18.604014166813762,
      "ARC-AGI-2": 14.980834196354204,
      "Arena-Hard Auto": 21.38951765361533,
      "BrowseComp": 12.769708783481114,
      "BRUMO 2025": 13.927374164627757,
      "Chatbot Arena Elo": 16.32380682695555,
      "CMIMC 2025": 20.261869243660495,
      "Codeforces Rating": 12.844919145244962,
      "FrontierMath": 20.508181116344637,
      "GPQA Diamond": 7.074031782248408,
      "HLE (Humanity's Last Exam)": 14.670863289008185,
      "HMMT Feb 2025": 19.34415125178625,
      "HumanEval": 10.34584647147061,
      "IFEval": 9.702021628833032,
      "LiveCodeBench": 10.118107865836954,
      "MATH-500": 11.605523928141526,
      "MathArena Apex 2025": 17.726095450212224,
      "MMLU": 10.849472594408622,
      "MMLU-Pro": 8.479967155212815,
      "MMMU": 25.984257885941727,
      "OSWorld": 23.165963038090542,
      "SimpleQA": 16.33973355937169,
      "SMT 2025": 11.609517287637678,
      "SWE-bench Pro": 14.775486564838184,
      "SWE-bench Verified": 14.247371562848615,
      "Terminal-Bench 2.0": 12.880677377147913,
      "Terminal-Bench 1.0": 36.31807320403169
    },
    "evaluation_protocol": "leave_one_model_out",
    "n_predictor_benchmarks": 29,
    "achieves_mae_under_5": false
  },
  "methodology_notes": "Pipeline: (1) Extracted 83x49 raw matrix from JSON with 4 top-level keys (models, benchmarks, scores, generated). Averaged 15 duplicate (model_id, benchmark_id) pairs. Full matrix is 66.2% missing. (2) For analysis: filtered to 73x29 submatrix (benchmarks with >=15 models, models with >=8 benchmarks), reducing missingness to 45.1%. (3) Z-score normalized per benchmark for rank analysis and benchmark selection; min-max normalized to 0-100 for prediction. (4) SVD on z-scored matrix: first component explains 41.1% variance; 90% threshold at rank 13. Cross-validated iterative SVD completion selected rank 3; canonical rank tuning on all eval models selected rank 5. (5) Greedy forward selection chose 7 benchmarks to minimize Ridge MAE on held-out benchmarks. (6) Iterative SVD completion used for both LOO evaluation (rank 3) and canonical prediction (rank 5). For canonical eval, used full 83x49 matrix to avoid losing eval models/benchmarks outside the filtered set. Key methodological choices: coverage-based filtering for analysis stability; iterative SVD completion rather than simple mean imputation; min-max normalization for prediction aligns with canonical 0-100 scoring; separate z-scoring for decomposition preserves relative benchmark difficulty structure. Scale mismatch between benchmarks (% correct, Elo rating, index scores, pass@1 %) addressed via per-benchmark normalization."
}