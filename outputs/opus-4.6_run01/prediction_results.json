{
  "method": "Ridge regression (alpha=10) from 5 selected benchmarks to each target",
  "overall_mae": 12.274643541456651,
  "per_benchmark_mae": {
    "AIME 2024": 9.764446642871356,
    "ARC-AGI-1": 11.619304231431094,
    "Arena-Hard Auto": 20.30507180665301,
    "BrowseComp": 16.636973159753747,
    "BRUMO 2025": 12.049080299303379,
    "Chatbot Arena Elo": 15.710625268997765,
    "CMIMC 2025": 9.982934434860828,
    "Codeforces Rating": 10.22275440910127,
    "CritPt": 10.052201928098405,
    "FrontierMath": 17.677653117918922,
    "GSM8K": 12.2120599281777,
    "HLE (Humanity's Last Exam)": 12.46247097317971,
    "HMMT Feb 2025": 13.023984555367617,
    "HMMT Nov 2025": 13.909620569687737,
    "HumanEval": 9.394355866733873,
    "IFEval": 7.431016255715205,
    "LiveBench": 27.1516558253258,
    "MATH-500": 6.550765118398814,
    "MathArena Apex 2025": 17.951314025275767,
    "MMLU": 9.61072298396204,
    "MMLU-Pro": 8.590548798078007,
    "MMMU": 15.473029184265961,
    "MMMU-Pro": 22.91252249957487,
    "OSWorld": 22.02674369548775,
    "SimpleQA": 16.18936693605926,
    "SMT 2025": 7.561808142303418,
    "SWE-bench Pro": 14.763092331160303,
    "Tau-Bench Retail": 23.17388857599016,
    "Terminal-Bench 2.0": 13.879930617888645,
    "Terminal-Bench 1.0": 19.290059667513404
  },
  "evaluation_protocol": "leave-one-model-out cross-validation on observed entries",
  "n_predictor_benchmarks": 5,
  "predictor_benchmarks": [
    "GPQA Diamond",
    "ARC-AGI-2",
    "AIME 2025",
    "SWE-bench Verified",
    "LiveCodeBench"
  ],
  "achieves_mae_under_5": false,
  "scale": "normalized [0,100] per benchmark"
}