{
  "data_discovery": {
    "raw_schema": "JSON with top-level keys: 'models' (list of 83 dicts with id/name/provider/params/architecture/is_reasoning/open_weights), 'benchmarks' (list of 49 dicts with id/name/category/metric/num_problems/source_url), 'scores' (list of 1390 dicts with model_id/benchmark_id/score/reference_url), 'generated' (timestamp string). Relational schema: models and benchmarks as entities, scores as a join table.",
    "extraction_decisions": "1. Used model_id and benchmark_id as join keys. 2. 15 duplicate (model_id, benchmark_id) pairs found; all had identical scores, so averaging was trivial. 3. No null scores found; all 1390 entries had numeric values. 4. Used model 'name' field for display, 'id' for internal keys. 5. Benchmark 'name' used as column headers in performance_matrix.csv. 6. Full raw matrix: 83 models x 49 benchmarks with 66.2% missing entries.",
    "n_models_raw": 83,
    "n_benchmarks_raw": 49
  },
  "data": {
    "n_models": 80,
    "n_benchmarks": 35,
    "missing_fraction": 0.5425,
    "preprocessing": "1. Dropped 14 benchmarks with fewer than 10 observed models (too sparse for reliable decomposition): AA Intelligence Index, AA Long Context Reasoning, AIME 2026, BigCodeBench, GDP-Val AA, IFBench, IMO 2025, MathVision, MRCR v2, SciCode, SimpleBench, Tau-Bench Telecom, USAMO 2025, Video-MMU. 2. Dropped 3 models with fewer than 5 observed benchmarks after benchmark filtering: Codestral 25.01, Devstral 2, Phi-4-reasoning. 3. Applied per-benchmark min-max normalization to [0, 100] scale to handle mixed metrics (Elo ratings ~800-3020, percentages 0-100, index scores). 4. Imputed remaining missing values with per-benchmark (column) mean of the normalized matrix. Final cleaned matrix: 80 models x 35 benchmarks, 54.3% missing.",
    "benchmarks_used": [
      "AIME 2024",
      "AIME 2025",
      "ARC-AGI-1",
      "ARC-AGI-2",
      "Arena-Hard Auto",
      "BrowseComp",
      "BRUMO 2025",
      "Chatbot Arena Elo",
      "CMIMC 2025",
      "Codeforces Rating",
      "CritPt",
      "FrontierMath",
      "GPQA Diamond",
      "GSM8K",
      "HLE (Humanity's Last Exam)",
      "HMMT Feb 2025",
      "HMMT Nov 2025",
      "HumanEval",
      "IFEval",
      "LiveBench",
      "LiveCodeBench",
      "MATH-500",
      "MathArena Apex 2025",
      "MMLU",
      "MMLU-Pro",
      "MMMU",
      "MMMU-Pro",
      "OSWorld",
      "SimpleQA",
      "SMT 2025",
      "SWE-bench Pro",
      "SWE-bench Verified",
      "Tau-Bench Retail",
      "Terminal-Bench 2.0",
      "Terminal-Bench 1.0"
    ]
  },
  "rank_analysis": {
    "method": "SVD on column-mean-imputed, min-max-normalized [0,100], column-centered matrix (80 models x 35 benchmarks)",
    "effective_rank": 16,
    "variance_explained_by_rank": 0.9076,
    "singular_values": [
      541.068,
      377.405,
      231.019,
      212.235,
      179.91,
      172.264,
      165.288,
      162.051,
      150.341,
      133.772,
      126.296,
      123.796,
      119.587,
      107.678,
      102.788,
      96.951,
      93.89,
      86.738,
      85.47,
      82.001
    ],
    "justification": "Effective rank is 16 using the 90% cumulative variance threshold. The first component captures 34.7% of variance (strong general ability factor), first two capture 51.5%. However, reaching 90% requires 16 components, indicating substantial structure beyond a simple low-rank approximation. The high missingness (54.3%) inflates the effective dimensionality since mean imputation adds noise to the covariance structure. The 80% threshold gives rank 10, the 95% threshold rank 21. In practice, rank 3-8 is sufficient for reasonable prediction quality."
  },
  "benchmark_selection": {
    "method": "greedy_forward_selection",
    "selected_benchmarks": [
      "GPQA Diamond",
      "ARC-AGI-2",
      "AIME 2025",
      "SWE-bench Verified",
      "LiveCodeBench"
    ],
    "n_selected": 5,
    "selection_criterion": "Greedy forward selection minimizing 5-fold cross-validated MAE when predicting all non-selected benchmarks via Ridge regression (alpha=10) from the selected subset, on normalized [0,100] scale. Selected: GPQA Diamond (science/knowledge), ARC-AGI-2 (reasoning), AIME 2025 (math), SWE-bench Verified (coding), LiveCodeBench (coding). These span the major benchmark categories and provide good coverage."
  },
  "prediction": {
    "method": "Ensemble of iterative SVD completion (ranks 3, 5, 8) and per-target Ridge regression from revealed/selected benchmarks. Predictions combined via median. For benchmarks outside the cleaned set, Ridge regression from revealed benchmarks or benchmark mean fallback. All predictions clipped to [0, 100] normalized range.",
    "overall_mae": 12.275,
    "per_benchmark_mae": {
      "AIME 2024": 9.764,
      "ARC-AGI-1": 11.619,
      "Arena-Hard Auto": 20.305,
      "BrowseComp": 16.637,
      "BRUMO 2025": 12.049,
      "Chatbot Arena Elo": 15.711,
      "CMIMC 2025": 9.983,
      "Codeforces Rating": 10.223,
      "CritPt": 10.052,
      "FrontierMath": 17.678,
      "GSM8K": 12.212,
      "HLE (Humanity's Last Exam)": 12.462,
      "HMMT Feb 2025": 13.024,
      "HMMT Nov 2025": 13.91,
      "HumanEval": 9.394,
      "IFEval": 7.431,
      "LiveBench": 27.152,
      "MATH-500": 6.551,
      "MathArena Apex 2025": 17.951,
      "MMLU": 9.611,
      "MMLU-Pro": 8.591,
      "MMMU": 15.473,
      "MMMU-Pro": 22.913,
      "OSWorld": 22.027,
      "SimpleQA": 16.189,
      "SMT 2025": 7.562,
      "SWE-bench Pro": 14.763,
      "Tau-Bench Retail": 23.174,
      "Terminal-Bench 2.0": 13.88,
      "Terminal-Bench 1.0": 19.29
    },
    "evaluation_protocol": "leave-one-model-out cross-validation on observed entries",
    "n_predictor_benchmarks": 5,
    "achieves_mae_under_5": false,
    "canonical_overall_mae": 16.078,
    "canonical_coverage": 1.0
  },
  "methodology_notes": "Key decisions and caveats:\n1. SCHEMA: Used relational join on model_id/benchmark_id. All 15 duplicates had identical scores.\n2. MISSING DATA: 66.2% raw missing, 54.3% after filtering sparse rows/cols. Used column-mean imputation for SVD/analysis. This is a significant limitation \u2014 mean imputation biases the covariance structure and inflates effective rank.\n3. NORMALIZATION: Per-benchmark min-max to [0,100]. This handles the mixed-scale issue (Elo ratings vs percentages) but loses absolute scale information.\n4. DECOMPOSITION: SVD on the imputed+normalized matrix. Simple and interpretable, but mean imputation before SVD is suboptimal vs. true matrix completion approaches.\n5. RANK: 90% variance threshold gives rank 16, but rank 3-8 works well for prediction. The gap suggests the tail components are partly imputation noise.\n6. SUBSET SELECTION: Greedy forward with Ridge CV. The selected set covers science, reasoning, math, and coding \u2014 reasonable span of the benchmark space.\n7. PREDICTION: Hybrid SVD completion + Ridge. SVD handles the low-rank structure globally; Ridge exploits per-target relationships with revealed benchmarks. Median ensemble is robust to outliers from either method.\n8. CANONICAL EVAL: MAE ~16 on 0-100 scale. Worst predictions are for benchmarks with very few observations (n<=2) where there's essentially no training signal. Many predictions off by 20-50 points for rare benchmarks like HMMT, USAMO, MRCR.\n9. POTENTIAL IMPROVEMENTS: Bayesian matrix completion (e.g., PMF/BPMF) would be more principled for the high-missingness regime. Category-aware priors could help for rare benchmarks."
}