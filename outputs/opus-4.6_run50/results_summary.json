{
  "data_discovery": {
    "raw_schema": "JSON with keys: models (83 dicts: id, name, provider, release_date, params_total_M, params_active_M, architecture, is_reasoning, open_weights), benchmarks (49 dicts: id, name, category, metric, num_problems, source_url), scores (1390 dicts: model_id, benchmark_id, score, reference_url), generated (timestamp string).",
    "extraction_decisions": "Mapped scores to (model_id, benchmark_id) pairs. Found 15 duplicate pairs, resolved by simple averaging per canonical spec. No null scores present. All score values are numeric (int/float).",
    "n_models_raw": 83,
    "n_benchmarks_raw": 49
  },
  "data": {
    "n_models": 74,
    "n_benchmarks": 35,
    "missing_fraction": 0.5193,
    "preprocessing": "Filtered to benchmarks with >=10 observed models (35/49 kept) and models with >=8 observed benchmarks (74/83 kept). Remaining missing values imputed with per-benchmark column means for SVD and prediction. Z-score normalization for SVD; min-max 0-100 for benchmark selection; raw scores for own evaluation.",
    "benchmarks_used": [
      "GPQA Diamond",
      "AIME 2025",
      "MMLU",
      "MMLU-Pro",
      "SWE-bench Verified",
      "MATH-500",
      "LiveCodeBench",
      "FrontierMath",
      "HLE (Humanity's Last Exam)",
      "ARC-AGI-2",
      "BrowseComp",
      "SimpleQA",
      "IFEval",
      "HumanEval",
      "Codeforces Rating",
      "OSWorld",
      "MMMU",
      "MMMU-Pro",
      "Arena-Hard Auto",
      "Chatbot Arena Elo",
      "SWE-bench Pro",
      "AIME 2024",
      "HMMT Feb 2025",
      "Tau-Bench Retail",
      "CritPt",
      "GSM8K",
      "Terminal-Bench 2.0",
      "Terminal-Bench 1.0",
      "ARC-AGI-1",
      "BRUMO 2025",
      "SMT 2025",
      "HMMT Nov 2025",
      "CMIMC 2025",
      "MathArena Apex 2025",
      "LiveBench"
    ]
  },
  "rank_analysis": {
    "method": "SVD on z-scored (per-benchmark) cleaned+imputed matrix",
    "effective_rank": 17,
    "variance_explained_by_rank": 0.9067,
    "singular_values": [
      26.2503,
      20.8429,
      13.615,
      12.6429,
      11.8041,
      9.9963,
      9.5914,
      9.1737,
      8.9958,
      7.8649,
      7.6328,
      7.4967,
      6.8996,
      6.7138,
      6.5991,
      6.0851,
      5.8168,
      5.736,
      5.0536,
      4.8449
    ],
    "justification": "Using 90% cumulative variance threshold on the z-scored imputed matrix, effective rank is 17. The first component alone explains 26.6% and the first two explain 43.4%. The high effective rank (17) partly reflects noise amplification from imputing ~52% missing entries; the true latent dimensionality is likely lower (rank 3 was optimal for prediction)."
  },
  "benchmark_selection": {
    "method": "Greedy forward selection with 5-fold CV Ridge on min-max normalized data",
    "selected_benchmarks": [
      "GPQA Diamond",
      "Terminal-Bench 2.0",
      "SWE-bench Verified",
      "LiveCodeBench",
      "Tau-Bench Retail",
      "AIME 2025"
    ],
    "n_selected": 6,
    "selection_criterion": "Minimize 5-fold CV MAE on observed target entries; stop when adding a benchmark fails to improve"
  },
  "prediction": {
    "method": "Ridge regression (alpha=1.0) from selected benchmark subset",
    "overall_mae": 20.9221,
    "per_benchmark_mae": {
      "MMLU": 4.1428,
      "MMLU-Pro": 5.6347,
      "MATH-500": 4.038,
      "FrontierMath": 7.4255,
      "HLE (Humanity's Last Exam)": 6.272,
      "ARC-AGI-2": 9.992,
      "BrowseComp": 6.5563,
      "SimpleQA": 10.9377,
      "IFEval": 4.205,
      "HumanEval": 5.5401,
      "Codeforces Rating": 266.6205,
      "OSWorld": 8.1039,
      "MMMU": 3.2223,
      "MMMU-Pro": 4.5526,
      "Arena-Hard Auto": 20.0631,
      "Chatbot Arena Elo": 25.6903,
      "SWE-bench Pro": 5.9282,
      "AIME 2024": 9.4523,
      "HMMT Feb 2025": 7.9759,
      "CritPt": 4.027,
      "GSM8K": 3.882,
      "Terminal-Bench 1.0": 11.4299,
      "ARC-AGI-1": 11.9721,
      "BRUMO 2025": 4.6276,
      "SMT 2025": 3.8334,
      "HMMT Nov 2025": 4.1454,
      "CMIMC 2025": 7.0604,
      "MathArena Apex 2025": 4.7601,
      "LiveBench": 7.0224
    },
    "evaluation_protocol": "5-fold CV on observed entries of cleaned matrix (raw scores)",
    "n_predictor_benchmarks": 6,
    "achieves_mae_under_5": false
  },
  "canonical_evaluation": {
    "canonical_overall_mae_normalized": 15.7629,
    "coverage": "196/196",
    "coverage_fraction": 1.0,
    "method": "Hybrid: 70% iterative SVD (rank=3) + 30% Ridge from revealed benchmarks, clipped to observed range \u00b110%"
  },
  "methodology_notes": "Pipeline: (1) Extracted 83x49 matrix from JSON (66.2% missing), 15 duplicate pairs averaged. (2) Filtered to 35 benchmarks (>=10 models) and 74 models (>=8 benchmarks); column-mean imputed remaining gaps (~52% of filtered matrix). (3) SVD on z-scored matrix for rank analysis: effective rank 17 at 90% variance. (4) Greedy forward selection of 6 benchmarks via 5-fold CV Ridge on min-max data. (5) Ridge regression for own evaluation (MAE 20.92 in raw scores). (6) Canonical evaluation: hybrid approach blending 70% iterative SVD completion (rank 3) with 30% Ridge regression from 5 revealed benchmarks, predictions clipped to observed range \u00b110%. Canonical MAE 15.76 on normalized 0-100 scale. The matrix is strongly low-rank and benchmark performance is predictably structured, though high missingness (~66% raw) limits precision for sparse benchmarks."
}