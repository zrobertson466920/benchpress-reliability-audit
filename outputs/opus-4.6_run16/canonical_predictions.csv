model_id,model_name,benchmark_id,benchmark_name,y_pred
claude-opus-4,Claude Opus 4,aime_2024,AIME 2024,70.6877
claude-opus-4,Claude Opus 4,aime_2025,AIME 2025,66.6461
claude-opus-4,Claude Opus 4,arc_agi_1,ARC-AGI-1,44.934
claude-opus-4,Claude Opus 4,arc_agi_2,ARC-AGI-2,6.3995
claude-opus-4,Claude Opus 4,codeforces_rating,Codeforces Rating,2249.0318
claude-opus-4,Claude Opus 4,critpt,CritPt,0.5834
claude-opus-4,Claude Opus 4,frontiermath,FrontierMath,10.6774
claude-opus-4,Claude Opus 4,gpqa_diamond,GPQA Diamond,75.5738
claude-opus-4,Claude Opus 4,hle,HLE (Humanity's Last Exam),13.8319
claude-opus-4,Claude Opus 4,hmmt_2025,HMMT Feb 2025,50.3745
claude-opus-4,Claude Opus 4,ifeval,IFEval,88.9682
claude-opus-4,Claude Opus 4,livecodebench,LiveCodeBench,59.327
claude-opus-4,Claude Opus 4,math_500,MATH-500,95.7864
claude-opus-4,Claude Opus 4,mmlu,MMLU,88.4709
claude-opus-4,Claude Opus 4,mmmu,MMMU,76.8669
claude-opus-4,Claude Opus 4,simpleqa,SimpleQA,30.9353
claude-opus-4,Claude Opus 4,swe_bench_pro,SWE-bench Pro,32.4243
claude-opus-4,Claude Opus 4,tau_bench_retail,Tau-Bench Retail,82.6661
claude-opus-4,Claude Opus 4,terminal_bench,Terminal-Bench 2.0,30.2493
claude-opus-4.1,Claude Opus 4.1,frontiermath,FrontierMath,-9.467
claude-opus-4.1,Claude Opus 4.1,gpqa_diamond,GPQA Diamond,64.8918
claude-opus-4.1,Claude Opus 4.1,hle,HLE (Humanity's Last Exam),-19.3593
claude-opus-4.1,Claude Opus 4.1,humaneval,HumanEval,73.0439
claude-opus-4.1,Claude Opus 4.1,mmlu,MMLU,85.3355
claude-opus-4.1,Claude Opus 4.1,mmlu_pro,MMLU-Pro,84.2553
claude-opus-4.1,Claude Opus 4.1,simpleqa,SimpleQA,15.3033
claude-opus-4.1,Claude Opus 4.1,swe_bench_pro,SWE-bench Pro,44.4439
claude-opus-4.1,Claude Opus 4.1,terminal_bench,Terminal-Bench 2.0,10.2414
claude-opus-4.1,Claude Opus 4.1,terminal_bench_1,Terminal-Bench 1.0,8.7202
deepseek-r1-0528,DeepSeek-R1-0528,aime_2024,AIME 2024,88.2588
deepseek-r1-0528,DeepSeek-R1-0528,aime_2025,AIME 2025,85.3205
deepseek-r1-0528,DeepSeek-R1-0528,arc_agi_1,ARC-AGI-1,43.2069
deepseek-r1-0528,DeepSeek-R1-0528,brumo_2025,BRUMO 2025,86.1087
deepseek-r1-0528,DeepSeek-R1-0528,codeforces_rating,Codeforces Rating,2026.1371
deepseek-r1-0528,DeepSeek-R1-0528,gpqa_diamond,GPQA Diamond,80.8844
deepseek-r1-0528,DeepSeek-R1-0528,hle,HLE (Humanity's Last Exam),24.1357
deepseek-r1-0528,DeepSeek-R1-0528,humaneval,HumanEval,88.8596
deepseek-r1-0528,DeepSeek-R1-0528,ifeval,IFEval,88.9788
deepseek-r1-0528,DeepSeek-R1-0528,imo_2025,IMO 2025,49.4535
deepseek-r1-0528,DeepSeek-R1-0528,livecodebench,LiveCodeBench,72.4672
deepseek-r1-0528,DeepSeek-R1-0528,math_500,MATH-500,98.0249
deepseek-r1-0528,DeepSeek-R1-0528,matharena_apex_2025,MathArena Apex 2025,-4.5989
deepseek-r1-0528,DeepSeek-R1-0528,mmlu,MMLU,88.1327
deepseek-r1-0528,DeepSeek-R1-0528,mmlu_pro,MMLU-Pro,82.9089
deepseek-r1-0528,DeepSeek-R1-0528,simpleqa,SimpleQA,43.8461
deepseek-r1-0528,DeepSeek-R1-0528,smt_2025,SMT 2025,82.8698
deepseek-r1-0528,DeepSeek-R1-0528,swe_bench_verified,SWE-bench Verified,65.4054
deepseek-r1-0528,DeepSeek-R1-0528,usamo_2025,USAMO 2025,26.3332
doubao-seed-2.0-pro,Doubao Seed 2.0 Pro,aime_2024,AIME 2024,102.485
doubao-seed-2.0-pro,Doubao Seed 2.0 Pro,aime_2025,AIME 2025,102.7384
doubao-seed-2.0-pro,Doubao Seed 2.0 Pro,browsecomp,BrowseComp,79.8526
doubao-seed-2.0-pro,Doubao Seed 2.0 Pro,gpqa_diamond,GPQA Diamond,93.7236
doubao-seed-2.0-pro,Doubao Seed 2.0 Pro,hle,HLE (Humanity's Last Exam),40.2886
doubao-seed-2.0-pro,Doubao Seed 2.0 Pro,ifeval,IFEval,92.9732
doubao-seed-2.0-pro,Doubao Seed 2.0 Pro,mathvision,MathVision,52.8856
doubao-seed-2.0-pro,Doubao Seed 2.0 Pro,mmlu,MMLU,88.3778
doubao-seed-2.0-pro,Doubao Seed 2.0 Pro,mmmu,MMMU,84.5588
doubao-seed-2.0-pro,Doubao Seed 2.0 Pro,swe_bench_verified,SWE-bench Verified,81.0555
doubao-seed-2.0-pro,Doubao Seed 2.0 Pro,terminal_bench,Terminal-Bench 2.0,51.5181
gemini-2.5-pro,Gemini 2.5 Pro,aa_intelligence_index,AA Intelligence Index,71.7824
gemini-2.5-pro,Gemini 2.5 Pro,aime_2024,AIME 2024,84.899
gemini-2.5-pro,Gemini 2.5 Pro,aime_2025,AIME 2025,78.7873
gemini-2.5-pro,Gemini 2.5 Pro,arc_agi_1,ARC-AGI-1,43.2916
gemini-2.5-pro,Gemini 2.5 Pro,arc_agi_2,ARC-AGI-2,9.0823
gemini-2.5-pro,Gemini 2.5 Pro,arena_hard,Arena-Hard Auto,84.7448
gemini-2.5-pro,Gemini 2.5 Pro,brumo_2025,BRUMO 2025,85.3322
gemini-2.5-pro,Gemini 2.5 Pro,chatbot_arena_elo,Chatbot Arena Elo,1428.5642
gemini-2.5-pro,Gemini 2.5 Pro,codeforces_rating,Codeforces Rating,1995.2619
gemini-2.5-pro,Gemini 2.5 Pro,critpt,CritPt,2.3085
gemini-2.5-pro,Gemini 2.5 Pro,frontiermath,FrontierMath,0.9589
gemini-2.5-pro,Gemini 2.5 Pro,gpqa_diamond,GPQA Diamond,78.4805
gemini-2.5-pro,Gemini 2.5 Pro,hle,HLE (Humanity's Last Exam),18.384
gemini-2.5-pro,Gemini 2.5 Pro,hmmt_2025,HMMT Feb 2025,65.0055
gemini-2.5-pro,Gemini 2.5 Pro,hmmt_nov_2025,HMMT Nov 2025,83.2566
gemini-2.5-pro,Gemini 2.5 Pro,imo_2025,IMO 2025,-17.6788
gemini-2.5-pro,Gemini 2.5 Pro,livebench,LiveBench,49.1506
gemini-2.5-pro,Gemini 2.5 Pro,math_500,MATH-500,96.9228
gemini-2.5-pro,Gemini 2.5 Pro,matharena_apex_2025,MathArena Apex 2025,-4.3615
gemini-2.5-pro,Gemini 2.5 Pro,mmlu,MMLU,87.5773
gemini-2.5-pro,Gemini 2.5 Pro,mmlu_pro,MMLU-Pro,81.5759
gemini-2.5-pro,Gemini 2.5 Pro,mmmu,MMMU,76.1813
gemini-2.5-pro,Gemini 2.5 Pro,mrcr_v2,MRCR v2,33.959
gemini-2.5-pro,Gemini 2.5 Pro,simplebench,SimpleBench,53.4043
gemini-2.5-pro,Gemini 2.5 Pro,simpleqa,SimpleQA,34.0021
gemini-2.5-pro,Gemini 2.5 Pro,smt_2025,SMT 2025,78.054
gemini-2.5-pro,Gemini 2.5 Pro,swe_bench_verified,SWE-bench Verified,65.3161
gemini-2.5-pro,Gemini 2.5 Pro,terminal_bench_1,Terminal-Bench 1.0,28.4571
gemini-2.5-pro,Gemini 2.5 Pro,usamo_2025,USAMO 2025,16.0045
gemini-2.5-pro,Gemini 2.5 Pro,video_mmu,Video-MMU,84.7825
gpt-4.1,GPT-4.1,aime_2025,AIME 2025,62.2577
gpt-4.1,GPT-4.1,arc_agi_1,ARC-AGI-1,-19.6593
gpt-4.1,GPT-4.1,arena_hard,Arena-Hard Auto,59.9498
gpt-4.1,GPT-4.1,codeforces_rating,Codeforces Rating,695.9575
gpt-4.1,GPT-4.1,hle,HLE (Humanity's Last Exam),0.5921
gpt-4.1,GPT-4.1,hmmt_2025,HMMT Feb 2025,50.7226
gpt-4.1,GPT-4.1,humaneval,HumanEval,81.3828
gpt-4.1,GPT-4.1,ifeval,IFEval,75.7265
gpt-4.1,GPT-4.1,livecodebench,LiveCodeBench,49.4331
gpt-4.1,GPT-4.1,math_500,MATH-500,89.8263
gpt-4.1,GPT-4.1,mmlu,MMLU,90.5779
gpt-4.1,GPT-4.1,mmmu,MMMU,66.9282
gpt-4.1,GPT-4.1,simpleqa,SimpleQA,35.5484
gpt-4.1,GPT-4.1,swe_bench_verified,SWE-bench Verified,43.6935
gpt-4.1,GPT-4.1,terminal_bench_1,Terminal-Bench 1.0,13.0037
grok-3-beta,Grok 3 Beta,aime_2024,AIME 2024,91.5659
grok-3-beta,Grok 3 Beta,aime_2025,AIME 2025,85.0303
grok-3-beta,Grok 3 Beta,arc_agi_1,ARC-AGI-1,38.8568
grok-3-beta,Grok 3 Beta,chatbot_arena_elo,Chatbot Arena Elo,1472.0659
grok-3-beta,Grok 3 Beta,hle,HLE (Humanity's Last Exam),19.1915
grok-3-beta,Grok 3 Beta,humaneval,HumanEval,92.5539
grok-3-beta,Grok 3 Beta,ifeval,IFEval,90.3897
grok-3-beta,Grok 3 Beta,mmlu_pro,MMLU-Pro,82.1437
grok-3-beta,Grok 3 Beta,simpleqa,SimpleQA,32.6593
grok-3-beta,Grok 3 Beta,swe_bench_verified,SWE-bench Verified,64.9693
grok-4,Grok 4,aa_intelligence_index,AA Intelligence Index,60.5584
grok-4,Grok 4,aa_lcr,AA Long Context Reasoning,72.3294
grok-4,Grok 4,aime_2024,AIME 2024,95.387
grok-4,Grok 4,arc_agi_2,ARC-AGI-2,24.7942
grok-4,Grok 4,brumo_2025,BRUMO 2025,94.0285
grok-4,Grok 4,chatbot_arena_elo,Chatbot Arena Elo,1420.5921
grok-4,Grok 4,cmimc_2025,CMIMC 2025,79.6955
grok-4,Grok 4,codeforces_rating,Codeforces Rating,2126.8875
grok-4,Grok 4,frontiermath,FrontierMath,22.8744
grok-4,Grok 4,gpqa_diamond,GPQA Diamond,85.8077
grok-4,Grok 4,hle,HLE (Humanity's Last Exam),31.4765
grok-4,Grok 4,hmmt_nov_2025,HMMT Nov 2025,87.6383
grok-4,Grok 4,ifeval,IFEval,87.3578
grok-4,Grok 4,imo_2025,IMO 2025,45.9686
grok-4,Grok 4,livecodebench,LiveCodeBench,75.8387
grok-4,Grok 4,matharena_apex_2025,MathArena Apex 2025,9.8747
grok-4,Grok 4,mmlu,MMLU,88.2798
grok-4,Grok 4,mmlu_pro,MMLU-Pro,85.692
grok-4,Grok 4,mmmu,MMMU,79.5878
grok-4,Grok 4,mmmu_pro,MMMU-Pro,72.4192
grok-4,Grok 4,osworld,OSWorld,57.8092
grok-4,Grok 4,simpleqa,SimpleQA,50.8708
grok-4,Grok 4,smt_2025,SMT 2025,87.2396
grok-4,Grok 4,swe_bench_pro,SWE-bench Pro,42.3317
grok-4,Grok 4,swe_bench_verified,SWE-bench Verified,73.5675
grok-4,Grok 4,terminal_bench,Terminal-Bench 2.0,45.1135
grok-4,Grok 4,terminal_bench_1,Terminal-Bench 1.0,39.256
grok-4,Grok 4,usamo_2025,USAMO 2025,42.0282
kimi-k2,Kimi K2,aime_2024,AIME 2024,77.07
kimi-k2,Kimi K2,aime_2025,AIME 2025,77.7713
kimi-k2,Kimi K2,arena_hard,Arena-Hard Auto,82.0676
kimi-k2,Kimi K2,gpqa_diamond,GPQA Diamond,78.7722
kimi-k2,Kimi K2,hle,HLE (Humanity's Last Exam),17.0847
kimi-k2,Kimi K2,hmmt_2025,HMMT Feb 2025,69.8373
kimi-k2,Kimi K2,ifeval,IFEval,85.3297
kimi-k2,Kimi K2,livecodebench,LiveCodeBench,73.2168
kimi-k2,Kimi K2,math_500,MATH-500,98.4135
kimi-k2,Kimi K2,mmlu_pro,MMLU-Pro,84.8066
kimi-k2,Kimi K2,osworld,OSWorld,42.0434
kimi-k2,Kimi K2,simpleqa,SimpleQA,40.7269
kimi-k2,Kimi K2,swe_bench_verified,SWE-bench Verified,63.4499
llama-4-maverick,Llama 4 Maverick,arc_agi_1,ARC-AGI-1,12.2776
llama-4-maverick,Llama 4 Maverick,bigcodebench,BigCodeBench,48.2056
llama-4-maverick,Llama 4 Maverick,chatbot_arena_elo,Chatbot Arena Elo,1390.5651
llama-4-maverick,Llama 4 Maverick,gpqa_diamond,GPQA Diamond,68.0575
llama-4-maverick,Llama 4 Maverick,humaneval,HumanEval,87.3471
llama-4-maverick,Llama 4 Maverick,livecodebench,LiveCodeBench,52.8035
llama-4-maverick,Llama 4 Maverick,math_500,MATH-500,91.3417
llama-4-maverick,Llama 4 Maverick,mmmu,MMMU,74.0947
llama-4-maverick,Llama 4 Maverick,simpleqa,SimpleQA,30.8074
llama-4-maverick,Llama 4 Maverick,swe_bench_verified,SWE-bench Verified,50.8737
llama-4-maverick,Llama 4 Maverick,terminal_bench_1,Terminal-Bench 1.0,18.6748
minimax-m2,MiniMax-M2,aime_2024,AIME 2024,85.3286
minimax-m2,MiniMax-M2,aime_2025,AIME 2025,86.8343
minimax-m2,MiniMax-M2,chatbot_arena_elo,Chatbot Arena Elo,1550.7795
minimax-m2,MiniMax-M2,gpqa_diamond,GPQA Diamond,73.6824
minimax-m2,MiniMax-M2,hle,HLE (Humanity's Last Exam),29.0842
minimax-m2,MiniMax-M2,humaneval,HumanEval,91.3383
minimax-m2,MiniMax-M2,ifeval,IFEval,89.4475
minimax-m2,MiniMax-M2,math_500,MATH-500,101.6867
minimax-m2,MiniMax-M2,mmlu,MMLU,90.6399
minimax-m2,MiniMax-M2,mmlu_pro,MMLU-Pro,82.7343
minimax-m2,MiniMax-M2,mmmu,MMMU,77.0941
minimax-m2,MiniMax-M2,simpleqa,SimpleQA,47.3133
minimax-m2,MiniMax-M2,swe_bench_pro,SWE-bench Pro,34.4201
minimax-m2,MiniMax-M2,swe_bench_verified,SWE-bench Verified,69.6096
minimax-m2,MiniMax-M2,terminal_bench,Terminal-Bench 2.0,32.9692
o3-mini-high,o3-mini (high),aime_2024,AIME 2024,87.026
o3-mini-high,o3-mini (high),arc_agi_1,ARC-AGI-1,49.9194
o3-mini-high,o3-mini (high),arc_agi_2,ARC-AGI-2,23.0384
o3-mini-high,o3-mini (high),arena_hard,Arena-Hard Auto,88.1615
o3-mini-high,o3-mini (high),chatbot_arena_elo,Chatbot Arena Elo,1419.464
o3-mini-high,o3-mini (high),frontiermath,FrontierMath,17.7421
o3-mini-high,o3-mini (high),hle,HLE (Humanity's Last Exam),24.6857
o3-mini-high,o3-mini (high),hmmt_2025,HMMT Feb 2025,77.9776
o3-mini-high,o3-mini (high),humaneval,HumanEval,88.9628
o3-mini-high,o3-mini (high),ifeval,IFEval,87.1983
o3-mini-high,o3-mini (high),livecodebench,LiveCodeBench,71.2882
o3-mini-high,o3-mini (high),math_500,MATH-500,94.854
o3-mini-high,o3-mini (high),simpleqa,SimpleQA,41.5753
o3-mini-high,o3-mini (high),swe_bench_verified,SWE-bench Verified,65.7936
o3-mini-high,o3-mini (high),usamo_2025,USAMO 2025,25.1191
