{
  "data_discovery": {
    "raw_schema": "JSON with top-level keys: 'models' (list of 83 dicts with id/name/provider/params/architecture/is_reasoning/open_weights), 'benchmarks' (list of 49 dicts with id/name/category/metric/num_problems/source_url), 'scores' (list of 1390 dicts with model_id/benchmark_id/score/reference_url), 'generated' (timestamp string).",
    "extraction_decisions": "Mapped scores to (model_id, benchmark_id) -> score. Found 15 duplicate pairs (e.g., deepseek-r1-distill variants with multiple scores on same benchmark); resolved by simple averaging per canonical evaluation protocol. All score values are numeric floats. Used model 'id' as primary key and benchmark 'id' as primary key. No joins needed \u2014 flat score list with foreign keys. Benchmark names used as column headers in output matrices.",
    "n_models_raw": 83,
    "n_benchmarks_raw": 49
  },
  "data": {
    "n_models": 80,
    "n_benchmarks": 35,
    "missing_fraction": 0.5425,
    "preprocessing": "Filtered to benchmarks with >= 10 models scored (35/49 kept) and models with >= 5 benchmarks in filtered set (80/83 kept). For SVD: missing values imputed with per-benchmark column means, then z-scored (centered, unit variance per benchmark). No log/logit transform applied. Raw scores used for prediction outputs. Note: benchmarks use heterogeneous scales \u2014 most are '% correct' (0-100) but Codeforces Rating (~800-2100 Elo) and Chatbot Arena Elo (~1100-1400) are on different scales. Z-scoring handles this for decomposition but raw MAE is dominated by these outlier-scale benchmarks.",
    "benchmarks_used": [
      "AIME 2024",
      "AIME 2025",
      "ARC-AGI-1",
      "ARC-AGI-2",
      "Arena-Hard Auto",
      "BrowseComp",
      "BRUMO 2025",
      "Chatbot Arena Elo",
      "CMIMC 2025",
      "Codeforces Rating",
      "CritPt",
      "FrontierMath",
      "GPQA Diamond",
      "GSM8K",
      "HLE (Humanity's Last Exam)",
      "HMMT Feb 2025",
      "HMMT Nov 2025",
      "HumanEval",
      "IFEval",
      "LiveBench",
      "LiveCodeBench",
      "MATH-500",
      "MathArena Apex 2025",
      "MMLU",
      "MMLU-Pro",
      "MMMU",
      "MMMU-Pro",
      "OSWorld",
      "SimpleQA",
      "SMT 2025",
      "SWE-bench Pro",
      "SWE-bench Verified",
      "Tau-Bench Retail",
      "Terminal-Bench 2.0",
      "Terminal-Bench 1.0"
    ]
  },
  "rank_analysis": {
    "method": "SVD on z-scored mean-imputed filtered matrix (80 models x 35 benchmarks)",
    "effective_rank": 15,
    "variance_explained_by_rank": 0.9016,
    "singular_values": [
      22.3149,
      13.8272,
      8.7854,
      8.2368,
      7.2451,
      6.5667,
      6.2498,
      5.8444,
      5.4941,
      5.1403,
      4.7625,
      4.6567,
      4.3415,
      4.0558,
      3.9625,
      3.7203,
      3.679,
      3.2095,
      3.1861,
      3.0661
    ],
    "justification": "Using 90% cumulative variance threshold, effective rank = 15. The first component alone explains 38.9% (strong dominant factor reflecting overall model capability). First 3 components explain 59.8%, first 5 explain 69.2%. The relatively high effective rank (15 for 90%) reflects the impact of mean-imputation on a 54% missing matrix inflating the tail of the spectrum, plus genuine heterogeneity across benchmark categories (coding, math, reasoning, agentic tasks have partially independent difficulty axes). The 80% threshold gives rank 9."
  },
  "benchmark_selection": {
    "method": "greedy_forward_selection",
    "selected_benchmarks": [
      "Codeforces Rating",
      "Chatbot Arena Elo",
      "LiveCodeBench",
      "HMMT Feb 2025",
      "ARC-AGI-2"
    ],
    "n_selected": 5,
    "selection_criterion": "Greedy forward selection minimizing training MAE with ridge regression (alpha=1.0) predicting all non-selected benchmarks from the selected subset. Evaluated on the mean-imputed filtered matrix. Selected 5 benchmarks (matching canonical REVEAL_K=5). The selected set spans coding (Codeforces Rating, LiveCodeBench), math (HMMT Feb 2025), reasoning (ARC-AGI-2), and human preference (Chatbot Arena Elo) \u2014 a diverse coverage of benchmark categories."
  },
  "prediction": {
    "method": "Low-rank projection: rank-3 SVD basis fitted on z-scored mean-imputed matrix. For each model, observed benchmark z-scores are projected onto the rank-3 basis via ridge-regularized least squares, then all benchmarks are predicted by reconstructing from the low-rank coordinates. For canonical evaluation: per-target ridge regression from revealed benchmarks to each held-out benchmark, trained on all other models with complete observations on the relevant benchmarks.",
    "overall_mae": 21.335,
    "per_benchmark_mae": {
      "AIME 2024": 16.097,
      "AIME 2025": 12.381,
      "ARC-AGI-1": 19.73,
      "ARC-AGI-2": 11.268,
      "Arena-Hard Auto": 18.023,
      "BrowseComp": 10.438,
      "BRUMO 2025": 4.855,
      "Chatbot Arena Elo": 28.551,
      "CMIMC 2025": 8.984,
      "Codeforces Rating": 301.438,
      "CritPt": 3.876,
      "FrontierMath": 8.678,
      "GPQA Diamond": 9.451,
      "GSM8K": 5.169,
      "HLE (Humanity's Last Exam)": 8.121,
      "HMMT Feb 2025": 19.686,
      "HMMT Nov 2025": 3.867,
      "HumanEval": 5.252,
      "IFEval": 4.334,
      "LiveBench": 7.824,
      "LiveCodeBench": 11.06,
      "MATH-500": 4.794,
      "MathArena Apex 2025": 6.583,
      "MMLU": 4.194,
      "MMLU-Pro": 6.487,
      "MMMU": 4.433,
      "MMMU-Pro": 5.437,
      "OSWorld": 10.015,
      "SimpleQA": 10.291,
      "SMT 2025": 5.473,
      "SWE-bench Pro": 6.756,
      "SWE-bench Verified": 11.392,
      "Tau-Bench Retail": 4.146,
      "Terminal-Bench 2.0": 10.854,
      "Terminal-Bench 1.0": 11.212
    },
    "evaluation_protocol": "Leave-one-out per entry on the filtered matrix (80x35). For each model, each observed entry is held out in turn, the remaining observed entries are used to project onto the rank-3 SVD basis, and the held-out entry is predicted. 1281 total LOO evaluations.",
    "n_predictor_benchmarks": 5,
    "achieves_mae_under_5": false
  },
  "methodology_notes": "Key methodological choices and caveats: (1) Filtering: dropped 14 sparse benchmarks (<10 models) and 3 sparse models (<5 benchmarks in filtered set) to reduce the impact of extreme sparsity on SVD. (2) Imputation: simple column-mean imputation before SVD \u2014 this is known to bias singular values downward for components beyond the first, potentially inflating effective rank estimates. Iterative/EM imputation would be more principled but wasn't used for simplicity. (3) Scale heterogeneity: Codeforces Rating and Chatbot Arena Elo are on Elo-like scales (1000+), while most benchmarks are percentages (0-100). Z-scoring handles this for SVD but raw MAE is dominated by these benchmarks (Codeforces MAE=301 raw). The canonical evaluation uses normalized 0-100 scale which resolves this. (4) Rank choice: rank-3 for LOO prediction is aggressive (only 60% variance); higher rank would reduce bias but increase variance given the sparsity. (5) Canonical prediction uses a simpler per-benchmark ridge approach rather than the global low-rank projection, because the revealed benchmark set varies per model and may not overlap well with the SVD basis benchmarks. (6) The 54% missing fraction in the filtered matrix is substantial \u2014 this is not a 'slightly incomplete' matrix but a genuinely sparse one, limiting the reliability of any global decomposition."
}