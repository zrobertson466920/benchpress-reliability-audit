model_id,model_name,benchmark_id,benchmark_name,y_pred
claude-opus-4,Claude Opus 4,aime_2024,AIME 2024,69.4246
claude-opus-4,Claude Opus 4,aime_2025,AIME 2025,77.1333
claude-opus-4,Claude Opus 4,arc_agi_1,ARC-AGI-1,43.6032
claude-opus-4,Claude Opus 4,arc_agi_2,ARC-AGI-2,16.7281
claude-opus-4,Claude Opus 4,codeforces_rating,Codeforces Rating,1965.4902
claude-opus-4,Claude Opus 4,critpt,CritPt,5.7182
claude-opus-4,Claude Opus 4,frontiermath,FrontierMath,22.881
claude-opus-4,Claude Opus 4,gpqa_diamond,GPQA Diamond,68.8371
claude-opus-4,Claude Opus 4,hle,HLE (Humanity's Last Exam),26.0003
claude-opus-4,Claude Opus 4,hmmt_2025,HMMT Feb 2025,68.3172
claude-opus-4,Claude Opus 4,ifeval,IFEval,84.1399
claude-opus-4,Claude Opus 4,livecodebench,LiveCodeBench,59.6105
claude-opus-4,Claude Opus 4,math_500,MATH-500,91.3529
claude-opus-4,Claude Opus 4,mmlu,MMLU,84.8496
claude-opus-4,Claude Opus 4,mmmu,MMMU,78.5279
claude-opus-4,Claude Opus 4,simpleqa,SimpleQA,38.4958
claude-opus-4,Claude Opus 4,swe_bench_pro,SWE-bench Pro,42.29
claude-opus-4,Claude Opus 4,tau_bench_retail,Tau-Bench Retail,84.9545
claude-opus-4,Claude Opus 4,terminal_bench,Terminal-Bench 2.0,40.3852
claude-opus-4.1,Claude Opus 4.1,frontiermath,FrontierMath,23.9
claude-opus-4.1,Claude Opus 4.1,gpqa_diamond,GPQA Diamond,42.9534
claude-opus-4.1,Claude Opus 4.1,hle,HLE (Humanity's Last Exam),3.7847
claude-opus-4.1,Claude Opus 4.1,humaneval,HumanEval,80.6764
claude-opus-4.1,Claude Opus 4.1,mmlu,MMLU,85.1377
claude-opus-4.1,Claude Opus 4.1,mmlu_pro,MMLU-Pro,87.9687
claude-opus-4.1,Claude Opus 4.1,simpleqa,SimpleQA,31.9299
claude-opus-4.1,Claude Opus 4.1,swe_bench_pro,SWE-bench Pro,40.2864
claude-opus-4.1,Claude Opus 4.1,terminal_bench,Terminal-Bench 2.0,-93.3149
claude-opus-4.1,Claude Opus 4.1,terminal_bench_1,Terminal-Bench 1.0,27.4
deepseek-r1-0528,DeepSeek-R1-0528,aime_2024,AIME 2024,92.5198
deepseek-r1-0528,DeepSeek-R1-0528,aime_2025,AIME 2025,90.6099
deepseek-r1-0528,DeepSeek-R1-0528,arc_agi_1,ARC-AGI-1,48.8327
deepseek-r1-0528,DeepSeek-R1-0528,brumo_2025,BRUMO 2025,88.5014
deepseek-r1-0528,DeepSeek-R1-0528,codeforces_rating,Codeforces Rating,2244.041
deepseek-r1-0528,DeepSeek-R1-0528,gpqa_diamond,GPQA Diamond,84.3815
deepseek-r1-0528,DeepSeek-R1-0528,hle,HLE (Humanity's Last Exam),22.8205
deepseek-r1-0528,DeepSeek-R1-0528,humaneval,HumanEval,89.8428
deepseek-r1-0528,DeepSeek-R1-0528,ifeval,IFEval,91.4749
deepseek-r1-0528,DeepSeek-R1-0528,imo_2025,IMO 2025,46.4085
deepseek-r1-0528,DeepSeek-R1-0528,livecodebench,LiveCodeBench,77.5209
deepseek-r1-0528,DeepSeek-R1-0528,math_500,MATH-500,99.2867
deepseek-r1-0528,DeepSeek-R1-0528,matharena_apex_2025,MathArena Apex 2025,-8.6216
deepseek-r1-0528,DeepSeek-R1-0528,mmlu,MMLU,89.4588
deepseek-r1-0528,DeepSeek-R1-0528,mmlu_pro,MMLU-Pro,84.6191
deepseek-r1-0528,DeepSeek-R1-0528,simpleqa,SimpleQA,45.4634
deepseek-r1-0528,DeepSeek-R1-0528,smt_2025,SMT 2025,86.3831
deepseek-r1-0528,DeepSeek-R1-0528,swe_bench_verified,SWE-bench Verified,67.5668
deepseek-r1-0528,DeepSeek-R1-0528,usamo_2025,USAMO 2025,19.6557
doubao-seed-2.0-pro,Doubao Seed 2.0 Pro,aime_2024,AIME 2024,102.9379
doubao-seed-2.0-pro,Doubao Seed 2.0 Pro,aime_2025,AIME 2025,99.0458
doubao-seed-2.0-pro,Doubao Seed 2.0 Pro,browsecomp,BrowseComp,83.6285
doubao-seed-2.0-pro,Doubao Seed 2.0 Pro,gpqa_diamond,GPQA Diamond,94.4988
doubao-seed-2.0-pro,Doubao Seed 2.0 Pro,hle,HLE (Humanity's Last Exam),39.801
doubao-seed-2.0-pro,Doubao Seed 2.0 Pro,ifeval,IFEval,94.7053
doubao-seed-2.0-pro,Doubao Seed 2.0 Pro,mathvision,MathVision,81.4333
doubao-seed-2.0-pro,Doubao Seed 2.0 Pro,mmlu,MMLU,87.6243
doubao-seed-2.0-pro,Doubao Seed 2.0 Pro,mmmu,MMMU,89.1697
doubao-seed-2.0-pro,Doubao Seed 2.0 Pro,swe_bench_verified,SWE-bench Verified,79.267
doubao-seed-2.0-pro,Doubao Seed 2.0 Pro,terminal_bench,Terminal-Bench 2.0,54.3801
gemini-2.5-pro,Gemini 2.5 Pro,aa_intelligence_index,AA Intelligence Index,61.8571
gemini-2.5-pro,Gemini 2.5 Pro,aime_2024,AIME 2024,69.1541
gemini-2.5-pro,Gemini 2.5 Pro,aime_2025,AIME 2025,77.1883
gemini-2.5-pro,Gemini 2.5 Pro,arc_agi_1,ARC-AGI-1,43.4323
gemini-2.5-pro,Gemini 2.5 Pro,arc_agi_2,ARC-AGI-2,16.8438
gemini-2.5-pro,Gemini 2.5 Pro,arena_hard,Arena-Hard Auto,61.1639
gemini-2.5-pro,Gemini 2.5 Pro,brumo_2025,BRUMO 2025,92.0822
gemini-2.5-pro,Gemini 2.5 Pro,chatbot_arena_elo,Chatbot Arena Elo,1428.4348
gemini-2.5-pro,Gemini 2.5 Pro,codeforces_rating,Codeforces Rating,1963.2353
gemini-2.5-pro,Gemini 2.5 Pro,critpt,CritPt,5.5636
gemini-2.5-pro,Gemini 2.5 Pro,frontiermath,FrontierMath,23.119
gemini-2.5-pro,Gemini 2.5 Pro,gpqa_diamond,GPQA Diamond,68.8284
gemini-2.5-pro,Gemini 2.5 Pro,hle,HLE (Humanity's Last Exam),25.6841
gemini-2.5-pro,Gemini 2.5 Pro,hmmt_2025,HMMT Feb 2025,66.0207
gemini-2.5-pro,Gemini 2.5 Pro,hmmt_nov_2025,HMMT Nov 2025,91.5145
gemini-2.5-pro,Gemini 2.5 Pro,imo_2025,IMO 2025,28.5183
gemini-2.5-pro,Gemini 2.5 Pro,livebench,LiveBench,61.2545
gemini-2.5-pro,Gemini 2.5 Pro,math_500,MATH-500,91.3132
gemini-2.5-pro,Gemini 2.5 Pro,matharena_apex_2025,MathArena Apex 2025,8.2367
gemini-2.5-pro,Gemini 2.5 Pro,mmlu,MMLU,84.8354
gemini-2.5-pro,Gemini 2.5 Pro,mmlu_pro,MMLU-Pro,76.2146
gemini-2.5-pro,Gemini 2.5 Pro,mmmu,MMMU,78.3486
gemini-2.5-pro,Gemini 2.5 Pro,mrcr_v2,MRCR v2,71.1
gemini-2.5-pro,Gemini 2.5 Pro,simplebench,SimpleBench,67.7
gemini-2.5-pro,Gemini 2.5 Pro,simpleqa,SimpleQA,37.8688
gemini-2.5-pro,Gemini 2.5 Pro,smt_2025,SMT 2025,85.125
gemini-2.5-pro,Gemini 2.5 Pro,swe_bench_verified,SWE-bench Verified,61.6509
gemini-2.5-pro,Gemini 2.5 Pro,terminal_bench_1,Terminal-Bench 1.0,28.4882
gemini-2.5-pro,Gemini 2.5 Pro,usamo_2025,USAMO 2025,20.5214
gemini-2.5-pro,Gemini 2.5 Pro,video_mmu,Video-MMU,85.3
gpt-4.1,GPT-4.1,aime_2025,AIME 2025,77.4459
gpt-4.1,GPT-4.1,arc_agi_1,ARC-AGI-1,12.5643
gpt-4.1,GPT-4.1,arena_hard,Arena-Hard Auto,61.9778
gpt-4.1,GPT-4.1,codeforces_rating,Codeforces Rating,1149.2446
gpt-4.1,GPT-4.1,hle,HLE (Humanity's Last Exam),10.3541
gpt-4.1,GPT-4.1,hmmt_2025,HMMT Feb 2025,65.0589
gpt-4.1,GPT-4.1,humaneval,HumanEval,88.5503
gpt-4.1,GPT-4.1,ifeval,IFEval,84.133
gpt-4.1,GPT-4.1,livecodebench,LiveCodeBench,39.94
gpt-4.1,GPT-4.1,math_500,MATH-500,95.2627
gpt-4.1,GPT-4.1,mmlu,MMLU,91.8171
gpt-4.1,GPT-4.1,mmmu,MMMU,58.5622
gpt-4.1,GPT-4.1,simpleqa,SimpleQA,49.4654
gpt-4.1,GPT-4.1,swe_bench_verified,SWE-bench Verified,33.369
gpt-4.1,GPT-4.1,terminal_bench_1,Terminal-Bench 1.0,28.1941
grok-3-beta,Grok 3 Beta,aime_2024,AIME 2024,100.9596
grok-3-beta,Grok 3 Beta,aime_2025,AIME 2025,94.2755
grok-3-beta,Grok 3 Beta,arc_agi_1,ARC-AGI-1,48.3595
grok-3-beta,Grok 3 Beta,chatbot_arena_elo,Chatbot Arena Elo,1529.6603
grok-3-beta,Grok 3 Beta,hle,HLE (Humanity's Last Exam),21.7274
grok-3-beta,Grok 3 Beta,humaneval,HumanEval,92.5725
grok-3-beta,Grok 3 Beta,ifeval,IFEval,89.0865
grok-3-beta,Grok 3 Beta,mmlu_pro,MMLU-Pro,82.1766
grok-3-beta,Grok 3 Beta,simpleqa,SimpleQA,32.1299
grok-3-beta,Grok 3 Beta,swe_bench_verified,SWE-bench Verified,69.4751
grok-4,Grok 4,aa_intelligence_index,AA Intelligence Index,61.4286
grok-4,Grok 4,aa_lcr,AA Long Context Reasoning,73.0795
grok-4,Grok 4,aime_2024,AIME 2024,94.9522
grok-4,Grok 4,arc_agi_2,ARC-AGI-2,20.9808
grok-4,Grok 4,brumo_2025,BRUMO 2025,95.2635
grok-4,Grok 4,chatbot_arena_elo,Chatbot Arena Elo,1458.2926
grok-4,Grok 4,cmimc_2025,CMIMC 2025,78.2879
grok-4,Grok 4,codeforces_rating,Codeforces Rating,2265.7177
grok-4,Grok 4,frontiermath,FrontierMath,21.8051
grok-4,Grok 4,gpqa_diamond,GPQA Diamond,87.4617
grok-4,Grok 4,hle,HLE (Humanity's Last Exam),29.8079
grok-4,Grok 4,hmmt_nov_2025,HMMT Nov 2025,90.7944
grok-4,Grok 4,ifeval,IFEval,89.1915
grok-4,Grok 4,imo_2025,IMO 2025,28.9881
grok-4,Grok 4,livecodebench,LiveCodeBench,75.5255
grok-4,Grok 4,matharena_apex_2025,MathArena Apex 2025,8.8646
grok-4,Grok 4,mmlu,MMLU,89.9568
grok-4,Grok 4,mmlu_pro,MMLU-Pro,86.749
grok-4,Grok 4,mmmu,MMMU,81.4992
grok-4,Grok 4,mmmu_pro,MMMU-Pro,73.2857
grok-4,Grok 4,osworld,OSWorld,56.4003
grok-4,Grok 4,simpleqa,SimpleQA,52.7281
grok-4,Grok 4,smt_2025,SMT 2025,88.7306
grok-4,Grok 4,swe_bench_pro,SWE-bench Pro,42.1668
grok-4,Grok 4,swe_bench_verified,SWE-bench Verified,75.3735
grok-4,Grok 4,terminal_bench,Terminal-Bench 2.0,43.351
grok-4,Grok 4,terminal_bench_1,Terminal-Bench 1.0,35.7751
grok-4,Grok 4,usamo_2025,USAMO 2025,31.6964
kimi-k2,Kimi K2,aime_2024,AIME 2024,75.3856
kimi-k2,Kimi K2,aime_2025,AIME 2025,81.0637
kimi-k2,Kimi K2,arena_hard,Arena-Hard Auto,62.1722
kimi-k2,Kimi K2,gpqa_diamond,GPQA Diamond,80.6664
kimi-k2,Kimi K2,hle,HLE (Humanity's Last Exam),17.0291
kimi-k2,Kimi K2,hmmt_2025,HMMT Feb 2025,78.2184
kimi-k2,Kimi K2,ifeval,IFEval,85.0268
kimi-k2,Kimi K2,livecodebench,LiveCodeBench,79.8181
kimi-k2,Kimi K2,math_500,MATH-500,97.162
kimi-k2,Kimi K2,mmlu_pro,MMLU-Pro,85.875
kimi-k2,Kimi K2,osworld,OSWorld,35.2041
kimi-k2,Kimi K2,simpleqa,SimpleQA,44.8818
kimi-k2,Kimi K2,swe_bench_verified,SWE-bench Verified,68.0615
llama-4-maverick,Llama 4 Maverick,arc_agi_1,ARC-AGI-1,11.7202
llama-4-maverick,Llama 4 Maverick,bigcodebench,BigCodeBench,49.1511
llama-4-maverick,Llama 4 Maverick,chatbot_arena_elo,Chatbot Arena Elo,1369.2006
llama-4-maverick,Llama 4 Maverick,gpqa_diamond,GPQA Diamond,68.1008
llama-4-maverick,Llama 4 Maverick,humaneval,HumanEval,87.802
llama-4-maverick,Llama 4 Maverick,livecodebench,LiveCodeBench,49.0913
llama-4-maverick,Llama 4 Maverick,math_500,MATH-500,89.6686
llama-4-maverick,Llama 4 Maverick,mmmu,MMMU,75.4659
llama-4-maverick,Llama 4 Maverick,simpleqa,SimpleQA,30.9413
llama-4-maverick,Llama 4 Maverick,swe_bench_verified,SWE-bench Verified,49.5316
llama-4-maverick,Llama 4 Maverick,terminal_bench_1,Terminal-Bench 1.0,16.1181
minimax-m2,MiniMax-M2,aime_2024,AIME 2024,69.5967
minimax-m2,MiniMax-M2,aime_2025,AIME 2025,77.3283
minimax-m2,MiniMax-M2,chatbot_arena_elo,Chatbot Arena Elo,1429.6957
minimax-m2,MiniMax-M2,gpqa_diamond,GPQA Diamond,68.9034
minimax-m2,MiniMax-M2,hle,HLE (Humanity's Last Exam),25.8543
minimax-m2,MiniMax-M2,humaneval,HumanEval,85.4457
minimax-m2,MiniMax-M2,ifeval,IFEval,84.1641
minimax-m2,MiniMax-M2,math_500,MATH-500,91.3173
minimax-m2,MiniMax-M2,mmlu,MMLU,84.8754
minimax-m2,MiniMax-M2,mmlu_pro,MMLU-Pro,76.2675
minimax-m2,MiniMax-M2,mmmu,MMMU,78.5797
minimax-m2,MiniMax-M2,simpleqa,SimpleQA,38.1375
minimax-m2,MiniMax-M2,swe_bench_pro,SWE-bench Pro,42.5011
minimax-m2,MiniMax-M2,swe_bench_verified,SWE-bench Verified,61.5526
minimax-m2,MiniMax-M2,terminal_bench,Terminal-Bench 2.0,40.8741
o3-mini-high,o3-mini (high),aime_2024,AIME 2024,87.1377
o3-mini-high,o3-mini (high),arc_agi_1,ARC-AGI-1,47.492
o3-mini-high,o3-mini (high),arc_agi_2,ARC-AGI-2,22.7075
o3-mini-high,o3-mini (high),arena_hard,Arena-Hard Auto,81.9172
o3-mini-high,o3-mini (high),chatbot_arena_elo,Chatbot Arena Elo,1425.438
o3-mini-high,o3-mini (high),frontiermath,FrontierMath,15.9794
o3-mini-high,o3-mini (high),hle,HLE (Humanity's Last Exam),23.1822
o3-mini-high,o3-mini (high),hmmt_2025,HMMT Feb 2025,76.9609
o3-mini-high,o3-mini (high),humaneval,HumanEval,89.2495
o3-mini-high,o3-mini (high),ifeval,IFEval,87.7485
o3-mini-high,o3-mini (high),livecodebench,LiveCodeBench,71.5247
o3-mini-high,o3-mini (high),math_500,MATH-500,95.9853
o3-mini-high,o3-mini (high),simpleqa,SimpleQA,40.8261
o3-mini-high,o3-mini (high),swe_bench_verified,SWE-bench Verified,65.7276
o3-mini-high,o3-mini (high),usamo_2025,USAMO 2025,18.9185
