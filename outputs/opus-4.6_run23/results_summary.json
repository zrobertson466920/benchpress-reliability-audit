{
  "data_discovery": {
    "raw_schema": "JSON with top-level keys: models (list of 83 dicts with id/name/provider/release_date/params/architecture/is_reasoning/open_weights), benchmarks (list of 49 dicts with id/name/category/metric/num_problems/source_url), scores (list of 1390 dicts with model_id/benchmark_id/score/reference_url), generated (string timestamp)",
    "extraction_decisions": "Mapped each score entry to (model_id, benchmark_id) -> score. For 15 duplicate pairs, averaged the scores (consistent with canonical_evaluation.md). Used model id and benchmark id as primary keys. No joins needed; flat score list references model and benchmark lists directly.",
    "n_models_raw": 83,
    "n_benchmarks_raw": 49
  },
  "data": {
    "n_models": 80,
    "n_benchmarks": 35,
    "missing_fraction": 0.5425,
    "preprocessing": "Iteratively dropped benchmarks with <10 observations and models with <5 observations until stable. Reduced from 83x49 to 80x35. For SVD/prediction: min-max normalized each benchmark to 0-100 scale using observed min/max. Missing entries handled via iterative SVD completion (not pre-imputed).",
    "benchmarks_used": [
      "AIME 2024",
      "AIME 2025",
      "ARC-AGI-1",
      "ARC-AGI-2",
      "Arena-Hard Auto",
      "BrowseComp",
      "BRUMO 2025",
      "Chatbot Arena Elo",
      "CMIMC 2025",
      "Codeforces Rating",
      "CritPt",
      "FrontierMath",
      "GPQA Diamond",
      "GSM8K",
      "HLE (Humanity's Last Exam)",
      "HMMT Feb 2025",
      "HMMT Nov 2025",
      "HumanEval",
      "IFEval",
      "LiveBench",
      "LiveCodeBench",
      "MATH-500",
      "MathArena Apex 2025",
      "MMLU",
      "MMLU-Pro",
      "MMMU",
      "MMMU-Pro",
      "OSWorld",
      "SimpleQA",
      "SMT 2025",
      "SWE-bench Pro",
      "SWE-bench Verified",
      "Tau-Bench Retail",
      "Terminal-Bench 2.0",
      "Terminal-Bench 1.0"
    ]
  },
  "rank_analysis": {
    "method": "SVD on mean-imputed, min-max normalized (0-100) cleaned matrix (80 models x 35 benchmarks)",
    "effective_rank": 1,
    "variance_explained_by_rank": 0.9452,
    "singular_values": [
      3339.24,
      391.39,
      320.29,
      217.15,
      212.09,
      175.35,
      169.39,
      162.81,
      157.33,
      148.78
    ],
    "justification": "The first singular value captures 94.5% of total variance, indicating extremely strong rank-1 structure (a dominant 'general capability' factor). Effective rank is 1 at 90% threshold. Even at 95%, only 2 components needed. The matrix is strongly low-rank."
  },
  "benchmark_selection": {
    "method": "greedy_forward_correlation_coverage",
    "selected_benchmarks": [
      "ARC-AGI-2",
      "LiveCodeBench",
      "CMIMC 2025",
      "SMT 2025",
      "Arena-Hard Auto"
    ],
    "n_selected": 5,
    "selection_criterion": "Greedy forward selection maximizing average max-|correlation| between selected set and all remaining benchmarks. Selects benchmarks that collectively 'cover' the correlation structure."
  },
  "prediction": {
    "method": "Iterative SVD matrix completion (rank 5)",
    "overall_mae": 14.03,
    "per_benchmark_mae": {
      "MMLU-Pro": 8.687,
      "AIME 2024": 12.296,
      "Codeforces Rating": 13.19,
      "LiveBench": 10.128,
      "Arena-Hard Auto": 23.19,
      "GPQA Diamond": 9.696,
      "LiveCodeBench": 11.331,
      "MMMU": 23.278,
      "MMLU": 8.045,
      "MATH-500": 9.264,
      "IFEval": 10.328,
      "SimpleQA": 20.277,
      "SMT 2025": 22.842,
      "AIME 2025": 8.321,
      "BRUMO 2025": 8.194,
      "ARC-AGI-1": 10.579,
      "HLE (Humanity's Last Exam)": 16.395,
      "HumanEval": 8.858,
      "MathArena Apex 2025": 14.635,
      "SWE-bench Verified": 15.842,
      "HMMT Feb 2025": 21.475,
      "Terminal-Bench 2.0": 11.177,
      "Terminal-Bench 1.0": 16.656,
      "CritPt": 17.149,
      "SWE-bench Pro": 19.881,
      "FrontierMath": 21.046,
      "CMIMC 2025": 20.193,
      "BrowseComp": 21.838,
      "MMMU-Pro": 46.089,
      "Chatbot Arena Elo": 27.994,
      "Tau-Bench Retail": 15.342,
      "OSWorld": 36.053,
      "ARC-AGI-2": 16.292,
      "GSM8K": 6.489,
      "HMMT Nov 2025": 5.295
    },
    "evaluation_protocol": "20% random holdout of observed entries in cleaned matrix, iterative SVD completion (rank 5, 30 iterations), MAE on 0-100 normalized scale",
    "n_predictor_benchmarks": 5,
    "achieves_mae_under_5": false
  },
  "methodology_notes": "Strong rank-1 dominance (94.5% variance) suggests a single 'general capability' factor drives most benchmark variation. The remaining structure captures benchmark-specific variance (e.g., coding vs reasoning vs knowledge). High missingness (54% in cleaned matrix, 66% raw) is the main challenge; many benchmarks only tested on a subset of models. Iterative SVD completion handles this naturally. Selected benchmarks (ARC-AGI-2, LiveCodeBench, CMIMC 2025, SMT 2025, Arena-Hard Auto) span diverse categories to maximize predictive coverage. Canonical MAE of ~{canon_mae:.1f} on 0-100 scale reflects difficulty of predicting for held-out models with only 5 revealed benchmarks, especially given sparse data."
}