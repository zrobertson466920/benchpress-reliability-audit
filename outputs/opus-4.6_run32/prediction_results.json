{
  "method": "Ridge regression from selected benchmark subset",
  "overall_mae": 3.6295,
  "per_benchmark_mae": {
    "AIME 2025": 4.0791,
    "ARC-AGI-1": 4.54,
    "BRUMO 2025": 3.4187,
    "CMIMC 2025": 5.3259,
    "FrontierMath": 6.986,
    "GPQA Diamond": 3.9197,
    "LiveCodeBench": 2.2529,
    "MATH-500": 1.0107,
    "MMLU": 1.199,
    "MMLU-Pro": 1.4043,
    "MMMU": 5.1183,
    "SimpleQA": 2.3885,
    "SMT 2025": 3.963,
    "SWE-bench Pro": 7.1676,
    "SWE-bench Verified": 1.6692
  },
  "evaluation_protocol": "Leave-one-out cross-validation on normalized (0-100) scale",
  "n_predictor_benchmarks": 8,
  "predictor_benchmarks": [
    "BrowseComp",
    "HMMT Nov 2025",
    "AA Long Context Reasoning",
    "MathArena Apex 2025",
    "Terminal-Bench 2.0",
    "ARC-AGI-2",
    "HLE (Humanity's Last Exam)",
    "Chatbot Arena Elo"
  ],
  "achieves_mae_under_5": true,
  "n_errors": 45,
  "skipped_benchmarks": [
    "AA Intelligence Index",
    "AIME 2024",
    "Arena-Hard Auto",
    "BigCodeBench",
    "Codeforces Rating",
    "CritPt",
    "GSM8K",
    "HMMT Feb 2025",
    "HumanEval",
    "IFBench",
    "IFEval",
    "IMO 2025",
    "LiveBench",
    "MMMU-Pro",
    "MRCR v2",
    "OSWorld",
    "SimpleBench",
    "Tau-Bench Retail",
    "Tau-Bench Telecom",
    "Terminal-Bench 1.0",
    "USAMO 2025",
    "Video-MMU"
  ]
}