{
  "data_discovery": {
    "raw_schema": "JSON with 4 top-level keys: 'models' (83 dicts: id/name/provider/release_date/params_total_M/params_active_M/architecture/is_reasoning/open_weights), 'benchmarks' (49 dicts: id/name/category/metric/num_problems/source_url), 'scores' (1390 dicts: model_id/benchmark_id/score/reference_url), 'generated' (timestamp). Scores directly reference model_id and benchmark_id.",
    "extraction_decisions": "Built 83x49 matrix. Averaged 15 duplicate (model_id, benchmark_id) pairs (all deepseek-r1-distill variants with 2 entries each). No joins needed \u2014 flat score list.",
    "n_models_raw": 83,
    "n_benchmarks_raw": 49
  },
  "data": {
    "n_models": 80,
    "n_benchmarks": 45,
    "missing_fraction": 0.6239,
    "preprocessing": "Filtered benchmarks with <5 obs (['aime_2026', 'gdpval_aa', 'mathvision', 'scicode']) and models with <5 obs (['codestral-25.01', 'devstral-2', 'phi-4-reasoning']). Per-benchmark min-max normalization to [0,100]. Iterative SVD imputation at rank 3 for matrix completion. Raw values preserved for ridge regression training (no imputation in observed cells).",
    "benchmarks_used": [
      "AA Intelligence Index",
      "AA Long Context Reasoning",
      "AIME 2024",
      "AIME 2025",
      "ARC-AGI-1",
      "ARC-AGI-2",
      "Arena-Hard Auto",
      "BigCodeBench",
      "BrowseComp",
      "BRUMO 2025",
      "Chatbot Arena Elo",
      "CMIMC 2025",
      "Codeforces Rating",
      "CritPt",
      "FrontierMath",
      "GPQA Diamond",
      "GSM8K",
      "HLE (Humanity's Last Exam)",
      "HMMT Feb 2025",
      "HMMT Nov 2025",
      "HumanEval",
      "IFBench",
      "IFEval",
      "IMO 2025",
      "LiveBench",
      "LiveCodeBench",
      "MATH-500",
      "MathArena Apex 2025",
      "MMLU",
      "MMLU-Pro",
      "MMMU",
      "MMMU-Pro",
      "MRCR v2",
      "OSWorld",
      "SimpleBench",
      "SimpleQA",
      "SMT 2025",
      "SWE-bench Pro",
      "SWE-bench Verified",
      "Tau-Bench Retail",
      "Tau-Bench Telecom",
      "Terminal-Bench 2.0",
      "Terminal-Bench 1.0",
      "USAMO 2025",
      "Video-MMU"
    ]
  },
  "rank_analysis": {
    "method": "SVD on mean-imputed, per-benchmark min-max normalized (0-100) matrix, validated via cross-validation",
    "effective_rank": 3,
    "variance_explained_by_rank": 0.9677,
    "singular_values": [
      3724.3348,
      405.3692,
      348.2825,
      233.9178,
      216.5748,
      182.244,
      173.0435,
      172.0819,
      165.2251,
      158.3395,
      145.5737,
      134.5796,
      129.7789,
      126.8948,
      123.0011
    ],
    "justification": "Cross-validated rank selection: held out 20% of observed entries, tested ranks 1-10; rank 3 minimized test MAE (17.145). First singular value dominates (94.8% variance); S1/S2 ratio=9.2. 90%-variance threshold gives rank 1; S_k/S_1>0.05 gives rank 5. Matrix is strongly low-rank."
  },
  "benchmark_selection": {
    "method": "greedy_forward_selection",
    "selected_benchmarks": [
      "BrowseComp",
      "HMMT Nov 2025",
      "AA Long Context Reasoning",
      "MathArena Apex 2025",
      "Terminal-Bench 2.0",
      "ARC-AGI-2",
      "HLE (Humanity's Last Exam)",
      "Chatbot Arena Elo"
    ],
    "n_selected": 8,
    "selection_criterion": "Minimize LOO ridge regression MAE on normalized 0-100 scale across all target benchmarks"
  },
  "prediction": {
    "method": "Ridge regression from selected benchmark subset to each target; LOO evaluation on observed-only data",
    "overall_mae": 3.6295,
    "per_benchmark_mae": {
      "AIME 2025": 4.0791,
      "ARC-AGI-1": 4.54,
      "BRUMO 2025": 3.4187,
      "CMIMC 2025": 5.3259,
      "FrontierMath": 6.986,
      "GPQA Diamond": 3.9197,
      "LiveCodeBench": 2.2529,
      "MATH-500": 1.0107,
      "MMLU": 1.199,
      "MMLU-Pro": 1.4043,
      "MMMU": 5.1183,
      "SimpleQA": 2.3885,
      "SMT 2025": 3.963,
      "SWE-bench Pro": 7.1676,
      "SWE-bench Verified": 1.6692
    },
    "evaluation_protocol": "Leave-one-out cross-validation on per-benchmark min-max normalized 0-100 scale (observed data only)",
    "n_predictor_benchmarks": 8,
    "achieves_mae_under_5": true
  },
  "canonical_evaluation": {
    "canonical_overall_mae": 17.0789,
    "canonical_coverage": 1.0,
    "canonical_per_benchmark_mae": {
      "AA Intelligence Index": 29.8668,
      "AA Long Context Reasoning": 57.9736,
      "AIME 2024": 8.1128,
      "AIME 2025": 9.4851,
      "ARC-AGI-1": 19.2191,
      "ARC-AGI-2": 14.6246,
      "Arena-Hard Auto": 20.6655,
      "BRUMO 2025": 11.7882,
      "BigCodeBench": 11.0901,
      "BrowseComp": 29.7019,
      "CMIMC 2025": 2.7735,
      "Chatbot Arena Elo": 22.2971,
      "Codeforces Rating": 21.5613,
      "CritPt": 13.4542,
      "FrontierMath": 24.5861,
      "GPQA Diamond": 7.4855,
      "HLE (Humanity's Last Exam)": 23.411,
      "HMMT Feb 2025": 42.315,
      "HMMT Nov 2025": 36.3189,
      "HumanEval": 16.8681,
      "IFEval": 10.2555,
      "IMO 2025": 30.5946,
      "LiveBench": 24.5912,
      "LiveCodeBench": 12.9102,
      "MATH-500": 7.083,
      "MMLU": 7.625,
      "MMLU-Pro": 8.1776,
      "MMMU": 16.9988,
      "MMMU-Pro": 58.5777,
      "MRCR v2": 55.3314,
      "MathArena Apex 2025": 15.675,
      "MathVision": 16.0878,
      "OSWorld": 8.7455,
      "SMT 2025": 10.6945,
      "SWE-bench Pro": 28.3306,
      "SWE-bench Verified": 12.5618,
      "SimpleBench": 5.4761,
      "SimpleQA": 21.2673,
      "Tau-Bench Retail": 3.0307,
      "Terminal-Bench 1.0": 15.3342,
      "Terminal-Bench 2.0": 25.1356,
      "USAMO 2025": 25.5483,
      "Video-MMU": 2.1584
    },
    "n_scored_pairs": 196,
    "n_total_pairs": 196
  },
  "methodology_notes": "Pipeline: (1) Extract 83x49 matrix, average 15 duplicates. (2) Filter to benchmarks with >=5 obs and models with >=5 obs. (3) Min-max normalize per benchmark to [0,100]. (4) SVD rank analysis with CV-based rank selection (rank=3). (5) Iterative SVD imputation for matrix completion. (6) Greedy forward benchmark subset selection optimizing LOO ridge MAE. (7) Canonical evaluation: iterative SVD imputation + ridge from revealed benchmarks, blended 50/50. Key choices: filtering threshold=5, min-max normalization, iterative SVD imputation, ridge alpha=1.0."
}