model_id,model_name,benchmark_id,benchmark_name,y_pred
claude-opus-4,Claude Opus 4,aime_2024,AIME 2024,98.7844
claude-opus-4,Claude Opus 4,aime_2025,AIME 2025,93.2228
claude-opus-4,Claude Opus 4,arc_agi_1,ARC-AGI-1,65.7149
claude-opus-4,Claude Opus 4,arc_agi_2,ARC-AGI-2,21.8136
claude-opus-4,Claude Opus 4,codeforces_rating,Codeforces Rating,2412.3433
claude-opus-4,Claude Opus 4,critpt,CritPt,4.5295
claude-opus-4,Claude Opus 4,frontiermath,FrontierMath,15.4736
claude-opus-4,Claude Opus 4,gpqa_diamond,GPQA Diamond,87.7927
claude-opus-4,Claude Opus 4,hle,HLE (Humanity's Last Exam),30.6575
claude-opus-4,Claude Opus 4,hmmt_2025,HMMT Feb 2025,92.9632
claude-opus-4,Claude Opus 4,ifeval,IFEval,85.4498
claude-opus-4,Claude Opus 4,livecodebench,LiveCodeBench,80.3462
claude-opus-4,Claude Opus 4,math_500,MATH-500,93.5776
claude-opus-4,Claude Opus 4,mmlu,MMLU,88.3494
claude-opus-4,Claude Opus 4,mmmu,MMMU,80.7825
claude-opus-4,Claude Opus 4,simpleqa,SimpleQA,43.5706
claude-opus-4,Claude Opus 4,swe_bench_pro,SWE-bench Pro,35.8632
claude-opus-4,Claude Opus 4,tau_bench_retail,Tau-Bench Retail,80.6696
claude-opus-4,Claude Opus 4,terminal_bench,Terminal-Bench 2.0,30.0558
claude-opus-4.1,Claude Opus 4.1,frontiermath,FrontierMath,2.6878
claude-opus-4.1,Claude Opus 4.1,gpqa_diamond,GPQA Diamond,62.4356
claude-opus-4.1,Claude Opus 4.1,hle,HLE (Humanity's Last Exam),-0.95
claude-opus-4.1,Claude Opus 4.1,humaneval,HumanEval,71.0373
claude-opus-4.1,Claude Opus 4.1,mmlu,MMLU,82.8974
claude-opus-4.1,Claude Opus 4.1,mmlu_pro,MMLU-Pro,75.1309
claude-opus-4.1,Claude Opus 4.1,simpleqa,SimpleQA,16.758
claude-opus-4.1,Claude Opus 4.1,swe_bench_pro,SWE-bench Pro,55.3529
claude-opus-4.1,Claude Opus 4.1,terminal_bench,Terminal-Bench 2.0,6.3438
claude-opus-4.1,Claude Opus 4.1,terminal_bench_1,Terminal-Bench 1.0,32.2479
deepseek-r1-0528,DeepSeek-R1-0528,aime_2024,AIME 2024,91.9262
deepseek-r1-0528,DeepSeek-R1-0528,aime_2025,AIME 2025,88.5495
deepseek-r1-0528,DeepSeek-R1-0528,arc_agi_1,ARC-AGI-1,48.7463
deepseek-r1-0528,DeepSeek-R1-0528,brumo_2025,BRUMO 2025,85.9622
deepseek-r1-0528,DeepSeek-R1-0528,codeforces_rating,Codeforces Rating,2220.646
deepseek-r1-0528,DeepSeek-R1-0528,gpqa_diamond,GPQA Diamond,84.2392
deepseek-r1-0528,DeepSeek-R1-0528,hle,HLE (Humanity's Last Exam),23.6221
deepseek-r1-0528,DeepSeek-R1-0528,humaneval,HumanEval,89.8956
deepseek-r1-0528,DeepSeek-R1-0528,ifeval,IFEval,87.6531
deepseek-r1-0528,DeepSeek-R1-0528,imo_2025,IMO 2025,24.6091
deepseek-r1-0528,DeepSeek-R1-0528,livecodebench,LiveCodeBench,75.5635
deepseek-r1-0528,DeepSeek-R1-0528,math_500,MATH-500,95.1777
deepseek-r1-0528,DeepSeek-R1-0528,matharena_apex_2025,MathArena Apex 2025,-3.35
deepseek-r1-0528,DeepSeek-R1-0528,mmlu,MMLU,88.6147
deepseek-r1-0528,DeepSeek-R1-0528,mmlu_pro,MMLU-Pro,84.421
deepseek-r1-0528,DeepSeek-R1-0528,simpleqa,SimpleQA,38.2985
deepseek-r1-0528,DeepSeek-R1-0528,smt_2025,SMT 2025,81.3732
deepseek-r1-0528,DeepSeek-R1-0528,swe_bench_verified,SWE-bench Verified,68.4153
deepseek-r1-0528,DeepSeek-R1-0528,usamo_2025,USAMO 2025,24.3405
doubao-seed-2.0-pro,Doubao Seed 2.0 Pro,aime_2024,AIME 2024,106.0888
doubao-seed-2.0-pro,Doubao Seed 2.0 Pro,aime_2025,AIME 2025,97.5843
doubao-seed-2.0-pro,Doubao Seed 2.0 Pro,browsecomp,BrowseComp,64.8549
doubao-seed-2.0-pro,Doubao Seed 2.0 Pro,gpqa_diamond,GPQA Diamond,94.6624
doubao-seed-2.0-pro,Doubao Seed 2.0 Pro,hle,HLE (Humanity's Last Exam),36.6602
doubao-seed-2.0-pro,Doubao Seed 2.0 Pro,ifeval,IFEval,93.343
doubao-seed-2.0-pro,Doubao Seed 2.0 Pro,mathvision,MathVision,85.502
doubao-seed-2.0-pro,Doubao Seed 2.0 Pro,mmlu,MMLU,92.5638
doubao-seed-2.0-pro,Doubao Seed 2.0 Pro,mmmu,MMMU,83.8977
doubao-seed-2.0-pro,Doubao Seed 2.0 Pro,swe_bench_verified,SWE-bench Verified,79.149
doubao-seed-2.0-pro,Doubao Seed 2.0 Pro,terminal_bench,Terminal-Bench 2.0,45.2785
gemini-2.5-pro,Gemini 2.5 Pro,aa_intelligence_index,AA Intelligence Index,68.2114
gemini-2.5-pro,Gemini 2.5 Pro,aime_2024,AIME 2024,83.3827
gemini-2.5-pro,Gemini 2.5 Pro,aime_2025,AIME 2025,80.9397
gemini-2.5-pro,Gemini 2.5 Pro,arc_agi_1,ARC-AGI-1,40.195
gemini-2.5-pro,Gemini 2.5 Pro,arc_agi_2,ARC-AGI-2,6.2903
gemini-2.5-pro,Gemini 2.5 Pro,arena_hard,Arena-Hard Auto,68.0624
gemini-2.5-pro,Gemini 2.5 Pro,brumo_2025,BRUMO 2025,84.6842
gemini-2.5-pro,Gemini 2.5 Pro,chatbot_arena_elo,Chatbot Arena Elo,1410.9462
gemini-2.5-pro,Gemini 2.5 Pro,codeforces_rating,Codeforces Rating,2088.5338
gemini-2.5-pro,Gemini 2.5 Pro,critpt,CritPt,1.4667
gemini-2.5-pro,Gemini 2.5 Pro,frontiermath,FrontierMath,11.5126
gemini-2.5-pro,Gemini 2.5 Pro,gpqa_diamond,GPQA Diamond,78.8451
gemini-2.5-pro,Gemini 2.5 Pro,hle,HLE (Humanity's Last Exam),19.4312
gemini-2.5-pro,Gemini 2.5 Pro,hmmt_2025,HMMT Feb 2025,64.905
gemini-2.5-pro,Gemini 2.5 Pro,hmmt_nov_2025,HMMT Nov 2025,85.3123
gemini-2.5-pro,Gemini 2.5 Pro,imo_2025,IMO 2025,11.119
gemini-2.5-pro,Gemini 2.5 Pro,livebench,LiveBench,50.7505
gemini-2.5-pro,Gemini 2.5 Pro,math_500,MATH-500,90.506
gemini-2.5-pro,Gemini 2.5 Pro,matharena_apex_2025,MathArena Apex 2025,-2.3642
gemini-2.5-pro,Gemini 2.5 Pro,mmlu,MMLU,86.8002
gemini-2.5-pro,Gemini 2.5 Pro,mmlu_pro,MMLU-Pro,80.6655
gemini-2.5-pro,Gemini 2.5 Pro,mmmu,MMMU,76.5295
gemini-2.5-pro,Gemini 2.5 Pro,mrcr_v2,MRCR v2,37.6176
gemini-2.5-pro,Gemini 2.5 Pro,simplebench,SimpleBench,61.4143
gemini-2.5-pro,Gemini 2.5 Pro,simpleqa,SimpleQA,29.3332
gemini-2.5-pro,Gemini 2.5 Pro,smt_2025,SMT 2025,77.0499
gemini-2.5-pro,Gemini 2.5 Pro,swe_bench_verified,SWE-bench Verified,63.5175
gemini-2.5-pro,Gemini 2.5 Pro,terminal_bench_1,Terminal-Bench 1.0,27.6879
gemini-2.5-pro,Gemini 2.5 Pro,usamo_2025,USAMO 2025,21.2245
gemini-2.5-pro,Gemini 2.5 Pro,video_mmu,Video-MMU,84.323
gpt-4.1,GPT-4.1,aime_2025,AIME 2025,62.5029
gpt-4.1,GPT-4.1,arc_agi_1,ARC-AGI-1,-9.4
gpt-4.1,GPT-4.1,arena_hard,Arena-Hard Auto,70.5421
gpt-4.1,GPT-4.1,codeforces_rating,Codeforces Rating,751.4874
gpt-4.1,GPT-4.1,hle,HLE (Humanity's Last Exam),6.959
gpt-4.1,GPT-4.1,hmmt_2025,HMMT Feb 2025,49.1624
gpt-4.1,GPT-4.1,humaneval,HumanEval,84.4354
gpt-4.1,GPT-4.1,ifeval,IFEval,80.4339
gpt-4.1,GPT-4.1,livecodebench,LiveCodeBench,49.2451
gpt-4.1,GPT-4.1,math_500,MATH-500,93.5056
gpt-4.1,GPT-4.1,mmlu,MMLU,90.4265
gpt-4.1,GPT-4.1,mmmu,MMMU,69.3077
gpt-4.1,GPT-4.1,simpleqa,SimpleQA,33.9444
gpt-4.1,GPT-4.1,swe_bench_verified,SWE-bench Verified,42.1408
gpt-4.1,GPT-4.1,terminal_bench_1,Terminal-Bench 1.0,16.441
grok-3-beta,Grok 3 Beta,aime_2024,AIME 2024,93.8494
grok-3-beta,Grok 3 Beta,aime_2025,AIME 2025,87.0842
grok-3-beta,Grok 3 Beta,arc_agi_1,ARC-AGI-1,39.4206
grok-3-beta,Grok 3 Beta,chatbot_arena_elo,Chatbot Arena Elo,1432.3182
grok-3-beta,Grok 3 Beta,hle,HLE (Humanity's Last Exam),18.5422
grok-3-beta,Grok 3 Beta,humaneval,HumanEval,93.2009
grok-3-beta,Grok 3 Beta,ifeval,IFEval,90.0419
grok-3-beta,Grok 3 Beta,mmlu_pro,MMLU-Pro,84.4871
grok-3-beta,Grok 3 Beta,simpleqa,SimpleQA,28.1948
grok-3-beta,Grok 3 Beta,swe_bench_verified,SWE-bench Verified,64.8866
grok-4,Grok 4,aa_intelligence_index,AA Intelligence Index,61.6472
grok-4,Grok 4,aa_lcr,AA Long Context Reasoning,73.0437
grok-4,Grok 4,aime_2024,AIME 2024,93.6479
grok-4,Grok 4,arc_agi_2,ARC-AGI-2,26.5184
grok-4,Grok 4,brumo_2025,BRUMO 2025,95.2305
grok-4,Grok 4,chatbot_arena_elo,Chatbot Arena Elo,1438.2606
grok-4,Grok 4,cmimc_2025,CMIMC 2025,82.5363
grok-4,Grok 4,codeforces_rating,Codeforces Rating,2274.7346
grok-4,Grok 4,frontiermath,FrontierMath,23.3094
grok-4,Grok 4,gpqa_diamond,GPQA Diamond,85.2115
grok-4,Grok 4,hle,HLE (Humanity's Last Exam),30.9863
grok-4,Grok 4,hmmt_nov_2025,HMMT Nov 2025,90.8689
grok-4,Grok 4,ifeval,IFEval,88.4444
grok-4,Grok 4,imo_2025,IMO 2025,43.8786
grok-4,Grok 4,livecodebench,LiveCodeBench,75.6904
grok-4,Grok 4,matharena_apex_2025,MathArena Apex 2025,10.5792
grok-4,Grok 4,mmlu,MMLU,88.7015
grok-4,Grok 4,mmlu_pro,MMLU-Pro,84.7085
grok-4,Grok 4,mmmu,MMMU,81.1325
grok-4,Grok 4,mmmu_pro,MMMU-Pro,75.1917
grok-4,Grok 4,osworld,OSWorld,55.1502
grok-4,Grok 4,simpleqa,SimpleQA,52.368
grok-4,Grok 4,smt_2025,SMT 2025,88.1498
grok-4,Grok 4,swe_bench_pro,SWE-bench Pro,42.1724
grok-4,Grok 4,swe_bench_verified,SWE-bench Verified,74.2747
grok-4,Grok 4,terminal_bench,Terminal-Bench 2.0,44.575
grok-4,Grok 4,terminal_bench_1,Terminal-Bench 1.0,43.4239
grok-4,Grok 4,usamo_2025,USAMO 2025,28.5797
kimi-k2,Kimi K2,aime_2024,AIME 2024,75.2752
kimi-k2,Kimi K2,aime_2025,AIME 2025,76.6083
kimi-k2,Kimi K2,arena_hard,Arena-Hard Auto,69.1412
kimi-k2,Kimi K2,gpqa_diamond,GPQA Diamond,78.1744
kimi-k2,Kimi K2,hle,HLE (Humanity's Last Exam),15.5277
kimi-k2,Kimi K2,hmmt_2025,HMMT Feb 2025,64.2582
kimi-k2,Kimi K2,ifeval,IFEval,82.9214
kimi-k2,Kimi K2,livecodebench,LiveCodeBench,71.303
kimi-k2,Kimi K2,math_500,MATH-500,93.9008
kimi-k2,Kimi K2,mmlu_pro,MMLU-Pro,83.2983
kimi-k2,Kimi K2,osworld,OSWorld,37.3317
kimi-k2,Kimi K2,simpleqa,SimpleQA,34.6616
kimi-k2,Kimi K2,swe_bench_verified,SWE-bench Verified,61.6568
llama-4-maverick,Llama 4 Maverick,arc_agi_1,ARC-AGI-1,8.665
llama-4-maverick,Llama 4 Maverick,bigcodebench,BigCodeBench,47.9034
llama-4-maverick,Llama 4 Maverick,chatbot_arena_elo,Chatbot Arena Elo,1378.0269
llama-4-maverick,Llama 4 Maverick,gpqa_diamond,GPQA Diamond,66.8053
llama-4-maverick,Llama 4 Maverick,humaneval,HumanEval,88.0535
llama-4-maverick,Llama 4 Maverick,livecodebench,LiveCodeBench,50.1601
llama-4-maverick,Llama 4 Maverick,math_500,MATH-500,91.4844
llama-4-maverick,Llama 4 Maverick,mmmu,MMMU,73.2733
llama-4-maverick,Llama 4 Maverick,simpleqa,SimpleQA,28.7816
llama-4-maverick,Llama 4 Maverick,swe_bench_verified,SWE-bench Verified,47.2077
llama-4-maverick,Llama 4 Maverick,terminal_bench_1,Terminal-Bench 1.0,18.0091
minimax-m2,MiniMax-M2,aime_2024,AIME 2024,86.7445
minimax-m2,MiniMax-M2,aime_2025,AIME 2025,82.5478
minimax-m2,MiniMax-M2,chatbot_arena_elo,Chatbot Arena Elo,1436.767
minimax-m2,MiniMax-M2,gpqa_diamond,GPQA Diamond,78.7158
minimax-m2,MiniMax-M2,hle,HLE (Humanity's Last Exam),30.1462
minimax-m2,MiniMax-M2,humaneval,HumanEval,81.0509
minimax-m2,MiniMax-M2,ifeval,IFEval,75.9189
minimax-m2,MiniMax-M2,math_500,MATH-500,82.2002
minimax-m2,MiniMax-M2,mmlu,MMLU,81.007
minimax-m2,MiniMax-M2,mmlu_pro,MMLU-Pro,73.626
minimax-m2,MiniMax-M2,mmmu,MMMU,79.221
minimax-m2,MiniMax-M2,simpleqa,SimpleQA,41.2667
minimax-m2,MiniMax-M2,swe_bench_pro,SWE-bench Pro,33.5992
minimax-m2,MiniMax-M2,swe_bench_verified,SWE-bench Verified,71.2747
minimax-m2,MiniMax-M2,terminal_bench,Terminal-Bench 2.0,24.6036
o3-mini-high,o3-mini (high),aime_2024,AIME 2024,86.4738
o3-mini-high,o3-mini (high),arc_agi_1,ARC-AGI-1,49.51
o3-mini-high,o3-mini (high),arc_agi_2,ARC-AGI-2,22.88
o3-mini-high,o3-mini (high),arena_hard,Arena-Hard Auto,73.1242
o3-mini-high,o3-mini (high),chatbot_arena_elo,Chatbot Arena Elo,1432.5519
o3-mini-high,o3-mini (high),frontiermath,FrontierMath,19.7866
o3-mini-high,o3-mini (high),hle,HLE (Humanity's Last Exam),24.2851
o3-mini-high,o3-mini (high),hmmt_2025,HMMT Feb 2025,76.7863
o3-mini-high,o3-mini (high),humaneval,HumanEval,89.1552
o3-mini-high,o3-mini (high),ifeval,IFEval,87.5124
o3-mini-high,o3-mini (high),livecodebench,LiveCodeBench,71.1859
o3-mini-high,o3-mini (high),math_500,MATH-500,95.6362
o3-mini-high,o3-mini (high),simpleqa,SimpleQA,40.774
o3-mini-high,o3-mini (high),swe_bench_verified,SWE-bench Verified,65.6591
o3-mini-high,o3-mini (high),usamo_2025,USAMO 2025,21.3966
