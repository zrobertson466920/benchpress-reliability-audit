{
  "data_discovery": {
    "raw_schema": "JSON with keys: models (83 dicts: id, name, provider, release_date, params_total_M, params_active_M, architecture, is_reasoning, open_weights), benchmarks (49 dicts: id, name, category, metric, num_problems, source_url), scores (1390 dicts: model_id, benchmark_id, score, reference_url), generated (timestamp).",
    "extraction_decisions": "Mapped each score entry to (model_id, benchmark_id) -> score. Found 15 duplicate (model_id, benchmark_id) pairs (all from DeepSeek distilled models); resolved by simple averaging. Used sorted model_id and benchmark_id as canonical row/column ordering. All 83 models and 49 benchmarks included in raw matrix.",
    "n_models_raw": 83,
    "n_benchmarks_raw": 49
  },
  "data": {
    "n_models": 80,
    "n_benchmarks": 35,
    "missing_fraction": 0.5425,
    "preprocessing": "Iteratively filtered benchmarks with <10 models and models with <5 benchmarks until stable (83x49 -> 80x35). Per-benchmark min-max normalization to [0,100] to unify mixed metrics (Elo ratings range 800-3020, percentages 0-100, index scores 51-73). Missing entries mean-imputed per column for SVD decomposition.",
    "benchmarks_used": [
      "AIME 2024",
      "AIME 2025",
      "ARC-AGI-1",
      "ARC-AGI-2",
      "Arena-Hard Auto",
      "BrowseComp",
      "BRUMO 2025",
      "Chatbot Arena Elo",
      "CMIMC 2025",
      "Codeforces Rating",
      "CritPt",
      "FrontierMath",
      "GPQA Diamond",
      "GSM8K",
      "HLE (Humanity's Last Exam)",
      "HMMT Feb 2025",
      "HMMT Nov 2025",
      "HumanEval",
      "IFEval",
      "LiveBench",
      "LiveCodeBench",
      "MATH-500",
      "MathArena Apex 2025",
      "MMLU",
      "MMLU-Pro",
      "MMMU",
      "MMMU-Pro",
      "OSWorld",
      "SimpleQA",
      "SMT 2025",
      "SWE-bench Pro",
      "SWE-bench Verified",
      "Tau-Bench Retail",
      "Terminal-Bench 2.0",
      "Terminal-Bench 1.0"
    ]
  },
  "rank_analysis": {
    "method": "SVD on mean-imputed, per-benchmark min-max normalized (0-100), column-centered filtered matrix",
    "effective_rank": 16,
    "variance_explained_by_rank": 0.90759831755574,
    "singular_values": [
      541.0675617394056,
      377.40494779195495,
      231.01925795040526,
      212.23471751782031,
      179.90960408612693,
      172.26366088579408,
      165.28805578091706,
      162.05114529775454,
      150.34122325479592,
      133.7721463139821,
      126.29561854737918,
      123.79586690941535,
      119.58735098206445,
      107.67832139503719,
      102.78848412832713,
      96.95109035130425,
      93.89018844061991,
      86.73792704362073,
      85.46968670118584,
      82.00115191637825,
      81.10375168069268,
      77.74828008087364,
      72.80707941649358,
      69.16077542061532,
      65.84490209448653,
      60.117507472898275,
      59.117603371179975,
      52.1071579987439,
      49.626415921603304,
      44.325634682261374,
      43.963474425845426,
      38.63820831350081,
      38.31319156755235,
      30.719431682880643,
      23.955150880574514
    ],
    "justification": "Effective rank is 16 using 90% cumulative variance threshold on the filtered 80x35 matrix. The first component explains 34.6% of variance (dominant general-ability factor). SV1/SV2 ratio = 1.43 indicates moderate but not extreme dominance of rank-1. High effective rank (16) partly reflects noise from imputed missing values (missing fraction 0.54). Cross-validated matrix completion with rank 2 achieves best held-out MAE, suggesting true effective rank is 2-3 for prediction purposes."
  },
  "benchmark_selection": {
    "method": "greedy_forward_selection",
    "selected_benchmarks": [
      "BrowseComp",
      "CMIMC 2025",
      "SimpleQA",
      "SWE-bench Verified",
      "AIME 2025",
      "MMLU-Pro",
      "SMT 2025"
    ],
    "n_selected": 7,
    "selection_criterion": "Minimize LOO-model ridge regression MAE on normalized 0-100 filtered matrix (alpha=10)"
  },
  "prediction": {
    "method": "Ridge regression from selected benchmark subset to remaining benchmarks (alpha=10, fit_intercept=True)",
    "overall_mae": 10.82130242078163,
    "per_benchmark_mae": {
      "AIME 2024": 7.583699856632624,
      "ARC-AGI-1": 20.47529175654919,
      "ARC-AGI-2": 12.488237691299133,
      "Arena-Hard Auto": 25.847878115696695,
      "BRUMO 2025": 2.435956643992919,
      "Chatbot Arena Elo": 12.60065062617614,
      "Codeforces Rating": 11.411446052789554,
      "CritPt": 14.116679661515114,
      "FrontierMath": 13.302573325659708,
      "GPQA Diamond": 4.9068907290619075,
      "HLE (Humanity's Last Exam)": 19.134236054693798,
      "HMMT Feb 2025": 16.042612531757506,
      "HMMT Nov 2025": 11.920785955844082,
      "HumanEval": 7.105689142185576,
      "IFEval": 7.0057138776199634,
      "LiveBench": 22.646410639165396,
      "LiveCodeBench": 5.192701400176789,
      "MATH-500": 2.413071887740383,
      "MathArena Apex 2025": 4.700791944418824,
      "MMLU": 5.537213465658219,
      "MMMU": 6.470572782391443,
      "MMMU-Pro": 16.775037702485577,
      "OSWorld": 19.565408018127005,
      "SWE-bench Pro": 17.948790427657777,
      "Tau-Bench Retail": 10.188623460304285,
      "Terminal-Bench 2.0": 11.190440622943846,
      "Terminal-Bench 1.0": 14.223574884690446
    },
    "evaluation_protocol": "Leave-one-model-out cross-validation on filtered, min-max normalized matrix",
    "n_predictor_benchmarks": 7,
    "achieves_mae_under_5": false
  },
  "canonical_evaluation": {
    "method": "Hybrid: iterative SVD completion (rank 2) on full 83x49 normalized matrix with held-out masked, blended with ridge regression from 5 revealed benchmarks where training data sufficient (>=8 models with all revealed benchmarks observed, >=5 with target benchmark observed). Ridge weight = min(n_train/30, 0.6). Predictions clipped to [0,100] on normalized scale.",
    "canonical_overall_mae": 15.746165105641095,
    "n_predictions": 196,
    "n_heldout_pairs": 196,
    "coverage": 1.0
  },
  "methodology_notes": "Pipeline: (1) Extract 83x49 raw matrix from JSON, average 15 duplicate score entries. (2) Filter sparse rows/columns iteratively for analysis submatrix. (3) Per-benchmark min-max normalize to [0,100] to handle mixed metrics (Elo ratings, percentages, index scores). (4) Mean-impute missing for SVD; effective rank 2-3 for prediction despite higher 90%-variance rank due to imputation noise. (5) Greedy forward selection of 7 benchmarks minimizing LOO ridge MAE. (6) Canonical prediction uses hybrid approach: iterative rank-2 SVD completion on full matrix (leverages all cross-model structure) blended with ridge from revealed benchmarks (leverages direct feature mapping where enough training data exists). Scale mismatch between Elo-type and percentage metrics was the main preprocessing concern. High missingness (66%) is the dominant challenge; the sparse benchmarks (2-7 models) are essentially unpredictable without strong structural assumptions."
}