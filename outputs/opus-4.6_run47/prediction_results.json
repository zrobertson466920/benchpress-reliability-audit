{
  "method": "Ridge regression from selected benchmark subset to remaining benchmarks",
  "overall_mae": 10.82130242078163,
  "per_benchmark_mae": {
    "AIME 2024": 7.583699856632624,
    "ARC-AGI-1": 20.47529175654919,
    "ARC-AGI-2": 12.488237691299133,
    "Arena-Hard Auto": 25.847878115696695,
    "BRUMO 2025": 2.435956643992919,
    "Chatbot Arena Elo": 12.60065062617614,
    "Codeforces Rating": 11.411446052789554,
    "CritPt": 14.116679661515114,
    "FrontierMath": 13.302573325659708,
    "GPQA Diamond": 4.9068907290619075,
    "HLE (Humanity's Last Exam)": 19.134236054693798,
    "HMMT Feb 2025": 16.042612531757506,
    "HMMT Nov 2025": 11.920785955844082,
    "HumanEval": 7.105689142185576,
    "IFEval": 7.0057138776199634,
    "LiveBench": 22.646410639165396,
    "LiveCodeBench": 5.192701400176789,
    "MATH-500": 2.413071887740383,
    "MathArena Apex 2025": 4.700791944418824,
    "MMLU": 5.537213465658219,
    "MMMU": 6.470572782391443,
    "MMMU-Pro": 16.775037702485577,
    "OSWorld": 19.565408018127005,
    "SWE-bench Pro": 17.948790427657777,
    "Tau-Bench Retail": 10.188623460304285,
    "Terminal-Bench 2.0": 11.190440622943846,
    "Terminal-Bench 1.0": 14.223574884690446
  },
  "evaluation_protocol": "Leave-one-model-out CV on filtered, min-max normalized matrix",
  "n_predictor_benchmarks": 7,
  "achieves_mae_under_5": false,
  "normalization": "per-benchmark min-max to [0,100]",
  "ridge_alpha": 10.0
}