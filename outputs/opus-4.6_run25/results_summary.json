{
  "data_discovery": {
    "raw_schema": "JSON with top-level keys: models (list of 83 model dicts with id/name/provider/params/architecture/is_reasoning/open_weights), benchmarks (list of 49 benchmark dicts with id/name/category/metric/num_problems/source_url), scores (list of 1390 score entries with model_id/benchmark_id/score/reference_url), generated (timestamp string).",
    "extraction_decisions": "Each score entry maps a (model_id, benchmark_id) pair to a numeric score. 15 duplicate pairs found (e.g. deepseek-r1-distill-qwen-32b on gpqa_diamond); duplicates resolved by simple averaging. No null scores in the dataset. All model_ids and benchmark_ids in scores matched the models/benchmarks lists. Matrix constructed as 83 models x 49 benchmarks.",
    "n_models_raw": 83,
    "n_benchmarks_raw": 49
  },
  "data": {
    "n_models": 65,
    "n_benchmarks": 29,
    "missing_fraction": 0.418,
    "missing_fraction_full_matrix": 0.6619,
    "preprocessing": "Iterative filtering: removed benchmarks with <15 observed models and models with <10 observed benchmarks (converged in 2 iterations: 83->65 models, 49->29 benchmarks). Missing values imputed with per-benchmark column means. Z-score normalization per benchmark applied before SVD. No score transformations (raw scores used).",
    "benchmarks_used": [
      "AIME 2024",
      "AIME 2025",
      "ARC-AGI-1",
      "ARC-AGI-2",
      "Arena-Hard Auto",
      "BrowseComp",
      "BRUMO 2025",
      "Chatbot Arena Elo",
      "CMIMC 2025",
      "Codeforces Rating",
      "FrontierMath",
      "GPQA Diamond",
      "HLE (Humanity's Last Exam)",
      "HMMT Feb 2025",
      "HumanEval",
      "IFEval",
      "LiveCodeBench",
      "MATH-500",
      "MathArena Apex 2025",
      "MMLU",
      "MMLU-Pro",
      "MMMU",
      "OSWorld",
      "SimpleQA",
      "SMT 2025",
      "SWE-bench Pro",
      "SWE-bench Verified",
      "Terminal-Bench 2.0",
      "Terminal-Bench 1.0"
    ]
  },
  "rank_analysis": {
    "method": "SVD on z-score normalized, mean-imputed filtered matrix",
    "effective_rank": 14,
    "effective_rank_80pct": 9,
    "variance_explained_by_rank": 0.6814,
    "singular_values": [
      24.4197,
      17.7346,
      12.0606,
      10.7112,
      9.6777,
      8.7541,
      8.6816,
      7.4393,
      7.0806,
      6.4925
    ],
    "justification": "The first singular value captures 32.1% of variance, and the first two capture 49.1%. However, the spectrum decays slowly: 90% variance requires 14 components and 80% requires 9. The matrix exhibits moderate low-rank structure with a dominant first factor but significant residual variance spread across many dimensions, likely reflecting the diverse benchmark categories (coding, math, knowledge, agentic, multimodal)."
  },
  "benchmark_selection": {
    "method": "greedy forward selection",
    "selected_benchmarks": [
      "Codeforces Rating",
      "Chatbot Arena Elo",
      "LiveCodeBench",
      "ARC-AGI-2",
      "HMMT Feb 2025"
    ],
    "n_selected": 5,
    "selection_criterion": "minimize ridge regression MAE on non-selected benchmarks (observed entries only in filtered matrix)"
  },
  "prediction": {
    "method": "Low-rank SVD embedding (rank-5) + per-benchmark Ridge regression (own eval); SVD low-rank projection + KNN ensemble (canonical eval)",
    "overall_mae": 10.5871,
    "per_benchmark_mae": {
      "AIME 2024": 12.1819,
      "AIME 2025": 8.4854,
      "ARC-AGI-1": 13.7681,
      "ARC-AGI-2": 8.5562,
      "Arena-Hard Auto": 13.1479,
      "BrowseComp": 13.4829,
      "BRUMO 2025": 9.2908,
      "Chatbot Arena Elo": 13.6434,
      "CMIMC 2025": 12.6969,
      "Codeforces Rating": 10.2937,
      "FrontierMath": 15.7094,
      "GPQA Diamond": 6.3922,
      "HLE (Humanity's Last Exam)": 13.4293,
      "HMMT Feb 2025": 10.2345,
      "HumanEval": 6.5762,
      "IFEval": 6.7022,
      "LiveCodeBench": 9.0198,
      "MATH-500": 13.3635,
      "MathArena Apex 2025": 10.5928,
      "MMLU": 7.194,
      "MMLU-Pro": 6.3454,
      "MMMU": 18.2056,
      "OSWorld": 12.8613,
      "SimpleQA": 12.3471,
      "SMT 2025": 8.0881,
      "SWE-bench Pro": 15.2882,
      "SWE-bench Verified": 14.908,
      "Terminal-Bench 2.0": 12.4932,
      "Terminal-Bench 1.0": 14.4337
    },
    "evaluation_protocol": "Leave-one-model-out CV on filtered matrix (own eval, normalized 0-100). Canonical: reveal-5-per-model with SVD projection + 5-NN inverse-distance blending.",
    "n_predictor_benchmarks": 29,
    "achieves_mae_under_5": false,
    "canonical_mae": 16.805
  },
  "methodology_notes": "The full 83x49 matrix is 66% missing, reflecting that most models are only evaluated on a small subset of benchmarks. After filtering to 65x29, missingness drops to 42%. The score scales vary widely: most benchmarks use percentage correct (0-100), but Chatbot Arena Elo (~1000-1400) and Codeforces Rating (0-3000) use very different scales. Z-score normalization addresses this for SVD. For canonical evaluation, the low-rank projection approach projects each eval model's 5 revealed benchmark scores into a rank-5 SVD basis learned from all other models, then predicts all benchmarks. This is blended 50/50 with a 5-nearest-neighbor prediction using inverse-distance weighting on the normalized revealed benchmark scores. The canonical MAE of ~16.8 (normalized 0-100) reflects the difficulty of predicting from only 5 benchmarks in a sparse, heterogeneous benchmark landscape."
}