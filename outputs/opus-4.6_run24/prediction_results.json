{
  "method": "Ridge regression (alpha=1.0) from all observed benchmarks per model to each target benchmark, leave-one-model-out",
  "overall_mae": 13.707935580039951,
  "per_benchmark_mae": {
    "AIME 2024": 11.967799077733858,
    "AIME 2025": 13.195919311319814,
    "ARC-AGI-1": 20.53133213745505,
    "ARC-AGI-2": 14.1086420047458,
    "Arena-Hard Auto": 18.080777711718827,
    "BrowseComp": 18.926267961187165,
    "BRUMO 2025": 13.26462342874609,
    "Chatbot Arena Elo": 18.221505216382077,
    "CMIMC 2025": 18.406786629728384,
    "Codeforces Rating": 13.015437683822997,
    "CritPt": 17.312046513815158,
    "FrontierMath": 19.429247311208623,
    "GPQA Diamond": 10.295314926045302,
    "GSM8K": 11.222073507448211,
    "HLE (Humanity's Last Exam)": 16.619818148416908,
    "HMMT Feb 2025": 20.387187599184127,
    "HMMT Nov 2025": 11.4209693277908,
    "HumanEval": 9.367378409861796,
    "IFEval": 9.747884514321445,
    "LiveBench": 24.116890973720754,
    "LiveCodeBench": 9.675198310604085,
    "MATH-500": 8.146235305822294,
    "MathArena Apex 2025": 16.689791516904172,
    "MMLU": 8.507531939678506,
    "MMLU-Pro": 8.195232645011501,
    "MMMU": 16.121545197354212,
    "MMMU-Pro": 20.97023640060828,
    "OSWorld": 22.59910533236869,
    "SimpleQA": 18.142080581734614,
    "SMT 2025": 11.88998449200843,
    "SWE-bench Pro": 20.211065853115457,
    "SWE-bench Verified": 20.855327333711788,
    "Tau-Bench Retail": 12.323475599772044,
    "Terminal-Bench 2.0": 16.364689711456045,
    "Terminal-Bench 1.0": 21.213141842054046
  },
  "evaluation_protocol": "Leave-one-model-out on filtered normalized (0-100) matrix, scored on observed entries only",
  "n_predictor_benchmarks": 35,
  "achieves_mae_under_5": false,
  "n_models_evaluated": 80,
  "n_errors": 1281
}