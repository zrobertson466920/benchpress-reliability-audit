{
  "data_discovery": {
    "raw_schema": "JSON with keys: 'models' (83 dicts: id, name, provider, release_date, params_total_M, params_active_M, architecture, is_reasoning, open_weights), 'benchmarks' (49 dicts: id, name, category, metric, num_problems, source_url), 'scores' (1390 dicts: model_id, benchmark_id, score, reference_url), 'generated' (timestamp).",
    "extraction_decisions": "Built (model_id, benchmark_id) -> score mapping. Found 15 duplicate pairs; averaged their scores per canonical spec. Used model_id as row index, benchmark_id as column index. All 83 models and 49 benchmarks included in raw performance matrix.",
    "n_models_raw": 83,
    "n_benchmarks_raw": 49
  },
  "data": {
    "n_models": 80,
    "n_benchmarks": 35,
    "missing_fraction": 0.5425,
    "preprocessing": "Iterative filtering removed benchmarks with <10 model scores and models with <5 benchmark scores. Remaining missing values imputed with column means. Per-benchmark min-max normalization to [0,100] scale to handle mixed metrics (percentages, Elo ratings, indices, scores).",
    "benchmarks_used": [
      "AIME 2024",
      "AIME 2025",
      "ARC-AGI-1",
      "ARC-AGI-2",
      "Arena-Hard Auto",
      "BrowseComp",
      "BRUMO 2025",
      "Chatbot Arena Elo",
      "CMIMC 2025",
      "Codeforces Rating",
      "CritPt",
      "FrontierMath",
      "GPQA Diamond",
      "GSM8K",
      "HLE (Humanity's Last Exam)",
      "HMMT Feb 2025",
      "HMMT Nov 2025",
      "HumanEval",
      "IFEval",
      "LiveBench",
      "LiveCodeBench",
      "MATH-500",
      "MathArena Apex 2025",
      "MMLU",
      "MMLU-Pro",
      "MMMU",
      "MMMU-Pro",
      "OSWorld",
      "SimpleQA",
      "SMT 2025",
      "SWE-bench Pro",
      "SWE-bench Verified",
      "Tau-Bench Retail",
      "Terminal-Bench 2.0",
      "Terminal-Bench 1.0"
    ]
  },
  "rank_analysis": {
    "method": "SVD on min-max normalized, mean-imputed, column-centered filtered matrix",
    "effective_rank": 16,
    "variance_explained_by_rank": 0.9075983175557402,
    "singular_values": [
      541.0675617394056,
      377.4049477919546,
      231.01925795040526,
      212.2347175178208,
      179.909604086127,
      172.26366088579405,
      165.2880557809169,
      162.0511452977545,
      150.34122325479603,
      133.7721463139821,
      126.2956185473791,
      123.79586690941548,
      119.58735098206455,
      107.6783213950371,
      102.78848412832726,
      96.95109035130425,
      93.89018844061991,
      86.73792704362062,
      85.46968670118576,
      82.00115191637828,
      81.10375168069268,
      77.7482800808736,
      72.80707941649356,
      69.16077542061525,
      65.84490209448649,
      60.11750747289829,
      59.11760337117996,
      52.10715799874386,
      49.626415921603304,
      44.32563468226134,
      43.96347442584538,
      38.638208313500755,
      38.313191567552316,
      30.71943168288065,
      23.955150880574482
    ],
    "justification": "90% cumulative variance threshold yields rank 16. The first singular value is dominant (SV1=541 vs SV2=377), indicating a strong general capability factor. Participation ratio is 6.1. The matrix has moderate effective rank, reflecting that LLM performance is low-dimensional but not trivially rank-1 due to distinct capability clusters (math/coding vs language vs multimodal)."
  },
  "benchmark_selection": {
    "method": "Greedy forward selection",
    "selected_benchmarks": [
      "LiveCodeBench",
      "ARC-AGI-2",
      "HMMT Feb 2025",
      "Arena-Hard Auto",
      "SWE-bench Verified",
      "Terminal-Bench 1.0",
      "CMIMC 2025",
      "MMMU"
    ],
    "n_selected": 8,
    "selection_criterion": "Minimize leave-one-model-out MAE using ridge regression from selected subset to remaining benchmarks, scored on observed entries only"
  },
  "prediction": {
    "method": "Ensemble of three methods: (1) Low-rank projection (rank-8 SVD, ridge-regularized, lambda=0.1), (2) Ridge regression on observed-only training data (alpha=50), (3) WALS matrix completion (rank=5, lambda=10). Weights: 0.4/0.2/0.4. Self-evaluation uses ridge regression LOO on filtered matrix.",
    "overall_mae": 13.707935580039951,
    "per_benchmark_mae": {
      "AIME 2024": 11.967799077733858,
      "AIME 2025": 13.195919311319814,
      "ARC-AGI-1": 20.53133213745505,
      "ARC-AGI-2": 14.1086420047458,
      "Arena-Hard Auto": 18.080777711718827,
      "BrowseComp": 18.926267961187165,
      "BRUMO 2025": 13.26462342874609,
      "Chatbot Arena Elo": 18.221505216382077,
      "CMIMC 2025": 18.406786629728384,
      "Codeforces Rating": 13.015437683822997,
      "CritPt": 17.312046513815158,
      "FrontierMath": 19.429247311208623,
      "GPQA Diamond": 10.295314926045302,
      "GSM8K": 11.222073507448211,
      "HLE (Humanity's Last Exam)": 16.619818148416908,
      "HMMT Feb 2025": 20.387187599184127,
      "HMMT Nov 2025": 11.4209693277908,
      "HumanEval": 9.367378409861796,
      "IFEval": 9.747884514321445,
      "LiveBench": 24.116890973720754,
      "LiveCodeBench": 9.675198310604085,
      "MATH-500": 8.146235305822294,
      "MathArena Apex 2025": 16.689791516904172,
      "MMLU": 8.507531939678506,
      "MMLU-Pro": 8.195232645011501,
      "MMMU": 16.121545197354212,
      "MMMU-Pro": 20.97023640060828,
      "OSWorld": 22.59910533236869,
      "SimpleQA": 18.142080581734614,
      "SMT 2025": 11.88998449200843,
      "SWE-bench Pro": 20.211065853115457,
      "SWE-bench Verified": 20.855327333711788,
      "Tau-Bench Retail": 12.323475599772044,
      "Terminal-Bench 2.0": 16.364689711456045,
      "Terminal-Bench 1.0": 21.213141842054046
    },
    "evaluation_protocol": "Leave-one-model-out on filtered normalized (0-100) matrix, scored on observed entries only",
    "n_predictor_benchmarks": 35,
    "achieves_mae_under_5": false
  },
  "canonical_evaluation": {
    "canonical_overall_mae": 13.719801037488127,
    "canonical_coverage": 1.0,
    "canonical_per_benchmark_mae": {
      "AIME 2024": 6.4646771185281855,
      "AIME 2025": 11.594062923372984,
      "ARC-AGI-1": 17.569138941084088,
      "ARC-AGI-2": 10.199704698883775,
      "Codeforces Rating": 10.168252705840327,
      "CritPt": 12.335757726022061,
      "FrontierMath": 19.48055600426388,
      "GPQA Diamond": 6.245140499014777,
      "HLE (Humanity's Last Exam)": 18.140815987951985,
      "HMMT Feb 2025": 28.572801514438954,
      "IFEval": 4.626045828964383,
      "LiveCodeBench": 11.479000559004016,
      "MATH-500": 3.9469898126834213,
      "MMLU": 7.114266491677137,
      "MMMU": 14.469267587314663,
      "SimpleQA": 15.872129885947157,
      "SWE-bench Pro": 15.577484681337694,
      "Tau-Bench Retail": 4.137422055014731,
      "Terminal-Bench 2.0": 14.724946596379596,
      "HumanEval": 9.423854417564504,
      "MMLU-Pro": 9.080920055114788,
      "Terminal-Bench 1.0": 17.41469075804926,
      "BRUMO 2025": 9.335673859115184,
      "IMO 2025": 22.959190311615988,
      "MathArena Apex 2025": 11.12313211849591,
      "SMT 2025": 7.958065213268398,
      "SWE-bench Verified": 10.356879813459091,
      "USAMO 2025": 25.107846832379813,
      "BrowseComp": 22.4137103738642,
      "MathVision": 27.341380586061874,
      "AA Intelligence Index": 39.36081428923744,
      "Arena-Hard Auto": 17.636631084388235,
      "Chatbot Arena Elo": 15.090095836008787,
      "HMMT Nov 2025": 29.61289864866347,
      "LiveBench": 12.167236786869722,
      "MRCR v2": 33.22228705482292,
      "SimpleBench": 19.227614680547084,
      "Video-MMU": 26.174918560473188,
      "AA Long Context Reasoning": 54.88832152086836,
      "CMIMC 2025": 3.7678160232852917,
      "MMMU-Pro": 62.62518443982199,
      "OSWorld": 4.261538733434021,
      "BigCodeBench": 32.27962735648201
    },
    "n_predictions": 196,
    "n_held_out_pairs": 196,
    "n_scored": 196,
    "method": "3-way ensemble: low-rank projection (rank=8, w=0.4) + observed-only ridge (alpha=50, w=0.2) + WALS completion (rank=5, lam=10, w=0.4)"
  },
  "methodology_notes": "Pipeline: (1) Extracted 83x49 matrix from JSON, averaged 15 duplicate scores. (2) Filtered to denser submatrix (benchmarks >=10 models, models >=5 benchmarks), yielding 80x35 with 54% missingness. Mean-imputed and min-max normalized to 0-100. (3) SVD rank analysis on centered matrix; effective rank ~16 by 90% variance threshold, but dominant structure captured by first 2-3 components. (4) Greedy forward selection of 8 benchmarks minimizing LOO-MAE. (5) For canonical evaluation, used a 3-way ensemble: (a) low-rank projection finds the model's coordinates in the SVD subspace from its 5 revealed scores, then reconstructs all benchmarks; (b) ridge regression using only training models with observed values for revealed benchmarks (no imputation bias); (c) WALS matrix completion properly handles the missing data pattern. The matrix is strongly low-rank and benchmark performance is predictably structured, though high missingness (66% raw) and mixed metrics are the main methodological challenges. Scale mismatch addressed via per-benchmark normalization."
}