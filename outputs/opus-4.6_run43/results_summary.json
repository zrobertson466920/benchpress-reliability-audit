{
  "data_discovery": {
    "raw_schema": "JSON with keys: models (83 objects: id, name, provider, release_date, params_total_M, params_active_M, architecture, is_reasoning, open_weights), benchmarks (49 objects: id, name, category, metric, num_problems, source_url), scores (1390 entries: model_id, benchmark_id, score, reference_url), generated (timestamp).",
    "extraction_decisions": "Built 83x49 matrix from scores. 15 duplicate (model_id, benchmark_id) pairs with identical values, resolved by average. Ordering follows JSON array order. Scores directly reference model_id/benchmark_id.",
    "n_models_raw": 83,
    "n_benchmarks_raw": 49
  },
  "data": {
    "n_models": 65,
    "n_benchmarks": 35,
    "missing_fraction": 0.4853,
    "preprocessing": "Iteratively filtered benchmarks <12 obs and models <10 obs until stable. Per-benchmark min-max to 0-100. Iterative rank-10 SVD imputation (softImpute). Raw 83x49 (missing 66.2%) -> cleaned 65x35 (missing 48.5%).",
    "benchmarks_used": [
      "GPQA Diamond",
      "AIME 2025",
      "MMLU",
      "MMLU-Pro",
      "SWE-bench Verified",
      "MATH-500",
      "LiveCodeBench",
      "FrontierMath",
      "HLE (Humanity's Last Exam)",
      "ARC-AGI-2",
      "BrowseComp",
      "SimpleQA",
      "IFEval",
      "HumanEval",
      "Codeforces Rating",
      "OSWorld",
      "MMMU",
      "MMMU-Pro",
      "Arena-Hard Auto",
      "Chatbot Arena Elo",
      "SWE-bench Pro",
      "AIME 2024",
      "HMMT Feb 2025",
      "Tau-Bench Retail",
      "CritPt",
      "GSM8K",
      "Terminal-Bench 2.0",
      "Terminal-Bench 1.0",
      "ARC-AGI-1",
      "BRUMO 2025",
      "SMT 2025",
      "HMMT Nov 2025",
      "CMIMC 2025",
      "MathArena Apex 2025",
      "LiveBench"
    ]
  },
  "rank_analysis": {
    "method": "SVD on iteratively imputed (rank-10 softImpute), column-centered, min-max normalized 0-100 cleaned matrix",
    "effective_rank": 8,
    "variance_explained_by_rank": 0.9028,
    "singular_values": [
      593.7603,
      433.7013,
      333.5243,
      300.3505,
      276.1193,
      245.9541,
      239.6654,
      217.2581,
      204.2028,
      177.5594,
      73.4684,
      69.3909,
      60.2615,
      54.6498,
      48.6901
    ],
    "justification": "90% cumulative variance threshold gives rank 8. First component explains 32.4%, indicating a dominant general-capability factor with gradual tail of domain-specific dimensions. Ratio criterion (SV_k/SV_1>0.1) gives rank 13."
  },
  "benchmark_selection": {
    "method": "greedy_forward_selection",
    "selected_benchmarks": [
      "GPQA Diamond",
      "ARC-AGI-2",
      "GSM8K",
      "LiveCodeBench",
      "LiveBench"
    ],
    "n_selected": 5,
    "selection_criterion": "Minimize 5-fold CV MAE on normalized 0-100 cleaned matrix. Imputed features, Ridge alpha=1.0."
  },
  "prediction": {
    "method": "Ridge regression from selected subset (own eval); iterative rank-k SVD completion on full matrix (canonical eval).",
    "overall_mae": 12.2145,
    "per_benchmark_mae": {
      "AIME 2025": 9.0757,
      "MMLU": 7.2291,
      "MMLU-Pro": 7.456,
      "SWE-bench Verified": 10.2389,
      "MATH-500": 12.2756,
      "FrontierMath": 17.7061,
      "HLE (Humanity's Last Exam)": 10.4931,
      "BrowseComp": 14.4367,
      "SimpleQA": 11.7217,
      "IFEval": 9.3803,
      "HumanEval": 8.471,
      "Codeforces Rating": 10.486,
      "OSWorld": 26.1002,
      "MMMU": 12.9237,
      "MMMU-Pro": 23.3652,
      "Arena-Hard Auto": 19.1738,
      "Chatbot Arena Elo": 13.815,
      "SWE-bench Pro": 25.8951,
      "AIME 2024": 8.2773,
      "HMMT Feb 2025": 9.9444,
      "Tau-Bench Retail": 20.723,
      "CritPt": 8.9425,
      "Terminal-Bench 2.0": 14.7929,
      "Terminal-Bench 1.0": 14.3919,
      "ARC-AGI-1": 12.3927,
      "BRUMO 2025": 14.457,
      "SMT 2025": 18.7653,
      "HMMT Nov 2025": 28.9878,
      "CMIMC 2025": 12.6477,
      "MathArena Apex 2025": 15.8266
    },
    "evaluation_protocol": "5-fold CV on cleaned matrix (own); reveal-k-per-model SVD completion (canonical).",
    "n_predictor_benchmarks": 5,
    "achieves_mae_under_5": false
  },
  "methodology_notes": "Two approaches: (1) Ridge from greedy-selected benchmarks for own eval; (2) iterative low-rank SVD completion (rank-8) for canonical eval. Mixed metrics (Elo ~800-3000, pct 0-100, index scores) handled via per-benchmark min-max normalization. High missingness (66.2% raw) makes imputation quality critical. Iterative SVD converges quickly. Dominant first factor (general capability) with gradual tail. Scale mismatch is the main preprocessing challenge. Self-check canonical MAE: 11.3."
}