{
  "method": "Ridge regression (alpha=10) from all other imputed benchmarks to each target",
  "overall_mae": 12.424182730742961,
  "per_benchmark_mae": {
    "AIME 2024": 9.986333615871029,
    "AIME 2025": 11.40292012085796,
    "ARC-AGI-1": 34.98836830595421,
    "ARC-AGI-2": 17.22643273846035,
    "Arena-Hard Auto": 16.642559954272727,
    "BrowseComp": 14.556012096671697,
    "BRUMO 2025": 8.960343143970002,
    "Chatbot Arena Elo": 14.668953198268026,
    "CMIMC 2025": 13.419813121381466,
    "Codeforces Rating": 15.002004995540307,
    "CritPt": 12.214054371402716,
    "FrontierMath": 17.41843091039964,
    "GPQA Diamond": 7.714368834354576,
    "GSM8K": 8.40226111628429,
    "HLE (Humanity's Last Exam)": 18.514286775692494,
    "HMMT Feb 2025": 8.536220121119463,
    "HMMT Nov 2025": 15.640494842160933,
    "HumanEval": 7.790717132454668,
    "IFEval": 9.7509737197347,
    "LiveBench": 19.350246563573304,
    "LiveCodeBench": 11.81758289478036,
    "MATH-500": 9.0641512164136,
    "MathArena Apex 2025": 17.198340297512985,
    "MMLU": 9.01362975354612,
    "MMLU-Pro": 9.17479208154591,
    "MMMU": 13.989392483029707,
    "MMMU-Pro": 29.34323778318484,
    "OSWorld": 10.945869647786258,
    "SimpleQA": 12.462764742592762,
    "SMT 2025": 11.430465111644043,
    "SWE-bench Pro": 24.856975275905373,
    "SWE-bench Verified": 10.086874132480096,
    "Tau-Bench Retail": 16.12655712804476,
    "Terminal-Bench 2.0": 8.1222556034376,
    "Terminal-Bench 1.0": 14.606537425904971
  },
  "evaluation_protocol": "Leave-one-model-out CV on min-max normalized [0,100] scores; features from rank-5 imputed matrix, targets from observed values only",
  "n_predictor_benchmarks": 35,
  "n_errors": 1281,
  "achieves_mae_under_5": false
}