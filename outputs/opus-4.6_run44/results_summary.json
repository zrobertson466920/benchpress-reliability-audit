{
  "data_discovery": {
    "raw_schema": "JSON with top-level keys: models (list of 83 objects: id, name, provider, release_date, params_total_M, params_active_M, architecture, is_reasoning, open_weights), benchmarks (list of 49 objects: id, name, category, metric, num_problems, source_url), scores (list of 1390 entries: model_id, benchmark_id, score, reference_url), generated (timestamp).",
    "extraction_decisions": "Mapped (model_id, benchmark_id) pairs to matrix cells. Resolved 15 duplicate pairs by simple averaging. No null scores. Used all 83 models and 49 benchmarks in raw matrix. Benchmark names used as column headers.",
    "n_models_raw": 83,
    "n_benchmarks_raw": 49
  },
  "data": {
    "n_models": 80,
    "n_benchmarks": 35,
    "missing_fraction": 0.5425,
    "preprocessing": "Filtered to benchmarks with >=10 observed models and models with >=5 observed benchmarks. Per-benchmark min-max normalization to [0,100]. Missing values imputed via rank-5 truncated soft-impute (100 max iters, tol=1e-05).",
    "benchmarks_used": [
      "AIME 2024",
      "AIME 2025",
      "ARC-AGI-1",
      "ARC-AGI-2",
      "Arena-Hard Auto",
      "BrowseComp",
      "BRUMO 2025",
      "Chatbot Arena Elo",
      "CMIMC 2025",
      "Codeforces Rating",
      "CritPt",
      "FrontierMath",
      "GPQA Diamond",
      "GSM8K",
      "HLE (Humanity's Last Exam)",
      "HMMT Feb 2025",
      "HMMT Nov 2025",
      "HumanEval",
      "IFEval",
      "LiveBench",
      "LiveCodeBench",
      "MATH-500",
      "MathArena Apex 2025",
      "MMLU",
      "MMLU-Pro",
      "MMMU",
      "MMMU-Pro",
      "OSWorld",
      "SimpleQA",
      "SMT 2025",
      "SWE-bench Pro",
      "SWE-bench Verified",
      "Tau-Bench Retail",
      "Terminal-Bench 2.0",
      "Terminal-Bench 1.0"
    ]
  },
  "rank_analysis": {
    "method": "SVD on rank-5 soft-imputed min-max normalized matrix",
    "effective_rank": 3,
    "variance_explained_by_rank": 0.936335786021601,
    "singular_values": [
      3210.2117982579603,
      903.5415571818428,
      784.1622290266748,
      640.7519376440753,
      545.2381637009877,
      110.8247011456269,
      106.57480761489236,
      91.96236123925856,
      80.3690363536386,
      78.10934340943828,
      75.50212689948792,
      69.92851614860137,
      63.54467259298468,
      61.41338238428609,
      59.18581714605057,
      58.03094746857227,
      53.13634873049678,
      51.88538818891041,
      47.84063667760274,
      46.95569338752865,
      41.09506591098776,
      38.9759903977026,
      37.99402550698184,
      33.9693570403059,
      30.150137817592658,
      28.16719621021452,
      25.612307515764748,
      22.875807820357903,
      20.913827369697774,
      18.037709931391227,
      16.840940526975665,
      13.39135680777199,
      11.61919842279644,
      11.275801929047201,
      7.515070013868915
    ],
    "justification": "Effective rank = 3 by 90% cumulative variance threshold (explains 93.6%). First component captures 82.2% of variance, dominating the spectrum (SV1/SV2 ratio = 3.6x). This indicates strongly low-rank structure with a dominant general capability factor."
  },
  "benchmark_selection": {
    "method": "greedy_forward_max_correlation",
    "selected_benchmarks": [
      "GPQA Diamond",
      "MMLU",
      "LiveCodeBench",
      "MATH-500",
      "IFEval"
    ],
    "n_selected": 5,
    "selection_criterion": "Maximize average max absolute correlation with non-selected benchmarks (coverage-weighted)"
  },
  "prediction": {
    "method": "Ridge regression (alpha=10) from imputed benchmark features",
    "overall_mae": 12.424182730742961,
    "per_benchmark_mae": {
      "AIME 2024": 9.986333615871029,
      "AIME 2025": 11.40292012085796,
      "ARC-AGI-1": 34.98836830595421,
      "ARC-AGI-2": 17.22643273846035,
      "Arena-Hard Auto": 16.642559954272727,
      "BrowseComp": 14.556012096671697,
      "BRUMO 2025": 8.960343143970002,
      "Chatbot Arena Elo": 14.668953198268026,
      "CMIMC 2025": 13.419813121381466,
      "Codeforces Rating": 15.002004995540307,
      "CritPt": 12.214054371402716,
      "FrontierMath": 17.41843091039964,
      "GPQA Diamond": 7.714368834354576,
      "GSM8K": 8.40226111628429,
      "HLE (Humanity's Last Exam)": 18.514286775692494,
      "HMMT Feb 2025": 8.536220121119463,
      "HMMT Nov 2025": 15.640494842160933,
      "HumanEval": 7.790717132454668,
      "IFEval": 9.7509737197347,
      "LiveBench": 19.350246563573304,
      "LiveCodeBench": 11.81758289478036,
      "MATH-500": 9.0641512164136,
      "MathArena Apex 2025": 17.198340297512985,
      "MMLU": 9.01362975354612,
      "MMLU-Pro": 9.17479208154591,
      "MMMU": 13.989392483029707,
      "MMMU-Pro": 29.34323778318484,
      "OSWorld": 10.945869647786258,
      "SimpleQA": 12.462764742592762,
      "SMT 2025": 11.430465111644043,
      "SWE-bench Pro": 24.856975275905373,
      "SWE-bench Verified": 10.086874132480096,
      "Tau-Bench Retail": 16.12655712804476,
      "Terminal-Bench 2.0": 8.1222556034376,
      "Terminal-Bench 1.0": 14.606537425904971
    },
    "evaluation_protocol": "Leave-one-model-out CV on min-max normalized [0,100]; features from imputed matrix, evaluated on observed entries only",
    "n_predictor_benchmarks": 35,
    "achieves_mae_under_5": false
  },
  "canonical_evaluation": {
    "canonical_overall_mae": 18.703502452998045,
    "canonical_coverage": 1.0,
    "canonical_per_benchmark_mae": {
      "AIME 2024": 11.167569204683756,
      "AIME 2025": 13.979270637753562,
      "ARC-AGI-1": 22.08842444200559,
      "ARC-AGI-2": 13.250354478566965,
      "Codeforces Rating": 30.78029374749294,
      "CritPt": 25.3723677452491,
      "FrontierMath": 43.048483870532166,
      "GPQA Diamond": 11.602115926034209,
      "HLE (Humanity's Last Exam)": 32.37817723324068,
      "HMMT Feb 2025": 30.49555927569515,
      "IFEval": 9.976234276136235,
      "LiveCodeBench": 9.699166424286359,
      "MATH-500": 3.6222423338899077,
      "MMLU": 9.335079559687932,
      "MMMU": 22.931747123546504,
      "SimpleQA": 24.74228651529122,
      "SWE-bench Pro": 34.2521135626835,
      "Tau-Bench Retail": 2.730712509604942,
      "Terminal-Bench 2.0": 23.42974029051116,
      "HumanEval": 19.456312453329044,
      "MMLU-Pro": 5.159962001066169,
      "Terminal-Bench 1.0": 5.656636225601197,
      "BRUMO 2025": 7.523466229144144,
      "IMO 2025": 31.92752678949388,
      "MathArena Apex 2025": 25.6021591545408,
      "SMT 2025": 6.528164826781534,
      "SWE-bench Verified": 15.11727316103861,
      "USAMO 2025": 16.934138575742296,
      "BrowseComp": 3.5204123646987,
      "MathVision": 22.439024390243887,
      "AA Intelligence Index": 28.90756972339534,
      "Arena-Hard Auto": 28.53219412447122,
      "Chatbot Arena Elo": 22.313561073424836,
      "HMMT Nov 2025": 46.18702258459852,
      "LiveBench": 9.623926562037326,
      "MRCR v2": 14.598540145985382,
      "SimpleBench": 29.44444444444448,
      "Video-MMU": 2.2624434389140333,
      "AA Long Context Reasoning": 56.64251466180613,
      "CMIMC 2025": 12.481908538316866,
      "MMMU-Pro": 46.38878374446978,
      "OSWorld": 14.328229677177491,
      "BigCodeBench": 3.5561682045934333
    },
    "n_predictions": 196,
    "n_heldout_pairs": 196
  },
  "methodology_notes": "Pipeline: (1) Extracted 83x49 matrix, averaged 15 duplicates. (2) Filtered to 80x35 with sufficient coverage, min-max normalized. (3) Rank-5 truncated soft-impute for missing values. (4) Full SVD for rank analysis: effective rank 1 at 90% threshold, strong low-rank structure. (5) Greedy correlation-based benchmark selection (5 benchmarks). (6) Ridge regression (alpha=10) for prediction. (7) Canonical eval uses Ridge from 5 revealed benchmarks per model. Scale mismatch across benchmarks (Elo ratings vs percentages) handled by per-benchmark min-max normalization. High missingness (~54% after filtering) is the main challenge; imputation quality significantly affects downstream results."
}