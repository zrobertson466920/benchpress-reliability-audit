{
  "method": "Ridge regression (alpha=1.0) from selected benchmark subset to each target",
  "overall_mae": 5.5686,
  "per_benchmark_mae": {
    "GPQA Diamond": 7.7316,
    "AIME 2025": 7.283,
    "MMLU": 7.9024,
    "MMLU-Pro": 5.8048,
    "LiveCodeBench": 7.1366,
    "FrontierMath": 4.5914,
    "HLE (Humanity's Last Exam)": 4.9027,
    "ARC-AGI-2": 5.5562,
    "SimpleQA": 8.6255,
    "IFEval": 5.7244,
    "HumanEval": 6.9414,
    "Codeforces Rating": 7.3824,
    "OSWorld": 5.4737,
    "MMMU": 5.6175,
    "MMMU-Pro": 5.202,
    "Arena-Hard Auto": 8.5533,
    "Chatbot Arena Elo": 3.606,
    "SWE-bench Pro": 5.0103,
    "AIME 2024": 5.6406,
    "CritPt": 2.6605,
    "Terminal-Bench 2.0": 3.0118,
    "Terminal-Bench 1.0": 4.22,
    "ARC-AGI-1": 6.3785,
    "BRUMO 2025": 3.593,
    "SMT 2025": 4.0819,
    "CMIMC 2025": 3.5393,
    "MathArena Apex 2025": 5.1093,
    "LiveBench": 4.6403
  },
  "evaluation_protocol": "5-fold cross-validation on min-max normalized imputed matrix",
  "n_predictor_benchmarks": 7,
  "achieves_mae_under_5": false,
  "alpha": 1.0,
  "normalization": "min-max per benchmark to 0-100"
}