model_id,model_name,benchmark_id,benchmark_name,y_pred
claude-opus-4,Claude Opus 4,aime_2024,AIME 2024,57.3463
claude-opus-4,Claude Opus 4,aime_2025,AIME 2025,70.2887
claude-opus-4,Claude Opus 4,arc_agi_1,ARC-AGI-1,37.5715
claude-opus-4,Claude Opus 4,arc_agi_2,ARC-AGI-2,6.3794
claude-opus-4,Claude Opus 4,codeforces_rating,Codeforces Rating,1693.9936
claude-opus-4,Claude Opus 4,critpt,CritPt,5.2667
claude-opus-4,Claude Opus 4,frontiermath,FrontierMath,7.8826
claude-opus-4,Claude Opus 4,gpqa_diamond,GPQA Diamond,76.7996
claude-opus-4,Claude Opus 4,hle,HLE (Humanity's Last Exam),9.0838
claude-opus-4,Claude Opus 4,hmmt_2025,HMMT Feb 2025,24.345
claude-opus-4,Claude Opus 4,ifeval,IFEval,88.5041
claude-opus-4,Claude Opus 4,livecodebench,LiveCodeBench,54.6502
claude-opus-4,Claude Opus 4,math_500,MATH-500,94.1423
claude-opus-4,Claude Opus 4,mmlu,MMLU,86.8894
claude-opus-4,Claude Opus 4,mmmu,MMMU,74.3429
claude-opus-4,Claude Opus 4,simpleqa,SimpleQA,22.2597
claude-opus-4,Claude Opus 4,swe_bench_pro,SWE-bench Pro,38.093
claude-opus-4,Claude Opus 4,tau_bench_retail,Tau-Bench Retail,81.3765
claude-opus-4,Claude Opus 4,terminal_bench,Terminal-Bench 2.0,38.831
claude-opus-4.1,Claude Opus 4.1,frontiermath,FrontierMath,29.6304
claude-opus-4.1,Claude Opus 4.1,gpqa_diamond,GPQA Diamond,84.2125
claude-opus-4.1,Claude Opus 4.1,hle,HLE (Humanity's Last Exam),32.7027
claude-opus-4.1,Claude Opus 4.1,humaneval,HumanEval,90.8035
claude-opus-4.1,Claude Opus 4.1,mmlu,MMLU,87.8881
claude-opus-4.1,Claude Opus 4.1,mmlu_pro,MMLU-Pro,83.2743
claude-opus-4.1,Claude Opus 4.1,simpleqa,SimpleQA,55.2659
claude-opus-4.1,Claude Opus 4.1,swe_bench_pro,SWE-bench Pro,41.4795
claude-opus-4.1,Claude Opus 4.1,terminal_bench,Terminal-Bench 2.0,43.5002
claude-opus-4.1,Claude Opus 4.1,terminal_bench_1,Terminal-Bench 1.0,43.7782
deepseek-r1-0528,DeepSeek-R1-0528,aime_2024,AIME 2024,90.1471
deepseek-r1-0528,DeepSeek-R1-0528,aime_2025,AIME 2025,88.9208
deepseek-r1-0528,DeepSeek-R1-0528,arc_agi_1,ARC-AGI-1,57.3195
deepseek-r1-0528,DeepSeek-R1-0528,brumo_2025,BRUMO 2025,93.0245
deepseek-r1-0528,DeepSeek-R1-0528,codeforces_rating,Codeforces Rating,2328.2018
deepseek-r1-0528,DeepSeek-R1-0528,gpqa_diamond,GPQA Diamond,84.4222
deepseek-r1-0528,DeepSeek-R1-0528,hle,HLE (Humanity's Last Exam),26.3766
deepseek-r1-0528,DeepSeek-R1-0528,humaneval,HumanEval,90.6167
deepseek-r1-0528,DeepSeek-R1-0528,ifeval,IFEval,89.5381
deepseek-r1-0528,DeepSeek-R1-0528,imo_2025,IMO 2025,27.2608
deepseek-r1-0528,DeepSeek-R1-0528,livecodebench,LiveCodeBench,74.0726
deepseek-r1-0528,DeepSeek-R1-0528,math_500,MATH-500,96.9355
deepseek-r1-0528,DeepSeek-R1-0528,matharena_apex_2025,MathArena Apex 2025,5.2389
deepseek-r1-0528,DeepSeek-R1-0528,mmlu,MMLU,90.043
deepseek-r1-0528,DeepSeek-R1-0528,mmlu_pro,MMLU-Pro,85.0128
deepseek-r1-0528,DeepSeek-R1-0528,simpleqa,SimpleQA,51.8916
deepseek-r1-0528,DeepSeek-R1-0528,smt_2025,SMT 2025,87.5017
deepseek-r1-0528,DeepSeek-R1-0528,swe_bench_verified,SWE-bench Verified,69.479
deepseek-r1-0528,DeepSeek-R1-0528,usamo_2025,USAMO 2025,35.9492
doubao-seed-2.0-pro,Doubao Seed 2.0 Pro,aime_2024,AIME 2024,92.9569
doubao-seed-2.0-pro,Doubao Seed 2.0 Pro,aime_2025,AIME 2025,93.2649
doubao-seed-2.0-pro,Doubao Seed 2.0 Pro,browsecomp,BrowseComp,76.9711
doubao-seed-2.0-pro,Doubao Seed 2.0 Pro,gpqa_diamond,GPQA Diamond,87.4585
doubao-seed-2.0-pro,Doubao Seed 2.0 Pro,hle,HLE (Humanity's Last Exam),38.0475
doubao-seed-2.0-pro,Doubao Seed 2.0 Pro,ifeval,IFEval,90.6223
doubao-seed-2.0-pro,Doubao Seed 2.0 Pro,mathvision,MathVision,84.0458
doubao-seed-2.0-pro,Doubao Seed 2.0 Pro,mmlu,MMLU,89.2318
doubao-seed-2.0-pro,Doubao Seed 2.0 Pro,mmmu,MMMU,84.662
doubao-seed-2.0-pro,Doubao Seed 2.0 Pro,swe_bench_verified,SWE-bench Verified,75.699
doubao-seed-2.0-pro,Doubao Seed 2.0 Pro,terminal_bench,Terminal-Bench 2.0,50.658
gemini-2.5-pro,Gemini 2.5 Pro,aa_intelligence_index,AA Intelligence Index,62.875
gemini-2.5-pro,Gemini 2.5 Pro,aime_2024,AIME 2024,69.5226
gemini-2.5-pro,Gemini 2.5 Pro,aime_2025,AIME 2025,77.3443
gemini-2.5-pro,Gemini 2.5 Pro,arc_agi_1,ARC-AGI-1,43.3563
gemini-2.5-pro,Gemini 2.5 Pro,arc_agi_2,ARC-AGI-2,16.4818
gemini-2.5-pro,Gemini 2.5 Pro,arena_hard,Arena-Hard Auto,61.9649
gemini-2.5-pro,Gemini 2.5 Pro,brumo_2025,BRUMO 2025,91.9726
gemini-2.5-pro,Gemini 2.5 Pro,chatbot_arena_elo,Chatbot Arena Elo,1428.7917
gemini-2.5-pro,Gemini 2.5 Pro,codeforces_rating,Codeforces Rating,1963.9615
gemini-2.5-pro,Gemini 2.5 Pro,critpt,CritPt,5.2667
gemini-2.5-pro,Gemini 2.5 Pro,frontiermath,FrontierMath,22.2955
gemini-2.5-pro,Gemini 2.5 Pro,gpqa_diamond,GPQA Diamond,69.0157
gemini-2.5-pro,Gemini 2.5 Pro,hle,HLE (Humanity's Last Exam),25.5029
gemini-2.5-pro,Gemini 2.5 Pro,hmmt_2025,HMMT Feb 2025,66.57
gemini-2.5-pro,Gemini 2.5 Pro,hmmt_nov_2025,HMMT Nov 2025,89.4442
gemini-2.5-pro,Gemini 2.5 Pro,imo_2025,IMO 2025,28.9514
gemini-2.5-pro,Gemini 2.5 Pro,livebench,LiveBench,61.0083
gemini-2.5-pro,Gemini 2.5 Pro,math_500,MATH-500,91.3941
gemini-2.5-pro,Gemini 2.5 Pro,matharena_apex_2025,MathArena Apex 2025,7.7531
gemini-2.5-pro,Gemini 2.5 Pro,mmlu,MMLU,84.9053
gemini-2.5-pro,Gemini 2.5 Pro,mmlu_pro,MMLU-Pro,76.3506
gemini-2.5-pro,Gemini 2.5 Pro,mmmu,MMMU,78.4603
gemini-2.5-pro,Gemini 2.5 Pro,mrcr_v2,MRCR v2,72.6
gemini-2.5-pro,Gemini 2.5 Pro,simplebench,SimpleBench,66.64
gemini-2.5-pro,Gemini 2.5 Pro,simpleqa,SimpleQA,38.1755
gemini-2.5-pro,Gemini 2.5 Pro,smt_2025,SMT 2025,85.1137
gemini-2.5-pro,Gemini 2.5 Pro,swe_bench_verified,SWE-bench Verified,61.6879
gemini-2.5-pro,Gemini 2.5 Pro,terminal_bench_1,Terminal-Bench 1.0,28.3111
gemini-2.5-pro,Gemini 2.5 Pro,usamo_2025,USAMO 2025,20.9562
gemini-2.5-pro,Gemini 2.5 Pro,video_mmu,Video-MMU,85.2375
gpt-4.1,GPT-4.1,aime_2025,AIME 2025,90.1388
gpt-4.1,GPT-4.1,arc_agi_1,ARC-AGI-1,63.5277
gpt-4.1,GPT-4.1,arena_hard,Arena-Hard Auto,85.9941
gpt-4.1,GPT-4.1,codeforces_rating,Codeforces Rating,2210.1137
gpt-4.1,GPT-4.1,hle,HLE (Humanity's Last Exam),28.7697
gpt-4.1,GPT-4.1,hmmt_2025,HMMT Feb 2025,85.283
gpt-4.1,GPT-4.1,humaneval,HumanEval,90.8863
gpt-4.1,GPT-4.1,ifeval,IFEval,89.6528
gpt-4.1,GPT-4.1,livecodebench,LiveCodeBench,72.6478
gpt-4.1,GPT-4.1,math_500,MATH-500,96.5459
gpt-4.1,GPT-4.1,mmlu,MMLU,88.9501
gpt-4.1,GPT-4.1,mmmu,MMMU,83.052
gpt-4.1,GPT-4.1,simpleqa,SimpleQA,56.0565
gpt-4.1,GPT-4.1,swe_bench_verified,SWE-bench Verified,71.849
gpt-4.1,GPT-4.1,terminal_bench_1,Terminal-Bench 1.0,33.4547
grok-3-beta,Grok 3 Beta,aime_2024,AIME 2024,93.5956
grok-3-beta,Grok 3 Beta,aime_2025,AIME 2025,88.3546
grok-3-beta,Grok 3 Beta,arc_agi_1,ARC-AGI-1,46.691
grok-3-beta,Grok 3 Beta,chatbot_arena_elo,Chatbot Arena Elo,1436.5549
grok-3-beta,Grok 3 Beta,hle,HLE (Humanity's Last Exam),20.9421
grok-3-beta,Grok 3 Beta,humaneval,HumanEval,92.1983
grok-3-beta,Grok 3 Beta,ifeval,IFEval,88.6809
grok-3-beta,Grok 3 Beta,mmlu_pro,MMLU-Pro,81.1002
grok-3-beta,Grok 3 Beta,simpleqa,SimpleQA,29.2137
grok-3-beta,Grok 3 Beta,swe_bench_verified,SWE-bench Verified,67.4129
grok-4,Grok 4,aa_intelligence_index,AA Intelligence Index,67.6258
grok-4,Grok 4,aa_lcr,AA Long Context Reasoning,73.0341
grok-4,Grok 4,aime_2024,AIME 2024,92.7639
grok-4,Grok 4,arc_agi_2,ARC-AGI-2,20.9115
grok-4,Grok 4,brumo_2025,BRUMO 2025,94.9973
grok-4,Grok 4,chatbot_arena_elo,Chatbot Arena Elo,1440.2454
grok-4,Grok 4,cmimc_2025,CMIMC 2025,80.9909
grok-4,Grok 4,codeforces_rating,Codeforces Rating,2259.6489
grok-4,Grok 4,frontiermath,FrontierMath,22.1317
grok-4,Grok 4,gpqa_diamond,GPQA Diamond,86.3318
grok-4,Grok 4,hle,HLE (Humanity's Last Exam),30.2872
grok-4,Grok 4,hmmt_nov_2025,HMMT Nov 2025,90.7508
grok-4,Grok 4,ifeval,IFEval,89.2362
grok-4,Grok 4,imo_2025,IMO 2025,25.0499
grok-4,Grok 4,livecodebench,LiveCodeBench,75.2123
grok-4,Grok 4,matharena_apex_2025,MathArena Apex 2025,9.1815
grok-4,Grok 4,mmlu,MMLU,89.7784
grok-4,Grok 4,mmlu_pro,MMLU-Pro,86.236
grok-4,Grok 4,mmmu,MMMU,81.3967
grok-4,Grok 4,mmmu_pro,MMMU-Pro,75.972
grok-4,Grok 4,osworld,OSWorld,53.1255
grok-4,Grok 4,simpleqa,SimpleQA,52.6099
grok-4,Grok 4,smt_2025,SMT 2025,88.5722
grok-4,Grok 4,swe_bench_pro,SWE-bench Pro,44.1842
grok-4,Grok 4,swe_bench_verified,SWE-bench Verified,74.6595
grok-4,Grok 4,terminal_bench,Terminal-Bench 2.0,43.8257
grok-4,Grok 4,terminal_bench_1,Terminal-Bench 1.0,35.3155
grok-4,Grok 4,usamo_2025,USAMO 2025,18.9272
kimi-k2,Kimi K2,aime_2024,AIME 2024,77.2534
kimi-k2,Kimi K2,aime_2025,AIME 2025,81.8216
kimi-k2,Kimi K2,arena_hard,Arena-Hard Auto,61.9649
kimi-k2,Kimi K2,gpqa_diamond,GPQA Diamond,82.6232
kimi-k2,Kimi K2,hle,HLE (Humanity's Last Exam),18.8151
kimi-k2,Kimi K2,hmmt_2025,HMMT Feb 2025,86.1369
kimi-k2,Kimi K2,ifeval,IFEval,86.3733
kimi-k2,Kimi K2,livecodebench,LiveCodeBench,79.5568
kimi-k2,Kimi K2,math_500,MATH-500,97.4629
kimi-k2,Kimi K2,mmlu_pro,MMLU-Pro,86.62
kimi-k2,Kimi K2,osworld,OSWorld,44.6197
kimi-k2,Kimi K2,simpleqa,SimpleQA,46.723
kimi-k2,Kimi K2,swe_bench_verified,SWE-bench Verified,69.5553
llama-4-maverick,Llama 4 Maverick,arc_agi_1,ARC-AGI-1,16.3212
llama-4-maverick,Llama 4 Maverick,bigcodebench,BigCodeBench,44.7869
llama-4-maverick,Llama 4 Maverick,chatbot_arena_elo,Chatbot Arena Elo,1390.812
llama-4-maverick,Llama 4 Maverick,gpqa_diamond,GPQA Diamond,69.4949
llama-4-maverick,Llama 4 Maverick,humaneval,HumanEval,86.7675
llama-4-maverick,Llama 4 Maverick,livecodebench,LiveCodeBench,50.7374
llama-4-maverick,Llama 4 Maverick,math_500,MATH-500,91.3755
llama-4-maverick,Llama 4 Maverick,mmmu,MMMU,74.5547
llama-4-maverick,Llama 4 Maverick,simpleqa,SimpleQA,32.5824
llama-4-maverick,Llama 4 Maverick,swe_bench_verified,SWE-bench Verified,52.7428
llama-4-maverick,Llama 4 Maverick,terminal_bench_1,Terminal-Bench 1.0,21.224
minimax-m2,MiniMax-M2,aime_2024,AIME 2024,90.4204
minimax-m2,MiniMax-M2,aime_2025,AIME 2025,91.724
minimax-m2,MiniMax-M2,chatbot_arena_elo,Chatbot Arena Elo,1454.7986
minimax-m2,MiniMax-M2,gpqa_diamond,GPQA Diamond,85.1693
minimax-m2,MiniMax-M2,hle,HLE (Humanity's Last Exam),33.5838
minimax-m2,MiniMax-M2,humaneval,HumanEval,92.5847
minimax-m2,MiniMax-M2,ifeval,IFEval,89.0364
minimax-m2,MiniMax-M2,math_500,MATH-500,98.0657
minimax-m2,MiniMax-M2,mmlu,MMLU,89.9842
minimax-m2,MiniMax-M2,mmlu_pro,MMLU-Pro,85.2251
minimax-m2,MiniMax-M2,mmmu,MMMU,83.2434
minimax-m2,MiniMax-M2,simpleqa,SimpleQA,52.1959
minimax-m2,MiniMax-M2,swe_bench_pro,SWE-bench Pro,41.8081
minimax-m2,MiniMax-M2,swe_bench_verified,SWE-bench Verified,72.698
minimax-m2,MiniMax-M2,terminal_bench,Terminal-Bench 2.0,36.081
o3-mini-high,o3-mini (high),aime_2024,AIME 2024,86.1422
o3-mini-high,o3-mini (high),arc_agi_1,ARC-AGI-1,43.9148
o3-mini-high,o3-mini (high),arc_agi_2,ARC-AGI-2,19.9467
o3-mini-high,o3-mini (high),arena_hard,Arena-Hard Auto,81.5008
o3-mini-high,o3-mini (high),chatbot_arena_elo,Chatbot Arena Elo,1427.4085
o3-mini-high,o3-mini (high),frontiermath,FrontierMath,17.2369
o3-mini-high,o3-mini (high),hle,HLE (Humanity's Last Exam),22.7836
o3-mini-high,o3-mini (high),hmmt_2025,HMMT Feb 2025,73.8707
o3-mini-high,o3-mini (high),humaneval,HumanEval,89.5458
o3-mini-high,o3-mini (high),ifeval,IFEval,87.6602
o3-mini-high,o3-mini (high),livecodebench,LiveCodeBench,69.7854
o3-mini-high,o3-mini (high),math_500,MATH-500,95.3082
o3-mini-high,o3-mini (high),simpleqa,SimpleQA,39.3708
o3-mini-high,o3-mini (high),swe_bench_verified,SWE-bench Verified,67.3301
o3-mini-high,o3-mini (high),usamo_2025,USAMO 2025,23.6482
