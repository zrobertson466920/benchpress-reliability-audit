{
  "data_discovery": {
    "raw_schema": "JSON with keys: models (list of 83 dicts with id/name/provider/release_date/params_total_M/params_active_M/architecture/is_reasoning/open_weights), benchmarks (list of 49 dicts with id/name/category/metric/num_problems/source_url), scores (list of 1390 dicts with model_id/benchmark_id/score/reference_url), generated (timestamp string)",
    "extraction_decisions": "Mapped scores to (model_id, benchmark_id) pairs. 15 duplicate pairs found and resolved by simple averaging. No null scores found. Matrix indexed by model_id (rows) x benchmark_id (columns).",
    "n_models_raw": 83,
    "n_benchmarks_raw": 49
  },
  "data": {
    "n_models": 80,
    "n_benchmarks": 35,
    "missing_fraction": 0.5425,
    "preprocessing": "Filtered to benchmarks with >=10 model scores and models with >=5 benchmark scores. Min-max normalized per benchmark to 0-100 scale. Missing values imputed via iterative soft-impute (rank-3 SVD, 200 max iterations, tol=1e-4).",
    "benchmarks_used": [
      "GPQA Diamond",
      "AIME 2025",
      "MMLU",
      "MMLU-Pro",
      "SWE-bench Verified",
      "MATH-500",
      "LiveCodeBench",
      "FrontierMath",
      "HLE (Humanity's Last Exam)",
      "ARC-AGI-2",
      "BrowseComp",
      "SimpleQA",
      "IFEval",
      "HumanEval",
      "Codeforces Rating",
      "OSWorld",
      "MMMU",
      "MMMU-Pro",
      "Arena-Hard Auto",
      "Chatbot Arena Elo",
      "SWE-bench Pro",
      "AIME 2024",
      "HMMT Feb 2025",
      "Tau-Bench Retail",
      "CritPt",
      "GSM8K",
      "Terminal-Bench 2.0",
      "Terminal-Bench 1.0",
      "ARC-AGI-1",
      "BRUMO 2025",
      "SMT 2025",
      "HMMT Nov 2025",
      "CMIMC 2025",
      "MathArena Apex 2025",
      "LiveBench"
    ]
  },
  "rank_analysis": {
    "method": "SVD on min-max normalized soft-imputed matrix",
    "effective_rank": 2,
    "variance_explained_by_rank": 0.9209,
    "singular_values": [
      3249.1914,
      1176.2335,
      929.3748,
      135.0058,
      131.3998,
      124.9264,
      116.9229,
      113.981,
      102.9757,
      93.4099,
      89.7168,
      82.6138,
      78.3873,
      73.1894,
      69.127
    ],
    "justification": "Using 90% cumulative variance threshold on SVD of the normalized imputed matrix (80x35). The first 2 singular values capture 92.1% of total variance, indicating strong low-rank structure."
  },
  "benchmark_selection": {
    "method": "greedy_forward_selection",
    "selected_benchmarks": [
      "Tau-Bench Retail",
      "GSM8K",
      "HMMT Feb 2025",
      "HMMT Nov 2025",
      "BrowseComp",
      "MATH-500",
      "SWE-bench Verified"
    ],
    "n_selected": 7,
    "selection_criterion": "Minimize OLS projection MAE on normalized imputed matrix; greedy forward addition of 7 benchmarks."
  },
  "prediction": {
    "method": "Ridge regression (alpha=1.0) from selected benchmark subset to each target",
    "overall_mae": 5.5686,
    "per_benchmark_mae": {
      "GPQA Diamond": 7.7316,
      "AIME 2025": 7.283,
      "MMLU": 7.9024,
      "MMLU-Pro": 5.8048,
      "LiveCodeBench": 7.1366,
      "FrontierMath": 4.5914,
      "HLE (Humanity's Last Exam)": 4.9027,
      "ARC-AGI-2": 5.5562,
      "SimpleQA": 8.6255,
      "IFEval": 5.7244,
      "HumanEval": 6.9414,
      "Codeforces Rating": 7.3824,
      "OSWorld": 5.4737,
      "MMMU": 5.6175,
      "MMMU-Pro": 5.202,
      "Arena-Hard Auto": 8.5533,
      "Chatbot Arena Elo": 3.606,
      "SWE-bench Pro": 5.0103,
      "AIME 2024": 5.6406,
      "CritPt": 2.6605,
      "Terminal-Bench 2.0": 3.0118,
      "Terminal-Bench 1.0": 4.22,
      "ARC-AGI-1": 6.3785,
      "BRUMO 2025": 3.593,
      "SMT 2025": 4.0819,
      "CMIMC 2025": 3.5393,
      "MathArena Apex 2025": 5.1093,
      "LiveBench": 4.6403
    },
    "evaluation_protocol": "5-fold cross-validation on min-max normalized imputed matrix",
    "n_predictor_benchmarks": 7,
    "achieves_mae_under_5": false
  },
  "canonical_evaluation": {
    "canonical_overall_mae": 15.76,
    "n_predictions": 196,
    "n_held_out": 196,
    "coverage": 1.0,
    "method": "Blend of ridge regression (alpha=10, weight=2) and KNN (k=7, inverse-distance weighted, weight=1.5) with benchmark-mean prior (weight=0.3), all in normalized 0-100 space. Clipped to [0,100]. Ridge trained on other models using 5 revealed benchmarks as features. KNN finds nearest neighbors in revealed-benchmark space using RMSE distance."
  },
  "methodology_notes": "Pipeline: (1) Extract 83x49 matrix from JSON, average 15 duplicate pairs. (2) Filter to benchmarks with >=10 models and models with >=5 benchmarks. (3) Min-max normalize per benchmark to 0-100. (4) Impute missing via iterative rank-3 SVD (soft-impute). (5) SVD for rank analysis. (6) Greedy forward selection of 7 benchmarks minimizing OLS projection MAE. (7) Ridge regression (alpha=1.0) for prediction with 5-fold CV. (8) Canonical eval: blend of ridge regression (from 5 revealed benchmarks, alpha=10) + KNN (k=7, inverse-distance) + benchmark-mean prior, all in normalized 0-100 space with clipping. Key choices: filtered matrix (not full), min-max normalization (not z-score), soft-impute (not mean imputation), ridge (not lasso/OLS). Scale mismatch between Elo-rated and percentage benchmarks motivated min-max normalization."
}