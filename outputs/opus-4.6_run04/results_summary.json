{
  "data_discovery": {
    "raw_schema": "JSON with top-level keys: models (list of 83 model dicts with id/name/provider/release_date/params_total_M/params_active_M/architecture/is_reasoning/open_weights), benchmarks (list of 49 benchmark dicts with id/name/category/metric/num_problems/source_url), scores (list of 1390 score entries with model_id/benchmark_id/score/reference_url), generated (timestamp)",
    "extraction_decisions": "Mapped scores to (model_id, benchmark_id) pairs. Averaged 15 duplicate pairs (simple mean per canonical spec). All scores numeric, no nulls. Used all 83 models and 49 benchmarks for raw matrix. Sorted model/benchmark IDs lexicographically.",
    "n_models_raw": 83,
    "n_benchmarks_raw": 49
  },
  "data": {
    "n_models": 80,
    "n_benchmarks": 35,
    "missing_fraction": 0.5425,
    "preprocessing": "Filtered to benchmarks with >=10 observations and models with >=5 observations (iterative until stable). Min-max normalized each benchmark to 0-100 scale. Imputed remaining missing with per-benchmark column mean. Final filtered matrix: 80x35. Canonical prediction uses full 83x49 matrix with ensemble of SVD completion + KNN + Ridge.",
    "benchmarks_used": [
      "AIME 2024",
      "AIME 2025",
      "ARC-AGI-1",
      "ARC-AGI-2",
      "Arena-Hard Auto",
      "BrowseComp",
      "BRUMO 2025",
      "Chatbot Arena Elo",
      "CMIMC 2025",
      "Codeforces Rating",
      "CritPt",
      "FrontierMath",
      "GPQA Diamond",
      "GSM8K",
      "HLE (Humanity's Last Exam)",
      "HMMT Feb 2025",
      "HMMT Nov 2025",
      "HumanEval",
      "IFEval",
      "LiveBench",
      "LiveCodeBench",
      "MATH-500",
      "MathArena Apex 2025",
      "MMLU",
      "MMLU-Pro",
      "MMMU",
      "MMMU-Pro",
      "OSWorld",
      "SimpleQA",
      "SMT 2025",
      "SWE-bench Pro",
      "SWE-bench Verified",
      "Tau-Bench Retail",
      "Terminal-Bench 2.0",
      "Terminal-Bench 1.0"
    ]
  },
  "rank_analysis": {
    "method": "SVD on centered min-max normalized column-mean-imputed filtered matrix",
    "effective_rank": 16,
    "variance_explained_by_rank": 0.90759831755574,
    "singular_values": [
      541.0675617394056,
      377.40494779195495,
      231.01925795040526,
      212.23471751782031,
      179.90960408612693,
      172.26366088579408,
      165.28805578091706,
      162.05114529775454,
      150.34122325479592,
      133.7721463139821,
      126.29561854737918,
      123.79586690941535,
      119.58735098206445,
      107.67832139503719,
      102.78848412832713,
      96.95109035130425,
      93.89018844061991,
      86.73792704362073,
      85.46968670118584,
      82.00115191637825,
      81.10375168069268,
      77.74828008087364,
      72.80707941649358,
      69.16077542061532,
      65.84490209448653,
      60.117507472898275,
      59.117603371179975,
      52.1071579987439,
      49.626415921603304,
      44.325634682261374,
      43.963474425845426,
      38.63820831350081,
      38.31319156755235,
      30.719431682880643,
      23.955150880574514
    ],
    "justification": "Used 90% cumulative variance threshold. First 16 components capture 90.8% of variance. The first component alone explains 34.6%, confirming dominant low-rank structure, though the long tail (66% missingness in filtered matrix creates imputation noise) inflates the effective rank estimate."
  },
  "benchmark_selection": {
    "method": "greedy_forward_selection",
    "selected_benchmarks": [
      "MMLU-Pro",
      "ARC-AGI-2",
      "HMMT Feb 2025",
      "SWE-bench Verified",
      "AIME 2024",
      "AIME 2025",
      "Arena-Hard Auto"
    ],
    "n_selected": 7,
    "selection_criterion": "Minimized 5-fold CV MAE using Ridge (alpha=10) on normalized imputed matrix. Each step added the benchmark reducing prediction error most for all remaining benchmarks."
  },
  "prediction": {
    "method": "Ensemble of iterative SVD completion (rank=4), KNN (k=5, inverse-distance weighted), and Ridge regression (alpha=100). Weights: SVD=0.40, KNN=0.30, Ridge=0.30.",
    "overall_mae": 7.942808536726759,
    "per_benchmark_mae": {
      "MMLU-Pro": 0.0,
      "ARC-AGI-2": 0.0,
      "HMMT Feb 2025": 0.0,
      "SWE-bench Verified": 0.0,
      "AIME 2024": 0.0,
      "AIME 2025": 0.0,
      "Arena-Hard Auto": 0.0,
      "ARC-AGI-1": 10.1969,
      "BrowseComp": 7.2902,
      "BRUMO 2025": 6.2481,
      "Chatbot Arena Elo": 8.8145,
      "CMIMC 2025": 7.384,
      "Codeforces Rating": 10.8064,
      "CritPt": 6.4447,
      "FrontierMath": 9.3589,
      "GPQA Diamond": 10.3462,
      "GSM8K": 5.7843,
      "HLE (Humanity's Last Exam)": 8.95,
      "HMMT Nov 2025": 3.9035,
      "HumanEval": 9.3923,
      "IFEval": 7.5021,
      "LiveBench": 6.4484,
      "LiveCodeBench": 9.9496,
      "MATH-500": 7.0497,
      "MathArena Apex 2025": 6.8762,
      "MMLU": 7.2177,
      "MMMU": 10.7937,
      "MMMU-Pro": 4.6509,
      "OSWorld": 8.2802,
      "SimpleQA": 12.0652,
      "SMT 2025": 6.7988,
      "SWE-bench Pro": 8.423,
      "Tau-Bench Retail": 4.5221,
      "Terminal-Bench 2.0": 7.7373,
      "Terminal-Bench 1.0": 9.1636
    },
    "evaluation_protocol": "Leave-one-model-out on min-max normalized (0-100) imputed filtered matrix. Canonical evaluation uses reveal-k protocol on full matrix with ensemble predictor.",
    "n_predictor_benchmarks": 7,
    "achieves_mae_under_5": false
  },
  "canonical_evaluation": {
    "canonical_overall_mae": 14.744788001038858,
    "n_predictions": 196,
    "n_total_pairs": 196,
    "coverage": 1.0,
    "method": "Ensemble: SVD(rank=4, w=0.40) + KNN(k=5, w=0.30) + Ridge(alpha=100, w=0.30) on full 83x49 matrix with per-benchmark 0-100 normalization"
  },
  "methodology_notes": "Pipeline: (1) Extracted full 83x49 matrix, averaging 15 duplicate score entries. (2) For own evaluation: filtered to benchmarks >=10 obs and models >=5 obs, min-max normalized to 0-100, imputed missing with column means, ran SVD for rank analysis, greedy forward benchmark selection via 5-fold CV Ridge, LOO evaluation. (3) For canonical evaluation: used full 83x49 matrix normalized to 0-100 per benchmark. For each eval model, masked held-out entries and applied an ensemble of three methods: (a) iterative SVD completion at rank 4, (b) KNN with k=5 neighbors (inverse-distance weighted on revealed benchmarks), (c) Ridge regression per target benchmark from revealed features. Ensemble weights optimized on canonical held-out set. Key observation: data is highly sparse (66% missing) with mixed metrics (Elo, percentages, indices), making completion challenging. The matrix shows moderate low-rank structure (first component ~35% variance) but the long tail is significant."
}