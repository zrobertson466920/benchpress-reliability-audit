{
  "method": "Ridge regression from selected subset to remaining benchmarks",
  "overall_mae": 7.942808536726759,
  "per_benchmark_mae": {
    "MMLU-Pro": 0.0,
    "ARC-AGI-2": 0.0,
    "HMMT Feb 2025": 0.0,
    "SWE-bench Verified": 0.0,
    "AIME 2024": 0.0,
    "AIME 2025": 0.0,
    "Arena-Hard Auto": 0.0,
    "ARC-AGI-1": 10.1969,
    "BrowseComp": 7.2902,
    "BRUMO 2025": 6.2481,
    "Chatbot Arena Elo": 8.8145,
    "CMIMC 2025": 7.384,
    "Codeforces Rating": 10.8064,
    "CritPt": 6.4447,
    "FrontierMath": 9.3589,
    "GPQA Diamond": 10.3462,
    "GSM8K": 5.7843,
    "HLE (Humanity's Last Exam)": 8.95,
    "HMMT Nov 2025": 3.9035,
    "HumanEval": 9.3923,
    "IFEval": 7.5021,
    "LiveBench": 6.4484,
    "LiveCodeBench": 9.9496,
    "MATH-500": 7.0497,
    "MathArena Apex 2025": 6.8762,
    "MMLU": 7.2177,
    "MMMU": 10.7937,
    "MMMU-Pro": 4.6509,
    "OSWorld": 8.2802,
    "SimpleQA": 12.0652,
    "SMT 2025": 6.7988,
    "SWE-bench Pro": 8.423,
    "Tau-Bench Retail": 4.5221,
    "Terminal-Bench 2.0": 7.7373,
    "Terminal-Bench 1.0": 9.1636
  },
  "evaluation_protocol": "Leave-one-model-out on min-max normalized (0-100) imputed matrix",
  "n_predictor_benchmarks": 7,
  "achieves_mae_under_5": false,
  "alpha": 10.0,
  "note": "MAE on 0-100 normalized scale"
}