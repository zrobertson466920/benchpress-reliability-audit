{
  "data_discovery": {
    "raw_schema": "JSON with top-level keys: models (list of 83 model objects with id, name, provider, release_date, params_total_M, params_active_M, architecture, is_reasoning, open_weights), benchmarks (list of 49 benchmark objects with id, name, category, metric, num_problems, source_url), scores (list of 1390 score entries with model_id, benchmark_id, score, reference_url), generated (timestamp string '2026-02-24T12:03:39.038392').",
    "extraction_decisions": "Built model x benchmark matrix using model_id and benchmark_id as keys. 15 duplicate (model_id, benchmark_id) pairs found, all with identical scores, resolved by averaging per canonical spec. No null scores in dataset. Scores range from 0 to 3020 due to mixed metrics: most benchmarks use '% correct' (0-100 scale) but Codeforces Rating and Chatbot Arena Elo use Elo ratings (1000-3000 range), and AA Intelligence Index uses a custom index score.",
    "n_models_raw": 83,
    "n_benchmarks_raw": 49
  },
  "data": {
    "n_models": 80,
    "n_benchmarks": 35,
    "missing_fraction": 0.5425,
    "preprocessing": "Filtered to benchmarks with >=10 model coverage (35/49 kept) and models with >=5 benchmark coverage in filtered set (80/83 kept). Mean imputation per benchmark column for SVD and ridge regression. Z-score normalization per benchmark for SVD only; raw scores used for prediction and canonical evaluation.",
    "benchmarks_used": [
      "AIME 2024",
      "AIME 2025",
      "ARC-AGI-1",
      "ARC-AGI-2",
      "Arena-Hard Auto",
      "BrowseComp",
      "BRUMO 2025",
      "Chatbot Arena Elo",
      "CMIMC 2025",
      "Codeforces Rating",
      "CritPt",
      "FrontierMath",
      "GPQA Diamond",
      "GSM8K",
      "HLE (Humanity's Last Exam)",
      "HMMT Feb 2025",
      "HMMT Nov 2025",
      "HumanEval",
      "IFEval",
      "LiveBench",
      "LiveCodeBench",
      "MATH-500",
      "MathArena Apex 2025",
      "MMLU",
      "MMLU-Pro",
      "MMMU",
      "MMMU-Pro",
      "OSWorld",
      "SimpleQA",
      "SMT 2025",
      "SWE-bench Pro",
      "SWE-bench Verified",
      "Tau-Bench Retail",
      "Terminal-Bench 2.0",
      "Terminal-Bench 1.0"
    ]
  },
  "rank_analysis": {
    "method": "SVD on z-scored mean-imputed filtered matrix (80 models x 35 benchmarks)",
    "effective_rank": 17,
    "variance_explained_by_rank": 0.9023,
    "singular_values": [
      27.1991,
      21.511,
      14.1408,
      13.1209,
      12.1578,
      10.3512,
      9.9523,
      9.5991,
      9.3692,
      8.2093,
      7.9698,
      7.7354,
      7.277,
      6.9666,
      6.9043,
      6.3987,
      6.0892,
      6.0392,
      5.3267,
      5.1694
    ],
    "justification": "Using 90% cumulative variance threshold on z-scored imputed matrix gives effective rank 17. The spectrum decays gradually without a sharp elbow: SV1=27.2, SV2=21.5, SV3=14.1. The first component explains only 26.4% of variance, suggesting no single dominant factor. High effective rank partly reflects noise from mean imputation on a 54% missing matrix. A rank-3 model suffices for good prediction (canonical MAE ~16 on 0-100 scale)."
  },
  "benchmark_selection": {
    "method": "greedy_forward_selection",
    "selected_benchmarks": [
      "ARC-AGI-2",
      "LiveCodeBench",
      "SimpleQA",
      "AIME 2025",
      "Codeforces Rating",
      "FrontierMath",
      "GPQA Diamond",
      "ARC-AGI-1"
    ],
    "n_selected": 8,
    "selection_criterion": "Maximize mean absolute correlation with remaining unselected benchmarks at each greedy step, using the imputed correlation matrix."
  },
  "prediction": {
    "method": "Blended low-rank SVD projection + Ridge regression. For canonical eval: rank-3 SVD on z-scored training matrix (excluding eval model), project eval model into latent space via revealed benchmarks, reconstruct scores, blend 50/50 with per-target Ridge regression, clip to [min-5%, max+5%] of observed range.",
    "overall_mae": 8.0558,
    "per_benchmark_mae": {
      "AIME 2024": 10.9909,
      "Arena-Hard Auto": 21.0703,
      "BrowseComp": 8.4592,
      "BRUMO 2025": 5.591,
      "Chatbot Arena Elo": 24.8823,
      "CMIMC 2025": 8.7344,
      "CritPt": 3.0542,
      "GSM8K": 5.8859,
      "HLE (Humanity's Last Exam)": 8.1098,
      "HMMT Feb 2025": 18.307,
      "HMMT Nov 2025": 3.4934,
      "HumanEval": 4.9381,
      "IFEval": 4.2362,
      "LiveBench": 8.4969,
      "MATH-500": 4.1624,
      "MathArena Apex 2025": 6.5114,
      "MMLU": 4.2235,
      "MMLU-Pro": 5.6327,
      "MMMU": 4.3468,
      "MMMU-Pro": 5.7299,
      "OSWorld": 9.0581,
      "SMT 2025": 5.5196,
      "SWE-bench Pro": 6.498,
      "SWE-bench Verified": 10.8366,
      "Tau-Bench Retail": 3.5832,
      "Terminal-Bench 2.0": 10.416,
      "Terminal-Bench 1.0": 11.277
    },
    "evaluation_protocol": "Leave-one-model-out cross-validation on filtered imputed matrix (own eval). Canonical: reveal-5-per-model with SVD+Ridge blend.",
    "n_predictor_benchmarks": 8,
    "achieves_mae_under_5": false
  },
  "canonical_evaluation": {
    "canonical_overall_mae": 15.7894,
    "canonical_coverage": 1.0,
    "canonical_per_benchmark_mae": {
      "AA Intelligence Index": 61.8182,
      "AA Long Context Reasoning": 13.2529,
      "AIME 2024": 10.1635,
      "AIME 2025": 11.9156,
      "ARC-AGI-1": 19.7078,
      "ARC-AGI-2": 12.8676,
      "Arena-Hard Auto": 19.3741,
      "BigCodeBench": 6.8519,
      "BrowseComp": 32.411,
      "BRUMO 2025": 6.4401,
      "Chatbot Arena Elo": 15.2807,
      "CMIMC 2025": 3.9989,
      "Codeforces Rating": 7.2071,
      "CritPt": 28.4825,
      "FrontierMath": 26.3785,
      "GPQA Diamond": 5.1398,
      "HLE (Humanity's Last Exam)": 19.4988,
      "HMMT Feb 2025": 26.4586,
      "HMMT Nov 2025": 43.507,
      "HumanEval": 10.3429,
      "IFEval": 6.6736,
      "IMO 2025": 52.971,
      "LiveBench": 3.3322,
      "LiveCodeBench": 13.6292,
      "MATH-500": 4.7753,
      "MathArena Apex 2025": 5.2158,
      "MathVision": 38.5907,
      "MMLU": 7.8335,
      "MMLU-Pro": 8.3136,
      "MMMU": 15.9279,
      "MMMU-Pro": 67.1004,
      "MRCR v2": 72.2393,
      "OSWorld": 14.6975,
      "SimpleBench": 27.1906,
      "SimpleQA": 19.9687,
      "SMT 2025": 6.4158,
      "SWE-bench Pro": 12.7411,
      "SWE-bench Verified": 10.9128,
      "Tau-Bench Retail": 24.8602,
      "Terminal-Bench 2.0": 11.9075,
      "Terminal-Bench 1.0": 16.9721,
      "USAMO 2025": 37.2373,
      "Video-MMU": 16.8575
    },
    "method": "Rank-3 SVD projection blended 50/50 with Ridge regression, clipped to 5% margin beyond observed range",
    "hyperparameters": {
      "K_RANK": 3,
      "lambda_svd": 0.1,
      "lambda_ridge": 1.0,
      "blend_lr": 0.5,
      "clip_margin_fraction": 0.05
    }
  },
  "methodology_notes": "Pipeline: (1) Extracted 83x49 raw matrix from JSON; averaged 15 duplicate score entries (all identical). (2) Filtered to 80x35 submatrix (thresholds: >=10 models/benchmark, >=5 benchmarks/model). (3) Mean-imputed missing values per benchmark column. (4) Z-score normalized for SVD; raw scores for prediction. (5) Greedy correlation-based benchmark selection yielded 8 benchmarks. (6) Own evaluation: Ridge regression LOO, MAE=8.06. (7) Canonical evaluation: rank-3 SVD projection + Ridge blend with clipping, MAE=15.79 (normalized 0-100). Key limitations: (a) 66% missing data inflates effective rank under mean imputation; (b) mixed metric scales (Elo ~1000-3000 vs % 0-100) create heterogeneity partially addressed by per-benchmark z-scoring for SVD; (c) low-coverage benchmarks (e.g., 2-8 models) contribute high variance to predictions; (d) the 5-revealed-benchmark canonical protocol is challenging when target benchmarks have very different scales or few training examples."
}