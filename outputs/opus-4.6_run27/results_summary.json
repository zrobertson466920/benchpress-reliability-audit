{
  "data_discovery": {
    "raw_schema": "JSON: models (83), benchmarks (49), scores (1390), generated. Models have id/name/provider/release_date/params/architecture/is_reasoning/open_weights. Benchmarks have id/name/category/metric/num_problems/source_url. Scores have model_id/benchmark_id/score/reference_url.",
    "extraction_decisions": "Built model_id x benchmark_id matrix. 15 duplicate pairs averaged. Mixed metrics: % correct, Elo ratings, index scores, etc.",
    "n_models_raw": 83,
    "n_benchmarks_raw": 49
  },
  "data": {
    "n_models": 65,
    "n_benchmarks": 35,
    "missing_fraction": 0.48527472527472526,
    "preprocessing": "Filtered: models with >=10 benchmarks, benchmarks with >=10 models (83x49 -> 65x35). Min-max normalized per benchmark to [0,100]. Iterative SVD (EM) for completion.",
    "benchmarks_used": [
      "AIME 2024",
      "AIME 2025",
      "ARC-AGI-1",
      "ARC-AGI-2",
      "Arena-Hard Auto",
      "BrowseComp",
      "BRUMO 2025",
      "Chatbot Arena Elo",
      "CMIMC 2025",
      "Codeforces Rating",
      "CritPt",
      "FrontierMath",
      "GPQA Diamond",
      "GSM8K",
      "HLE (Humanity's Last Exam)",
      "HMMT Feb 2025",
      "HMMT Nov 2025",
      "HumanEval",
      "IFEval",
      "LiveBench",
      "LiveCodeBench",
      "MATH-500",
      "MathArena Apex 2025",
      "MMLU",
      "MMLU-Pro",
      "MMMU",
      "MMMU-Pro",
      "OSWorld",
      "SimpleQA",
      "SMT 2025",
      "SWE-bench Pro",
      "SWE-bench Verified",
      "Tau-Bench Retail",
      "Terminal-Bench 2.0",
      "Terminal-Bench 1.0"
    ]
  },
  "rank_analysis": {
    "method": "Iterative SVD (EM-style) with 10% held-out CV for rank selection",
    "effective_rank": 4,
    "variance_explained_by_rank": 0.9461352951092158,
    "singular_values": [
      814.5075349064982,
      690.8512187845548,
      663.9312547224615,
      470.8123878111315,
      118.68830045258197,
      110.2349842157828,
      98.69599976734149,
      94.81255087897176,
      87.92034627640979,
      76.94794767603105,
      73.98147829638911,
      71.40453126827897,
      67.3141748652703,
      59.47779361713637,
      57.237483504227235,
      54.04540646525679,
      53.299252085835654,
      50.84282216130502,
      45.40888415178661,
      45.06481566777444,
      39.70738923006757,
      37.55841529004325,
      35.11193930518235,
      33.678447436907426,
      31.10753621531563,
      29.728517948396185,
      25.138773082819668,
      20.525508365214847,
      19.136961603224982,
      17.728670645363373,
      14.82464226836951,
      13.611381849573725,
      12.71942648241477,
      10.361182972582496,
      8.156504810872967
    ],
    "justification": "CV rank selection: rank 4 minimized holdout MAE (12.85). Top 4 SVs (815,691,664,471) capture 94.6% variance; sharp drop at SV5 (119). The matrix is strongly low-rank and benchmark performance is predictably structured."
  },
  "benchmark_selection": {
    "method": "greedy_forward_selection",
    "selected_benchmarks": [
      "CritPt",
      "AIME 2024",
      "MMMU-Pro",
      "BRUMO 2025",
      "OSWorld",
      "Terminal-Bench 1.0",
      "MathArena Apex 2025"
    ],
    "n_selected": 7,
    "selection_criterion": "Minimize Ridge LOO MAE on SVD-completed matrix"
  },
  "prediction": {
    "method": "Ridge regression (alpha=1.0) from selected subset",
    "overall_mae": 7.406202437079689,
    "per_benchmark_mae": {
      "AIME 2025": 8.481052010078423,
      "ARC-AGI-1": 8.311518100056162,
      "ARC-AGI-2": 7.837833405049912,
      "Arena-Hard Auto": 8.063810885096029,
      "BrowseComp": 7.522182316555033,
      "Chatbot Arena Elo": 7.131262629459596,
      "CMIMC 2025": 12.5711776788834,
      "Codeforces Rating": 7.931783597463562,
      "FrontierMath": 9.625547238389434,
      "GPQA Diamond": 5.860474813948801,
      "GSM8K": 4.603408684898283,
      "HLE (Humanity's Last Exam)": 8.02387010782634,
      "HMMT Feb 2025": 4.923720388902155,
      "HMMT Nov 2025": 10.602033155985271,
      "HumanEval": 5.963007468484194,
      "IFEval": 6.26124663419486,
      "LiveBench": 9.876880775960165,
      "LiveCodeBench": 6.536825590600372,
      "MATH-500": 8.579898159862772,
      "MMLU": 5.55024578709644,
      "MMLU-Pro": 5.55861608147309,
      "MMMU": 9.35625732821485,
      "SimpleQA": 9.577175196107854,
      "SMT 2025": 7.061793800280924,
      "SWE-bench Pro": 11.488280888957975,
      "SWE-bench Verified": 8.711717232113875,
      "Tau-Bench Retail": 7.163131959385484,
      "Terminal-Bench 2.0": 4.303820939271584
    },
    "evaluation_protocol": "LOO on observed entries (fallback to completed matrix for sparse benchmarks)",
    "n_predictor_benchmarks": 7,
    "achieves_mae_under_5": false
  },
  "canonical_evaluation": {
    "method": "Ensemble: iterative SVD completion (rank-4, wt=1), KNN (K=10 inv-dist, wt=2), Ridge from revealed (alpha=10, wt=1). Per-benchmark normalization to [0,100].",
    "canonical_overall_mae": 15.970637800172955,
    "n_predictions": 196,
    "coverage": 1.0
  },
  "methodology_notes": "Pipeline: (1) 83x49 raw matrix, 15 dup pairs averaged. (2) Filter >=10/10 -> 65x35. (3) Min-max norm to [0,100]. (4) Iterative SVD: CV rank=4. (5) Greedy subset selection of 7 benchmarks on completed matrix. (6) Canonical eval: KNN-weighted ensemble (SVD + KNN + Ridge from revealed). High missingness (66%) is the main challenge; sparse benchmarks (<10 obs) in canonical eval are hardest. Scale mismatch handled by per-benchmark normalization."
}