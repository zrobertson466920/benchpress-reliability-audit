{
  "data_discovery": {
    "raw_schema": "JSON with top-level keys: models (list of 83 dicts with id/name/provider/etc), benchmarks (list of 49 dicts with id/name/category/metric), scores (list of 1390 dicts with model_id/benchmark_id/score/reference_url), generated (timestamp string).",
    "extraction_decisions": "Built 83x49 matrix from scores. 15 duplicate (model_id, benchmark_id) pairs found (all had identical scores); averaged duplicates per canonical protocol. All scores numeric. Model IDs and benchmark IDs used as internal identifiers; names used for display.",
    "n_models_raw": 83,
    "n_benchmarks_raw": 49
  },
  "data": {
    "n_models": 80,
    "n_benchmarks": 35,
    "missing_fraction": 0.5425,
    "preprocessing": "Filtered to benchmarks with >= 10 model scores and models with >= 5 benchmark scores. Min-max normalized each benchmark to 0-100 scale. Missing values imputed via iterative rank-5 SVD completion (column-mean initialization, 20 max iterations).",
    "benchmarks_used": [
      "AIME 2024",
      "AIME 2025",
      "ARC-AGI-1",
      "ARC-AGI-2",
      "Arena-Hard Auto",
      "BrowseComp",
      "BRUMO 2025",
      "Chatbot Arena Elo",
      "CMIMC 2025",
      "Codeforces Rating",
      "CritPt",
      "FrontierMath",
      "GPQA Diamond",
      "GSM8K",
      "HLE (Humanity's Last Exam)",
      "HMMT Feb 2025",
      "HMMT Nov 2025",
      "HumanEval",
      "IFEval",
      "LiveBench",
      "LiveCodeBench",
      "MATH-500",
      "MathArena Apex 2025",
      "MMLU",
      "MMLU-Pro",
      "MMMU",
      "MMMU-Pro",
      "OSWorld",
      "SimpleQA",
      "SMT 2025",
      "SWE-bench Pro",
      "SWE-bench Verified",
      "Tau-Bench Retail",
      "Terminal-Bench 2.0",
      "Terminal-Bench 1.0"
    ]
  },
  "rank_analysis": {
    "method": "SVD on min-max normalized (0-100), iteratively imputed matrix",
    "effective_rank": 2,
    "variance_explained_by_rank": 0.9351,
    "singular_values": [
      3255.5465,
      645.5531,
      540.016,
      444.4762,
      402.3983,
      124.1176,
      118.7179,
      101.5375,
      88.4927,
      85.5235,
      82.6926,
      77.821,
      74.3223,
      72.9667,
      66.6589
    ],
    "justification": "Effective rank = 2 using 90% cumulative variance threshold. The first singular value dominates (3255.5 vs 645.6), indicating a strong general capability factor. Rapid decay confirms strong low-rank structure."
  },
  "benchmark_selection": {
    "method": "Greedy forward selection maximizing mean absolute correlation with unselected benchmarks",
    "selected_benchmarks": [
      "Chatbot Arena Elo",
      "AIME 2025",
      "GPQA Diamond",
      "BRUMO 2025",
      "CritPt",
      "SMT 2025",
      "ARC-AGI-2"
    ],
    "n_selected": 7,
    "selection_criterion": "Mean absolute Pearson correlation with remaining benchmarks (on observed entries, min 5 pairs)"
  },
  "prediction": {
    "method": "Ridge regression (RidgeCV, alphas=[0.1,1,10,100]) from selected benchmark subset to each target",
    "overall_mae": 11.0248,
    "per_benchmark_mae": {
      "AIME 2024": 9.1084,
      "AIME 2025": 9.9994,
      "ARC-AGI-1": 13.512,
      "ARC-AGI-2": 7.9036,
      "Arena-Hard Auto": 15.3621,
      "BrowseComp": 13.4713,
      "BRUMO 2025": 9.2745,
      "Chatbot Arena Elo": 12.9389,
      "CMIMC 2025": 13.3848,
      "Codeforces Rating": 10.1969,
      "CritPt": 12.3317,
      "FrontierMath": 15.1406,
      "GPQA Diamond": 9.5429,
      "GSM8K": 12.3515,
      "HLE (Humanity's Last Exam)": 12.4751,
      "HMMT Feb 2025": 11.113,
      "HMMT Nov 2025": 21.0692,
      "HumanEval": 9.3353,
      "IFEval": 7.5901,
      "LiveBench": 18.834,
      "LiveCodeBench": 7.8595,
      "MATH-500": 7.801,
      "MathArena Apex 2025": 16.7506,
      "MMLU": 9.484,
      "MMLU-Pro": 8.3165,
      "MMMU": 8.5099,
      "MMMU-Pro": 22.7258,
      "OSWorld": 17.3634,
      "SimpleQA": 9.2955,
      "SMT 2025": 7.3005,
      "SWE-bench Pro": 16.8712,
      "SWE-bench Verified": 13.8044,
      "Tau-Bench Retail": 23.5277,
      "Terminal-Bench 2.0": 11.7437,
      "Terminal-Bench 1.0": 26.1801
    },
    "evaluation_protocol": "Leave-one-model-out cross-validation on originally observed entries only (normalized 0-100 scale)",
    "n_predictor_benchmarks": 7,
    "achieves_mae_under_5": false
  },
  "canonical_evaluation": {
    "canonical_overall_mae": 15.95,
    "n_predictions": 196,
    "n_heldout_pairs": 196,
    "coverage": 1.0,
    "method": "Two-stage: (1) iterative rank-3 SVD completion on full matrix with eval model's heldout masked, (2) ridge regression from 5 revealed benchmarks, (3) 60/40 ridge-SVD blend with clipping to observed score range. Fallback to SVD-only or benchmark mean when insufficient data."
  },
  "methodology_notes": "Pipeline: (1) Extract 83x49 matrix from JSON, average 15 duplicate pairs (all identical). (2) Filter to benchmarks with >=10 models and models with >=5 benchmarks for analysis matrix (80x35). (3) Min-max normalize per benchmark to 0-100. (4) Impute missing via iterative rank-5 SVD. (5) SVD for rank analysis: dominant rank-1 factor (90% variance), effective rank 2 at 93.5%. (6) Greedy forward benchmark selection by correlation coverage (7 benchmarks). (7) Ridge regression for own evaluation (LOO-model CV, MAE ~11 on 0-100). (8) For canonical eval: two-stage approach combining iterative rank-3 SVD matrix completion with ridge regression from the 5 revealed benchmarks, blended 60/40 ridge-SVD with clipping to observed ranges. Key choices: filtered rather than full matrix for analysis, min-max normalization, 90% variance threshold for effective rank, correlation-based subset selection, ridge+SVD blend for canonical."
}