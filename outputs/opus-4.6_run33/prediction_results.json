{
  "method": "Ridge from 5 selected benchmarks on iteratively-completed matrix",
  "overall_mae": 9.9805,
  "per_benchmark_mae": {
    "GPQA Diamond": 7.3596,
    "AIME 2025": 8.2209,
    "MMLU": 8.7758,
    "MMLU-Pro": 7.9857,
    "SWE-bench Verified": 12.3267,
    "MATH-500": 4.6035,
    "LiveCodeBench": 8.1289,
    "HLE (Humanity's Last Exam)": 10.8564,
    "ARC-AGI-2": 10.7454,
    "IFEval": 4.7676,
    "HumanEval": 8.3926,
    "Codeforces Rating": 10.9972,
    "Chatbot Arena Elo": 9.3045,
    "HMMT Feb 2025": 12.4058,
    "ARC-AGI-1": 12.9319,
    "MMMU": 12.2004,
    "Terminal-Bench 1.0": 18.208,
    "BrowseComp": 10.7383,
    "MMMU-Pro": 23.0441,
    "CritPt": 11.867,
    "BRUMO 2025": 9.0629,
    "SMT 2025": 11.7074,
    "CMIMC 2025": 11.4848,
    "OSWorld": 17.6568,
    "SWE-bench Pro": 14.3832,
    "Terminal-Bench 2.0": 11.2376,
    "MathArena Apex 2025": 18.9861,
    "LiveBench": 16.1292,
    "Tau-Bench Retail": 13.1819,
    "GSM8K": 6.7072
  },
  "evaluation_protocol": "LOO-model, observed-only MAE, 0-100 normalized",
  "n_predictor_benchmarks": 5,
  "achieves_mae_under_5": false,
  "alpha": 10.0
}