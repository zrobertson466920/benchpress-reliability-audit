{
  "data_discovery": {
    "raw_schema": "JSON with keys: models (83 objects: id/name/provider/release_date/params/architecture/is_reasoning/open_weights), benchmarks (49 objects: id/name/category/metric/num_problems/source_url), scores (1390 entries: model_id/benchmark_id/score/reference_url), generated (timestamp).",
    "extraction_decisions": "Mapped scores to (model_id, benchmark_id) pairs. 15 exact-duplicate entries averaged. Flat relational structure, no joins. 1375 unique pairs in 83x49 matrix.",
    "n_models_raw": 83,
    "n_benchmarks_raw": 49
  },
  "data": {
    "n_models": 65,
    "n_benchmarks": 35,
    "missing_fraction": 0.4853,
    "preprocessing": "Full 83x49 matrix for canonical eval. Filtered to benchmarks>=12obs, models>=10obs for rank/selection analysis. Min-max normalized per benchmark to [0,100]. Iterative SVD completion for missing data.",
    "benchmarks_used": [
      "GPQA Diamond",
      "AIME 2025",
      "MMLU",
      "MMLU-Pro",
      "SWE-bench Verified",
      "MATH-500",
      "LiveCodeBench",
      "FrontierMath",
      "HLE (Humanity's Last Exam)",
      "ARC-AGI-2",
      "BrowseComp",
      "SimpleQA",
      "IFEval",
      "HumanEval",
      "Codeforces Rating",
      "OSWorld",
      "MMMU",
      "MMMU-Pro",
      "Arena-Hard Auto",
      "Chatbot Arena Elo",
      "SWE-bench Pro",
      "AIME 2024",
      "HMMT Feb 2025",
      "Tau-Bench Retail",
      "CritPt",
      "GSM8K",
      "Terminal-Bench 2.0",
      "Terminal-Bench 1.0",
      "ARC-AGI-1",
      "BRUMO 2025",
      "SMT 2025",
      "HMMT Nov 2025",
      "CMIMC 2025",
      "MathArena Apex 2025",
      "LiveBench"
    ]
  },
  "rank_analysis": {
    "method": "SVD on iteratively completed min-max normalized matrix, CV rank selection",
    "effective_rank": 1,
    "variance_explained_by_rank": 0.3438,
    "singular_values": [
      485.694,
      331.1311,
      222.007,
      196.4794,
      175.1911,
      156.1966,
      150.0745,
      145.6259,
      131.3154,
      124.6963,
      120.3336,
      111.7347,
      101.5791,
      100.0716,
      91.6212
    ],
    "justification": "CV over ranks 1-10 selects rank 1 (MAE=11.11). Top component explains 34.4% variance. Effective rank at 90% cumvar: 15. CV-selected rank preferred as it accounts for overfitting on sparse data.",
    "effective_rank_90pct": 15,
    "effective_rank_95pct": 20
  },
  "benchmark_selection": {
    "method": "greedy_forward_selection",
    "selected_benchmarks": [
      "HMMT Nov 2025",
      "AIME 2024",
      "Arena-Hard Auto",
      "SimpleQA",
      "FrontierMath"
    ],
    "n_selected": 5,
    "selection_criterion": "Minimize LOO-model MAE on observed entries, Ridge alpha=10, iterative-SVD-completed matrix"
  },
  "prediction": {
    "method": "Ensemble: iterative SVD completion + Ridge from revealed + KNN in latent space",
    "overall_mae": 9.9805,
    "per_benchmark_mae": {
      "GPQA Diamond": 7.3596,
      "AIME 2025": 8.2209,
      "MMLU": 8.7758,
      "MMLU-Pro": 7.9857,
      "SWE-bench Verified": 12.3267,
      "MATH-500": 4.6035,
      "LiveCodeBench": 8.1289,
      "HLE (Humanity's Last Exam)": 10.8564,
      "ARC-AGI-2": 10.7454,
      "IFEval": 4.7676,
      "HumanEval": 8.3926,
      "Codeforces Rating": 10.9972,
      "Chatbot Arena Elo": 9.3045,
      "HMMT Feb 2025": 12.4058,
      "ARC-AGI-1": 12.9319,
      "MMMU": 12.2004,
      "Terminal-Bench 1.0": 18.208,
      "BrowseComp": 10.7383,
      "MMMU-Pro": 23.0441,
      "CritPt": 11.867,
      "BRUMO 2025": 9.0629,
      "SMT 2025": 11.7074,
      "CMIMC 2025": 11.4848,
      "OSWorld": 17.6568,
      "SWE-bench Pro": 14.3832,
      "Terminal-Bench 2.0": 11.2376,
      "MathArena Apex 2025": 18.9861,
      "LiveBench": 16.1292,
      "Tau-Bench Retail": 13.1819,
      "GSM8K": 6.7072
    },
    "evaluation_protocol": "LOO-model on completed matrix (own); reveal-k canonical (canonical)",
    "n_predictor_benchmarks": 5,
    "achieves_mae_under_5": false
  },
  "canonical_evaluation": {
    "canonical_overall_mae": 13.3925,
    "canonical_coverage": 1.0,
    "canonical_per_benchmark_mae": {
      "AIME 2024": 5.899,
      "AIME 2025": 8.5022,
      "ARC-AGI-1": 14.6034,
      "ARC-AGI-2": 10.207,
      "Codeforces Rating": 9.8135,
      "CritPt": 14.0325,
      "FrontierMath": 13.0943,
      "GPQA Diamond": 4.2754,
      "HLE (Humanity's Last Exam)": 15.1238,
      "HMMT Feb 2025": 24.0641,
      "IFEval": 5.8457,
      "LiveCodeBench": 10.8574,
      "MATH-500": 3.9269,
      "MMLU": 6.2344,
      "MMMU": 13.278,
      "SimpleQA": 16.3115,
      "SWE-bench Pro": 16.2436,
      "Tau-Bench Retail": 5.6871,
      "Terminal-Bench 2.0": 13.9949,
      "HumanEval": 10.1337,
      "MMLU-Pro": 6.2609,
      "Terminal-Bench 1.0": 10.9372,
      "BRUMO 2025": 9.0542,
      "IMO 2025": 35.9414,
      "MathArena Apex 2025": 5.1999,
      "SMT 2025": 9.5888,
      "SWE-bench Verified": 9.6914,
      "USAMO 2025": 31.277,
      "BrowseComp": 11.2251,
      "MathVision": 85.2099,
      "AA Intelligence Index": 54.762,
      "Arena-Hard Auto": 17.4632,
      "Chatbot Arena Elo": 15.557,
      "HMMT Nov 2025": 33.2962,
      "LiveBench": 21.4957,
      "MRCR v2": 87.3415,
      "SimpleBench": 4.4444,
      "Video-MMU": 15.8662,
      "AA Long Context Reasoning": 50.9227,
      "CMIMC 2025": 3.7947,
      "MMMU-Pro": 63.5032,
      "OSWorld": 7.1494,
      "BigCodeBench": 1.8519
    },
    "method": "Ensemble of iterative SVD completion on full 83x49 + Ridge from revealed + KNN latent. Clamped to [0,100]."
  },
  "methodology_notes": "Key decisions: (1) Full 83x49 matrix kept for canonical eval; filtered submatrix for rank analysis. (2) Min-max normalization handles heterogeneous scales. (3) Iterative SVD completion avoids mean-imputation rank inflation. (4) CV rank selection. (5) Greedy forward benchmark subset selection on observed-only LOO MAE. (6) 3-method ensemble for canonical predictions. (7) Predictions clamped to [0,100]. (8) No nonlinear transforms."
}