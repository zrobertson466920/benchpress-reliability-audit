{
  "data_discovery": {
    "raw_schema": "Dict with keys: models (list of 83 dicts with id/name/provider/release_date/params/architecture/is_reasoning), benchmarks (list of 49 dicts with id/name/category/metric/num_problems/source_url), scores (list of 1390 dicts with model_id/benchmark_id/score/reference_url), generated (timestamp string)",
    "extraction_decisions": "1) Used model 'id' as primary key, 'name' for display. 2) Used benchmark 'id' as primary key, 'name' for column headers. 3) Resolved 15 duplicate (model_id, benchmark_id) pairs by simple averaging. 4) Scores with null values excluded (none found). 5) All 83 models and 49 benchmarks retained in raw matrix.",
    "n_models_raw": 83,
    "n_benchmarks_raw": 49
  },
  "data": {
    "n_models": 83,
    "n_benchmarks": 49,
    "missing_fraction": 0.661912957954266,
    "preprocessing": "Per-benchmark min-max normalization to 0-100 scale (handles mixed metrics: percentages, Elo ratings, index scores). Missing values imputed via iterative SVD completion (soft-impute, rank=5, max_iter=200, tol=1e-5). No models or benchmarks dropped. Full matrix retained.",
    "benchmarks_used": [
      "GPQA Diamond",
      "AIME 2025",
      "MMLU",
      "MMLU-Pro",
      "SWE-bench Verified",
      "MATH-500",
      "LiveCodeBench",
      "FrontierMath",
      "HLE (Humanity's Last Exam)",
      "ARC-AGI-2",
      "BrowseComp",
      "SimpleQA",
      "IFEval",
      "HumanEval",
      "Codeforces Rating",
      "OSWorld",
      "MMMU",
      "MMMU-Pro",
      "Arena-Hard Auto",
      "Chatbot Arena Elo",
      "SWE-bench Pro",
      "AIME 2024",
      "HMMT Feb 2025",
      "Tau-Bench Retail",
      "Tau-Bench Telecom",
      "Video-MMU",
      "MRCR v2",
      "AA Intelligence Index",
      "AA Long Context Reasoning",
      "CritPt",
      "SciCode",
      "MathVision",
      "GDP-Val AA",
      "GSM8K",
      "IFBench",
      "Terminal-Bench 2.0",
      "Terminal-Bench 1.0",
      "ARC-AGI-1",
      "BRUMO 2025",
      "SMT 2025",
      "USAMO 2025",
      "HMMT Nov 2025",
      "CMIMC 2025",
      "IMO 2025",
      "AIME 2026",
      "MathArena Apex 2025",
      "LiveBench",
      "SimpleBench",
      "BigCodeBench"
    ]
  },
  "rank_analysis": {
    "method": "SVD on min-max normalized, soft-impute completed matrix (83x49)",
    "effective_rank": 4,
    "variance_explained_by_rank": 0.9415355065546215,
    "singular_values": [
      3768.4045483366385,
      1264.1402065411023,
      1182.9240926639018,
      1064.739855392858,
      1022.1709288656533,
      108.94630167416979,
      106.84378249685047,
      93.99067804921069,
      84.44751106214005,
      77.82046171888449,
      73.6117892854745,
      71.85775813512296,
      66.53113844474781,
      65.46533342499745,
      61.706665816447256,
      54.81752655928834,
      53.16292260901065,
      49.52827282890136,
      48.9905564778701,
      45.13012109075445,
      43.83758732988016,
      41.047326421786664,
      40.374204125827575,
      37.320464393556,
      30.825832058236713,
      30.173834109620223,
      27.291859757789876,
      23.711226842659887,
      22.19276895919056,
      21.668359996136072,
      18.51622166443598,
      16.369242268077617,
      15.481238251530748,
      12.49040115158076,
      10.438432272766404,
      10.1675068524725,
      9.60870660070552,
      7.700719243870246,
      7.607228828618747,
      6.694701187885933,
      5.77634221617487,
      4.839496120462686,
      4.788219201196112,
      3.9236870994020583,
      3.253762017077202,
      2.8520391161910235,
      1.948883863791534,
      1.462103891206146,
      1.114448633050103
    ],
    "justification": "Using 90% cumulative variance threshold on SVD of the imputed normalized matrix, effective rank = 4. The first singular value dominates (3768.4 vs 1264.1 for second), indicating a strong general capability factor. The top 4 components capture 94.2% of variance."
  },
  "benchmark_selection": {
    "method": "greedy_forward_selection",
    "selected_benchmarks": [
      "HMMT Feb 2025",
      "IMO 2025",
      "CMIMC 2025",
      "Terminal-Bench 1.0",
      "GSM8K",
      "SimpleBench",
      "IFBench"
    ],
    "n_selected": 7,
    "selection_criterion": "Minimize leave-one-model-out MAE using ridge regression (alpha=10) from selected benchmarks to all remaining benchmarks on the normalized imputed matrix."
  },
  "prediction": {
    "method": "ridge_regression_from_selected_subset",
    "overall_mae": 3.1941788702301945,
    "per_benchmark_mae": {
      "GPQA Diamond": 5.851433647180178,
      "AIME 2025": 5.716170916296168,
      "MMLU": 5.733921534366217,
      "MMLU-Pro": 4.5712132439248965,
      "SWE-bench Verified": 6.305208668852116,
      "MATH-500": 5.4564483907707,
      "LiveCodeBench": 6.1465937359624405,
      "FrontierMath": 2.692006643978855,
      "HLE (Humanity's Last Exam)": 4.907320704916567,
      "ARC-AGI-2": 3.0778061525365397,
      "BrowseComp": 1.9371269408658243,
      "SimpleQA": 5.934885543135415,
      "IFEval": 5.107898100633911,
      "HumanEval": 5.3609926415819515,
      "Codeforces Rating": 6.337495846663157,
      "OSWorld": 2.992151488116029,
      "MMMU": 3.7875007577955038,
      "MMMU-Pro": 2.867230736807231,
      "Arena-Hard Auto": 2.975364754415831,
      "Chatbot Arena Elo": 2.6137839088017687,
      "SWE-bench Pro": 3.4600821242129816,
      "AIME 2024": 5.95882413412709,
      "HMMT Feb 2025": 0.0,
      "Tau-Bench Retail": 2.047715413267142,
      "Tau-Bench Telecom": 2.197139748307331,
      "Video-MMU": 1.3931001416696491,
      "MRCR v2": 2.0253654688376668,
      "AA Intelligence Index": 1.485717182370445,
      "AA Long Context Reasoning": 1.7895880431515752,
      "CritPt": 1.5277906032844106,
      "SciCode": 1.0100663002951895,
      "MathVision": 1.0714538230401964,
      "GDP-Val AA": 1.200655511618239,
      "GSM8K": 0.0,
      "IFBench": 0.0,
      "Terminal-Bench 2.0": 2.591077656919447,
      "Terminal-Bench 1.0": 0.0,
      "ARC-AGI-1": 3.3283004827860294,
      "BRUMO 2025": 1.6664276223207657,
      "SMT 2025": 2.1824580938467903,
      "USAMO 2025": 1.7130350876127636,
      "HMMT Nov 2025": 1.6317924160665893,
      "CMIMC 2025": 0.0,
      "IMO 2025": 0.0,
      "AIME 2026": 0.48494451287794427,
      "MathArena Apex 2025": 1.9066630689994308,
      "LiveBench": 2.0247753219435056,
      "SimpleBench": 0.0,
      "BigCodeBench": 1.085985434511723
    },
    "evaluation_protocol": "leave_one_model_out_CV_on_normalized_imputed_matrix",
    "n_predictor_benchmarks": 7,
    "achieves_mae_under_5": true
  },
  "canonical_evaluation": {
    "canonical_overall_mae": 12.980687941084389,
    "canonical_coverage": 1.0,
    "n_predictions": 196,
    "n_heldout_pairs": 196,
    "method_note": "For each eval model, trained ridge regression on all other models using the 5 revealed benchmarks as features (imputed feature matrix) to predict each held-out benchmark. Only models with observed target values used as training targets. Adaptive regularization alpha (5-30) based on training set size. Predictions clipped to [0,100] normalized then converted back to raw scale."
  },
  "methodology_notes": "Full pipeline: (1) Extracted all 83x49 matrix from JSON with duplicate averaging. (2) Min-max normalized each benchmark to 0-100 to handle heterogeneous metrics. (3) Imputed missing values (66.2% missing) via iterative rank-5 SVD (soft-impute). (4) SVD on completed matrix for rank analysis. (5) Greedy forward selection of 7 benchmarks minimizing LOO ridge regression error. (6) Ridge regression predictor with alpha=10. (7) Canonical evaluation using reveal-k protocol with ridge from revealed\u2192heldout trained on other models. Key decision: kept all models/benchmarks rather than filtering sparse rows/columns, relying on imputation to handle missingness. This preserves maximum information but makes the imputation step critical."
}