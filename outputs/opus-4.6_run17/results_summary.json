{
  "data_discovery": {
    "raw_schema": "JSON with keys: models (83 objects: id/name/provider/release_date/params_total_M/params_active_M/architecture/is_reasoning/open_weights), benchmarks (49 objects: id/name/category/metric/num_problems/source_url), scores (1390 entries: model_id/benchmark_id/score/reference_url), generated (timestamp).",
    "extraction_decisions": "Built 83x49 matrix from scores list. 15 duplicate (model_id, benchmark_id) pairs found (DeepSeek distill models); resolved by averaging. Model IDs as row identifiers, benchmark IDs as columns. All scores numeric.",
    "n_models_raw": 83,
    "n_benchmarks_raw": 49
  },
  "data": {
    "n_models": 65,
    "n_benchmarks": 29,
    "missing_fraction": 0.6619,
    "preprocessing": "Filtered to benchmarks >= 15 obs, models >= 10 obs (iterative). For SVD: mean-imputed then z-scored. For canonical predictions: iterative SVD completion on raw + min-max normalized scales.",
    "benchmarks_used": [
      "AIME 2024",
      "AIME 2025",
      "ARC-AGI-1",
      "ARC-AGI-2",
      "Arena-Hard Auto",
      "BrowseComp",
      "BRUMO 2025",
      "Chatbot Arena Elo",
      "CMIMC 2025",
      "Codeforces Rating",
      "FrontierMath",
      "GPQA Diamond",
      "HLE (Humanity's Last Exam)",
      "HMMT Feb 2025",
      "HumanEval",
      "IFEval",
      "LiveCodeBench",
      "MATH-500",
      "MathArena Apex 2025",
      "MMLU",
      "MMLU-Pro",
      "MMMU",
      "OSWorld",
      "SimpleQA",
      "SMT 2025",
      "SWE-bench Pro",
      "SWE-bench Verified",
      "Terminal-Bench 2.0",
      "Terminal-Bench 1.0"
    ]
  },
  "rank_analysis": {
    "method": "SVD on z-scored filtered matrix (65x29, mean-imputed)",
    "effective_rank": 14,
    "variance_explained_by_rank": 0.5691,
    "singular_values": [
      24.4197,
      17.7346,
      12.0606,
      10.7112,
      9.6777,
      8.7541,
      8.6816,
      7.4393,
      7.0806,
      6.4925
    ],
    "justification": "90% variance threshold gives effective rank = 14. Top 3 components explain 56.9%. Spectrum is diffuse, likely inflated by mean-imputation artifacts on 42% missing data. True effective rank is probably 2-3 for the underlying signal."
  },
  "benchmark_selection": {
    "method": "greedy_forward_correlation_coverage_obs_filtered",
    "selected_benchmarks": [
      "AIME 2025",
      "SimpleQA",
      "MMLU-Pro",
      "LiveCodeBench",
      "SWE-bench Verified",
      "HumanEval",
      "Codeforces Rating"
    ],
    "n_selected": 7,
    "selection_criterion": "Greedy forward selection maximizing avg max|corr| coverage, restricted to benchmarks with >= 40 observations. Result: AIME 2025, SimpleQA, MMLU-Pro, LiveCodeBench, SWE-bench Verified, HumanEval, Codeforces Rating."
  },
  "prediction": {
    "method": "Ensemble: iterative SVD completion (raw rank 3 + min-max rank 5, averaged). Self-eval: Ridge from 7 selected benchmarks.",
    "overall_mae": 8.98,
    "per_benchmark_mae": {
      "AIME 2024": 8.32,
      "ARC-AGI-1": 21.73,
      "ARC-AGI-2": 13.68,
      "Arena-Hard Auto": 27.51,
      "BrowseComp": 14.07,
      "BRUMO 2025": 4.32,
      "Chatbot Arena Elo": 20.76,
      "CMIMC 2025": 3.97,
      "FrontierMath": 12.74,
      "GPQA Diamond": 6.18,
      "HLE (Humanity's Last Exam)": 6.81,
      "HMMT Feb 2025": 5.37,
      "IFEval": 3.17,
      "MATH-500": 1.77,
      "MathArena Apex 2025": 20.69,
      "MMLU": 1.46,
      "MMMU": 2.65,
      "OSWorld": 15.01,
      "SMT 2025": 4.34,
      "SWE-bench Pro": 4.49,
      "Terminal-Bench 2.0": 10.96,
      "Terminal-Bench 1.0": 12.91
    },
    "evaluation_protocol": "Self: LOO CV with Ridge. Canonical: iterative SVD blend on full 83x49 matrix with per-model held-out masking.",
    "n_predictor_benchmarks": 7,
    "achieves_mae_under_5": false,
    "canonical_overall_mae": 14.9,
    "canonical_per_benchmark_mae": {
      "AIME 2024": 9.32,
      "AIME 2025": 11.39,
      "ARC-AGI-1": 13.57,
      "ARC-AGI-2": 12.72,
      "Codeforces Rating": 8.61,
      "CritPt": 10.56,
      "FrontierMath": 18.32,
      "GPQA Diamond": 5.96,
      "HLE (Humanity's Last Exam)": 19.2,
      "HMMT Feb 2025": 28.37,
      "IFEval": 7.37,
      "LiveCodeBench": 10.53,
      "MATH-500": 4.63,
      "MMLU": 7.97,
      "MMMU": 12.7,
      "SimpleQA": 17.12,
      "SWE-bench Pro": 13.09,
      "Tau-Bench Retail": 0.17,
      "Terminal-Bench 2.0": 20.13,
      "HumanEval": 10.38,
      "MMLU-Pro": 8.02,
      "Terminal-Bench 1.0": 14.38,
      "BRUMO 2025": 10.23,
      "IMO 2025": 35.01,
      "MathArena Apex 2025": 9.94,
      "SMT 2025": 11.02,
      "SWE-bench Verified": 10.51,
      "USAMO 2025": 32.71,
      "BrowseComp": 38.55,
      "MathVision": 70.77,
      "AA Intelligence Index": 47.75,
      "Arena-Hard Auto": 15.3,
      "Chatbot Arena Elo": 19.82,
      "HMMT Nov 2025": 41.72,
      "LiveBench": 22.22,
      "MRCR v2": 38.51,
      "SimpleBench": 13.89,
      "Video-MMU": 8.71,
      "AA Long Context Reasoning": 66.23,
      "CMIMC 2025": 3.58,
      "MMMU-Pro": 65.74,
      "OSWorld": 9.31,
      "BigCodeBench": 25.41
    }
  },
  "methodology_notes": "Matrix is 83x49 with 66% missing data. Iterative SVD completion used as primary method: initialize missing with column means, iterate rank-k SVD replacing only missing entries until convergence. Canonical predictions blend rank-3 (raw) and rank-5 (min-max 0-100 normalized) completions, reducing scale-dependency bias. Z-scored mean-imputed filtered submatrix (65x29) used for spectrum analysis: first component 32%, top-3 57%, suggesting moderate low-rank structure inflated by imputation noise. Benchmark selection used correlation-based greedy forward on well-observed benchmarks (>=40 models), yielding a coding-heavy set reflecting dataset composition."
}