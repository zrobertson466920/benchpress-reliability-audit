{
  "data_discovery": {
    "raw_schema": "JSON with top-level keys: models (list of 83 model objects with id/name/provider/params/architecture/is_reasoning/open_weights), benchmarks (list of 49 benchmark objects with id/name/category/metric/num_problems/source_url), scores (list of 1390 score entries with model_id/benchmark_id/score/reference_url), generated (timestamp string).",
    "extraction_decisions": "Built model\u00d7benchmark matrix using sorted model_ids as rows and sorted benchmark_ids as columns. Duplicates (15 pairs, all with identical scores) resolved by simple average. No null scores found. Score values used as-is (no unit conversion).",
    "n_models_raw": 83,
    "n_benchmarks_raw": 49
  },
  "data": {
    "n_models": 83,
    "n_benchmarks": 35,
    "missing_fraction": 0.5556,
    "preprocessing": "Dropped 14 benchmarks with fewer than 10 observations (kept 35 of 49). Applied per-benchmark min-max normalization to [0,100] scale. Missing values imputed via iterative rank-4 SVD approximation (column-mean initialization, 1000 max iterations). All 83 models retained.",
    "benchmarks_used": [
      "AIME 2024",
      "AIME 2025",
      "ARC-AGI-1",
      "ARC-AGI-2",
      "Arena-Hard Auto",
      "BrowseComp",
      "BRUMO 2025",
      "Chatbot Arena Elo",
      "CMIMC 2025",
      "Codeforces Rating",
      "CritPt",
      "FrontierMath",
      "GPQA Diamond",
      "GSM8K",
      "HLE (Humanity's Last Exam)",
      "HMMT Feb 2025",
      "HMMT Nov 2025",
      "HumanEval",
      "IFEval",
      "LiveBench",
      "LiveCodeBench",
      "MATH-500",
      "MathArena Apex 2025",
      "MMLU",
      "MMLU-Pro",
      "MMMU",
      "MMMU-Pro",
      "OSWorld",
      "SimpleQA",
      "SMT 2025",
      "SWE-bench Pro",
      "SWE-bench Verified",
      "Tau-Bench Retail",
      "Terminal-Bench 2.0",
      "Terminal-Bench 1.0"
    ]
  },
  "rank_analysis": {
    "method": "SVD on iteratively imputed min-max normalized matrix (rank-4 imputation)",
    "effective_rank": 3,
    "variance_explained_by_rank": 0.9397,
    "singular_values": [
      3921.44,
      1842.11,
      1485.25,
      1114.61,
      120.55,
      106.11,
      103.32,
      90.25,
      84.98,
      79.88
    ],
    "justification": "The first singular value explains 71.9% of variance, the first 3 explain 91.5%, and the first 5 explain 99.5%. There is a dramatic drop in singular value magnitude after rank 5 (ratio 5.96 between SV5 and SV6). Effective rank is 3 at the 90% variance threshold. The matrix is strongly low-rank, consistent with a small number of latent capability dimensions driving performance across benchmarks."
  },
  "benchmark_selection": {
    "method": "greedy_forward_selection",
    "selected_benchmarks": [
      "HMMT Feb 2025",
      "AIME 2024",
      "Terminal-Bench 1.0",
      "CMIMC 2025",
      "GSM8K"
    ],
    "n_selected": 5,
    "selection_criterion": "Minimize 5-fold CV MAE using ridge regression from selected subset to all remaining benchmarks on the imputed normalized matrix, evaluated on observed entries only."
  },
  "prediction": {
    "method": "ridge_regression_from_all_other_benchmarks",
    "overall_mae": 12.151,
    "per_benchmark_mae": {
      "AIME 2024": 9.981,
      "AIME 2025": 11.36,
      "ARC-AGI-1": 23.849,
      "ARC-AGI-2": 11.841,
      "Arena-Hard Auto": 8.471,
      "BrowseComp": 8.623,
      "BRUMO 2025": 8.634,
      "Chatbot Arena Elo": 14.211,
      "CMIMC 2025": 10.507,
      "Codeforces Rating": 12.902,
      "CritPt": 8.849,
      "FrontierMath": 16.675,
      "GPQA Diamond": 14.1,
      "GSM8K": 4.357,
      "HLE (Humanity's Last Exam)": 22.16,
      "HMMT Feb 2025": 10.923,
      "HMMT Nov 2025": 7.683,
      "HumanEval": 8.514,
      "IFEval": 12.601,
      "LiveBench": 4.744,
      "LiveCodeBench": 12.716,
      "MATH-500": 10.008,
      "MathArena Apex 2025": 11.274,
      "MMLU": 10.99,
      "MMLU-Pro": 9.235,
      "MMMU": 11.836,
      "MMMU-Pro": 30.637,
      "OSWorld": 13.351,
      "SimpleQA": 12.219,
      "SMT 2025": 17.272,
      "SWE-bench Pro": 11.503,
      "SWE-bench Verified": 14.433,
      "Tau-Bench Retail": 9.723,
      "Terminal-Bench 2.0": 9.614,
      "Terminal-Bench 1.0": 10.992
    },
    "evaluation_protocol": "5-fold CV on min-max normalized imputed matrix. For each target benchmark, train ridge (alpha=1.0) from all other 34 benchmarks. Score on observed entries only.",
    "n_predictor_benchmarks": 34,
    "achieves_mae_under_5": false
  },
  "canonical_evaluation": {
    "overall_mae": 14.841,
    "coverage": 1.0,
    "per_benchmark_mae": {
      "AIME 2024": 7.586,
      "AIME 2025": 11.4,
      "ARC-AGI-1": 21.427,
      "ARC-AGI-2": 13.74,
      "Codeforces Rating": 14.989,
      "CritPt": 2.924,
      "FrontierMath": 22.712,
      "GPQA Diamond": 7.516,
      "HLE (Humanity's Last Exam)": 15.81,
      "HMMT Feb 2025": 20.481,
      "IFEval": 5.997,
      "LiveCodeBench": 9.727,
      "MATH-500": 5.016,
      "MMLU": 5.418,
      "MMMU": 22.075,
      "SimpleQA": 15.595,
      "SWE-bench Pro": 11.053,
      "Tau-Bench Retail": 2.533,
      "Terminal-Bench 2.0": 12.911,
      "HumanEval": 12.006,
      "MMLU-Pro": 8.02,
      "Terminal-Bench 1.0": 36.321,
      "BRUMO 2025": 4.938,
      "IMO 2025": 18.204,
      "MathArena Apex 2025": 16.507,
      "SMT 2025": 7.282,
      "SWE-bench Verified": 17.43,
      "USAMO 2025": 30.077,
      "BrowseComp": 19.759,
      "MathVision": 26.951,
      "AA Intelligence Index": 39.205,
      "Arena-Hard Auto": 30.147,
      "Chatbot Arena Elo": 17.075,
      "HMMT Nov 2025": 15.558,
      "LiveBench": 34.714,
      "MRCR v2": 12.774,
      "SimpleBench": 23.556,
      "Video-MMU": 1.98,
      "AA Long Context Reasoning": 48.112,
      "CMIMC 2025": 3.681,
      "MMMU-Pro": 53.966,
      "OSWorld": 10.947,
      "BigCodeBench": 26.014
    },
    "method": "For each eval model, ridge regression (alpha=100) from the 5 revealed benchmark scores to each held-out target, trained on all other models' imputed values.",
    "n_eval_models": 12,
    "n_heldout_pairs": 196
  },
  "methodology_notes": "Pipeline: (1) Extract 83\u00d749 matrix from JSON, average duplicate scores. (2) Filter to 35 benchmarks with \u226510 observations. (3) Min-max normalize each benchmark to [0,100]. (4) Impute missing values via iterative rank-4 SVD (column-mean init). (5) SVD on imputed matrix reveals effective rank 3 (91.5% variance). (6) Greedy forward benchmark selection minimizing 5-fold CV MAE. (7) Self-evaluation: ridge regression all-others\u2192target, 5-fold CV, MAE=12.15 on normalized scale. (8) Canonical evaluation: ridge from 5 revealed benchmarks per eval model, MAE=14.84 on normalized 0-100 scale. Key degrees of freedom: filtered sparse benchmarks (<10 obs), used min-max normalization (not z-score or logit), iterative SVD imputation at rank 4, ridge regression for prediction. The high missingness (55.6% after filtering) is the main challenge; imputed values for sparse benchmarks are uncertain. Canonical evaluation is harder than self-eval because only 5 features are available per model rather than 34."
}