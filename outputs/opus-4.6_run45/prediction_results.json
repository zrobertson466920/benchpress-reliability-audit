{
  "method": "RidgeCV from all other benchmarks (mean-imputed)",
  "overall_mae": 19.5021,
  "per_benchmark_mae": {
    "AIME 2024": 9.8413,
    "AIME 2025": 10.6503,
    "ARC-AGI-1": 16.0508,
    "ARC-AGI-2": 9.3717,
    "Arena-Hard Auto": 16.3534,
    "BrowseComp": 8.4026,
    "BRUMO 2025": 4.7285,
    "Chatbot Arena Elo": 30.2269,
    "CMIMC 2025": 8.2975,
    "Codeforces Rating": 285.6976,
    "CritPt": 2.2855,
    "FrontierMath": 4.7275,
    "GPQA Diamond": 7.4201,
    "GSM8K": 4.8515,
    "HLE (Humanity's Last Exam)": 7.3042,
    "HMMT Feb 2025": 11.6256,
    "HMMT Nov 2025": 5.439,
    "HumanEval": 5.3246,
    "IFEval": 5.7537,
    "LiveBench": 6.1223,
    "LiveCodeBench": 8.3677,
    "MATH-500": 5.0666,
    "MathArena Apex 2025": 3.9983,
    "MMLU": 3.4266,
    "MMLU-Pro": 5.2586,
    "MMMU": 3.3024,
    "MMMU-Pro": 6.0236,
    "OSWorld": 8.7993,
    "SimpleQA": 13.6278,
    "SMT 2025": 4.4812,
    "SWE-bench Pro": 7.9629,
    "SWE-bench Verified": 10.8222,
    "Tau-Bench Retail": 4.9306,
    "Terminal-Bench 2.0": 11.306,
    "Terminal-Bench 1.0": 6.0891
  },
  "evaluation_protocol": "leave-one-model-out cross-validation",
  "n_predictor_benchmarks": 34,
  "achieves_mae_under_5": false
}