{
  "data_discovery": {
    "raw_schema": "JSON with top-level keys: models (list of 83 model objects with id, name, provider, etc.), benchmarks (list of 49 benchmark objects with id, name, category, metric), scores (list of 1390 score entries with model_id, benchmark_id, score, reference_url), generated (timestamp string).",
    "extraction_decisions": "Built model x benchmark matrix by iterating over scores. 15 duplicate (model_id, benchmark_id) pairs found and resolved by simple averaging. All 83 models and 49 benchmarks included in raw matrix. Model IDs and benchmark IDs used as row/column indices; names used for display in CSVs.",
    "n_models_raw": 83,
    "n_benchmarks_raw": 49
  },
  "data": {
    "n_models": 80,
    "n_benchmarks": 35,
    "missing_fraction": 0.5425,
    "preprocessing": "Filtered to benchmarks with >=10 observations and models with >=5 observations (iterative until stable). Missing values imputed with per-benchmark column means. Z-score normalization applied for SVD/rank analysis only; raw values used for prediction and evaluation.",
    "benchmarks_used": [
      "AIME 2024",
      "AIME 2025",
      "ARC-AGI-1",
      "ARC-AGI-2",
      "Arena-Hard Auto",
      "BrowseComp",
      "BRUMO 2025",
      "Chatbot Arena Elo",
      "CMIMC 2025",
      "Codeforces Rating",
      "CritPt",
      "FrontierMath",
      "GPQA Diamond",
      "GSM8K",
      "HLE (Humanity's Last Exam)",
      "HMMT Feb 2025",
      "HMMT Nov 2025",
      "HumanEval",
      "IFEval",
      "LiveBench",
      "LiveCodeBench",
      "MATH-500",
      "MathArena Apex 2025",
      "MMLU",
      "MMLU-Pro",
      "MMMU",
      "MMMU-Pro",
      "OSWorld",
      "SimpleQA",
      "SMT 2025",
      "SWE-bench Pro",
      "SWE-bench Verified",
      "Tau-Bench Retail",
      "Terminal-Bench 2.0",
      "Terminal-Bench 1.0"
    ]
  },
  "rank_analysis": {
    "method": "SVD on z-score normalized, column-mean-imputed cleaned matrix",
    "effective_rank": 2,
    "variance_explained_by_rank": 0.4295,
    "singular_values": [
      27.0286,
      21.3762,
      14.0521,
      13.0386,
      12.0815,
      10.2863,
      9.8899,
      9.5389,
      9.3104,
      8.1579,
      7.9198,
      7.6869,
      7.2314,
      6.9229,
      6.861
    ],
    "justification": "Effective rank determined by spectral gap criterion (largest ratio between consecutive singular values). SV[2]/SV[3] = 1.52 is the largest gap in the first 10 SVs. The top 2 components explain 42.9% of variance. Predictive evaluation on the canonical task found rank 5 optimal for SVD completion, suggesting 2-5 effective dimensions depending on the criterion used. The first component alone explains 26.4%, indicating a strong general ability factor. Note: SVD was performed on a mean-imputed z-scored matrix (54% missing in cleaned submatrix), so imputation noise may inflate apparent dimensionality."
  },
  "benchmark_selection": {
    "method": "greedy_forward_ridge",
    "selected_benchmarks": [
      "MMLU-Pro",
      "CritPt",
      "SMT 2025",
      "ARC-AGI-1",
      "OSWorld"
    ],
    "n_selected": 5,
    "selection_criterion": "Greedy forward selection minimizing training MAE of Ridge regression from selected benchmarks to all benchmarks on the cleaned z-normalized matrix."
  },
  "prediction": {
    "method": "RidgeCV regression from all other benchmarks (mean-imputed) for own eval; ensemble of iterative SVD (rank=5) + Ridge(alpha=10) for canonical eval",
    "overall_mae": 19.5021,
    "per_benchmark_mae": {
      "AIME 2024": 9.8413,
      "AIME 2025": 10.6503,
      "ARC-AGI-1": 16.0508,
      "ARC-AGI-2": 9.3717,
      "Arena-Hard Auto": 16.3534,
      "BrowseComp": 8.4026,
      "BRUMO 2025": 4.7285,
      "Chatbot Arena Elo": 30.2269,
      "CMIMC 2025": 8.2975,
      "Codeforces Rating": 285.6976,
      "CritPt": 2.2855,
      "FrontierMath": 4.7275,
      "GPQA Diamond": 7.4201,
      "GSM8K": 4.8515,
      "HLE (Humanity's Last Exam)": 7.3042,
      "HMMT Feb 2025": 11.6256,
      "HMMT Nov 2025": 5.439,
      "HumanEval": 5.3246,
      "IFEval": 5.7537,
      "LiveBench": 6.1223,
      "LiveCodeBench": 8.3677,
      "MATH-500": 5.0666,
      "MathArena Apex 2025": 3.9983,
      "MMLU": 3.4266,
      "MMLU-Pro": 5.2586,
      "MMMU": 3.3024,
      "MMMU-Pro": 6.0236,
      "OSWorld": 8.7993,
      "SimpleQA": 13.6278,
      "SMT 2025": 4.4812,
      "SWE-bench Pro": 7.9629,
      "SWE-bench Verified": 10.8222,
      "Tau-Bench Retail": 4.9306,
      "Terminal-Bench 2.0": 11.306,
      "Terminal-Bench 1.0": 6.0891
    },
    "evaluation_protocol": "leave-one-model-out cross-validation over cleaned matrix",
    "n_predictor_benchmarks": 34,
    "achieves_mae_under_5": false
  },
  "canonical_evaluation": {
    "canonical_overall_mae": 15.2312,
    "canonical_coverage": 1.0,
    "n_predictions": 196,
    "n_scored": 196,
    "method": "Ensemble: iterative SVD (rank=5) weight=0.8 + Ridge(alpha=10) weight=0.2, in 0-100 normalized space"
  },
  "methodology_notes": "Pipeline: (1) Extracted 83x49 matrix from JSON, averaged 15 duplicates. (2) Filtered to models>=5 obs, benchmarks>=10 obs for cleaned 80x35 submatrix. (3) Column-mean imputation + z-score for SVD rank analysis. (4) Greedy forward selection of 5 benchmarks via ridge. (5) RidgeCV LOMO for own eval. (6) Canonical: ensemble of iterative SVD completion (rank=5, 0-100 normalized) and per-benchmark Ridge regression (alpha=10), weight 80% SVD / 20% Ridge. Scale differences handled via per-benchmark 0-100 normalization."
}