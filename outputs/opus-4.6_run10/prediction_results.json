{
  "method": "ridge_regression_loo_model",
  "overall_mae": 15.37434447378916,
  "per_benchmark_mae": {
    "GPQA Diamond": 9.36177460094105,
    "MMLU": 9.026121697886227,
    "MMLU-Pro": 9.016071205036706,
    "SWE-bench Verified": 24.023499131012528,
    "ARC-AGI-2": 25.464913925966425,
    "SimpleQA": 38.243519361228124,
    "IFEval": 10.061061323141443,
    "HumanEval": 13.056166171001012,
    "Arena-Hard Auto": 35.39005987638379,
    "AIME 2024": 11.412989702437018,
    "ARC-AGI-1": 17.975174520847645,
    "AIME 2025": 11.744634646753664,
    "MATH-500": 8.330411721063227,
    "LiveCodeBench": 9.763283520056001,
    "HLE (Humanity's Last Exam)": 22.565985758402434,
    "Codeforces Rating": 13.081446452147276,
    "MMMU": 23.43766968297115,
    "HMMT Feb 2025": 16.68428521184683,
    "Terminal-Bench 1.0": 29.698077728367785,
    "Tau-Bench Retail": 29.227769255947834,
    "Terminal-Bench 2.0": 17.180751857902557,
    "OSWorld": 47.895158823734135,
    "SWE-bench Pro": 25.846578135527523,
    "LiveBench": 32.04297937673908,
    "FrontierMath": 21.04368642848976,
    "BrowseComp": 33.360399778861606,
    "MMMU-Pro": 24.962205820511983,
    "Chatbot Arena Elo": 24.326705366791998,
    "CritPt": 22.044846410555426,
    "GSM8K": 10.888083447000186,
    "BRUMO 2025": 20.160699305621264,
    "SMT 2025": 20.079236419929234,
    "CMIMC 2025": 31.71587626851858,
    "MathArena Apex 2025": 36.31669888514871,
    "HMMT Nov 2025": 9.45550615001836
  },
  "evaluation_protocol": "leave-one-model-out, predict each held-out benchmark from remaining observed benchmarks via ridge",
  "n_predictor_benchmarks": 35,
  "achieves_mae_under_5": false,
  "n_eval_predictions": 596
}