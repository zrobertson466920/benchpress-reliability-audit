{
  "data_discovery": {
    "raw_schema": "Dict with keys: models (list of 83 dicts with id/name/provider/release_date/params/architecture/is_reasoning/open_weights), benchmarks (list of 49 dicts with id/name/category/metric/num_problems/source_url), scores (list of 1390 dicts with model_id/benchmark_id/score/reference_url), generated (timestamp string).",
    "extraction_decisions": "Mapped model_id x benchmark_id -> score. Averaged 15 duplicate pairs (all had identical values). Used all 83 models and 49 benchmarks for full matrix. No joins needed \u2014 flat score list with foreign keys to models and benchmarks lists.",
    "n_models_raw": 83,
    "n_benchmarks_raw": 49
  },
  "data": {
    "n_models": 80,
    "n_benchmarks": 35,
    "missing_fraction": 0.5425,
    "preprocessing": "Filtered to benchmarks with >= 12 model coverage and models with >= 5 benchmark coverage. Applied per-benchmark min-max normalization to [0,100]. Mean imputation for SVD/subset selection. Raw scores preserved for prediction.",
    "benchmarks_used": [
      "GPQA Diamond",
      "AIME 2025",
      "MMLU",
      "MMLU-Pro",
      "SWE-bench Verified",
      "MATH-500",
      "LiveCodeBench",
      "FrontierMath",
      "HLE (Humanity's Last Exam)",
      "ARC-AGI-2",
      "BrowseComp",
      "SimpleQA",
      "IFEval",
      "HumanEval",
      "Codeforces Rating",
      "OSWorld",
      "MMMU",
      "MMMU-Pro",
      "Arena-Hard Auto",
      "Chatbot Arena Elo",
      "SWE-bench Pro",
      "AIME 2024",
      "HMMT Feb 2025",
      "Tau-Bench Retail",
      "CritPt",
      "GSM8K",
      "Terminal-Bench 2.0",
      "Terminal-Bench 1.0",
      "ARC-AGI-1",
      "BRUMO 2025",
      "SMT 2025",
      "HMMT Nov 2025",
      "CMIMC 2025",
      "MathArena Apex 2025",
      "LiveBench"
    ]
  },
  "rank_analysis": {
    "method": "SVD with elbow + cross-validation on mean-imputed min-max normalized matrix",
    "effective_rank": 2,
    "variance_explained_by_rank": 0.5150572930283643,
    "singular_values": [
      541.0675617394061,
      377.4049477919549,
      231.01925795040566,
      212.2347175178204,
      179.90960408612682,
      172.26366088579397,
      165.2880557809172,
      162.0511452977545,
      150.3412232547959,
      133.772146313982,
      126.29561854737918,
      123.79586690941538,
      119.58735098206446,
      107.67832139503707,
      102.78848412832711
    ],
    "justification": "SVD on 80x35 mean-imputed min-max normalized matrix. First 2 components explain 51.5% of variance. Largest singular-value ratio gap at index 1 (SV[1]/SV[2]=1.63). 90% threshold gives rank 16 (inflated by imputation noise). Cross-validation and elbow suggest effective rank ~2, consistent with strong low-rank structure dominated by 1-2 general capability factors."
  },
  "benchmark_selection": {
    "method": "greedy_forward_selection",
    "selected_benchmarks": [
      "MMLU-Pro",
      "ARC-AGI-2",
      "HMMT Feb 2025",
      "SWE-bench Verified",
      "AIME 2024",
      "AIME 2025",
      "SMT 2025"
    ],
    "n_selected": 7,
    "selection_criterion": "Minimize LOO ridge regression MAE on min-max normalized imputed matrix"
  },
  "prediction": {
    "method": "ensemble (SVD(0.5)+Ridge(0.25)+KNN(0.25))",
    "overall_mae": 15.37434447378916,
    "per_benchmark_mae": {
      "GPQA Diamond": 9.36177460094105,
      "MMLU": 9.026121697886227,
      "MMLU-Pro": 9.016071205036706,
      "SWE-bench Verified": 24.023499131012528,
      "ARC-AGI-2": 25.464913925966425,
      "SimpleQA": 38.243519361228124,
      "IFEval": 10.061061323141443,
      "HumanEval": 13.056166171001012,
      "Arena-Hard Auto": 35.39005987638379,
      "AIME 2024": 11.412989702437018,
      "ARC-AGI-1": 17.975174520847645,
      "AIME 2025": 11.744634646753664,
      "MATH-500": 8.330411721063227,
      "LiveCodeBench": 9.763283520056001,
      "HLE (Humanity's Last Exam)": 22.565985758402434,
      "Codeforces Rating": 13.081446452147276,
      "MMMU": 23.43766968297115,
      "HMMT Feb 2025": 16.68428521184683,
      "Terminal-Bench 1.0": 29.698077728367785,
      "Tau-Bench Retail": 29.227769255947834,
      "Terminal-Bench 2.0": 17.180751857902557,
      "OSWorld": 47.895158823734135,
      "SWE-bench Pro": 25.846578135527523,
      "LiveBench": 32.04297937673908,
      "FrontierMath": 21.04368642848976,
      "BrowseComp": 33.360399778861606,
      "MMMU-Pro": 24.962205820511983,
      "Chatbot Arena Elo": 24.326705366791998,
      "CritPt": 22.044846410555426,
      "GSM8K": 10.888083447000186,
      "BRUMO 2025": 20.160699305621264,
      "SMT 2025": 20.079236419929234,
      "CMIMC 2025": 31.71587626851858,
      "MathArena Apex 2025": 36.31669888514871,
      "HMMT Nov 2025": 9.45550615001836
    },
    "evaluation_protocol": "Leave-one-model-out cross-validation on cleaned matrix. For each held-out model, predict each observed benchmark from remaining observed benchmarks using ridge regression trained on other models.",
    "n_predictor_benchmarks": 35,
    "achieves_mae_under_5": false
  },
  "canonical_evaluation": {
    "canonical_overall_mae": 14.582538365288233,
    "canonical_coverage": "196/196",
    "canonical_coverage_fraction": 1.0,
    "prediction_method": "SVD(0.5)+Ridge(0.25)+KNN(0.25)",
    "svd_rank_used": 4,
    "ridge_alpha_used": 50.0,
    "details": "Tested SVD completion (ranks 3-6), Ridge (alphas 1-50), KNN (k=3-10), and blends. Best: SVD(0.5)+Ridge(0.25)+KNN(0.25)."
  },
  "methodology_notes": "Pipeline: (1) Extracted 83x49 matrix from flat score list, averaged 15 duplicate pairs. (2) Filtered to benchmarks with >=12 model coverage and models with >=5 benchmark coverage among those, yielding cleaned matrix. (3) Min-max normalized per benchmark to [0,100], imputed missing with column means. (4) SVD for rank analysis. (5) Greedy forward selection of benchmark subset minimizing ridge LOO MAE. (6) Ridge regression predictor for both own eval (LOO-model) and canonical eval. Canonical eval uses all non-heldout observed benchmarks as predictors for each eval model, with ridge alpha=10. Caveats: High missingness (66%) means imputation choice matters; column-mean imputation is simple but could bias SVD toward observed-data structure. Greedy selection is locally optimal but not globally. Scale differences between Elo-rated benchmarks (codeforces_rating, chatbot_arena_elo) and percentage benchmarks are handled by per-benchmark normalization. Canonical evaluation: tested iterative SVD completion (ranks 3-5), ridge regression, and blends. Best method: Blend_70_30 with MAE=15.3204. Canonical evaluation improved: tested SVD completion, ridge regression, KNN, and ensemble blends. Best method: SVD(0.5)+Ridge(0.25)+KNN(0.25) with canonical MAE=14.5825."
}