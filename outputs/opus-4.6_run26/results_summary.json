{
  "data_discovery": {
    "raw_schema": "JSON with keys: models (list of 83 dicts with id/name/provider/release_date/params_total_M/params_active_M/architecture/is_reasoning/open_weights), benchmarks (list of 49 dicts with id/name/category/metric/num_problems/source_url), scores (list of 1390 dicts with model_id/benchmark_id/score/reference_url), generated (timestamp string).",
    "extraction_decisions": "Mapped scores to (model_id, benchmark_id) -> score. Averaged 15 duplicate pairs (all identical). Used all 83 models and 49 benchmarks for the full matrix.",
    "n_models_raw": 83,
    "n_benchmarks_raw": 49
  },
  "data": {
    "n_models": 65,
    "n_benchmarks": 35,
    "missing_fraction": 0.48527472527472526,
    "preprocessing": "Filtered to benchmarks with >=12 models and models with >=10 benchmarks (iterative until stable). Min-max normalized to 0-100 per benchmark. Column-mean imputed remaining NaNs. Full 83x49 matrix (66.2% missing) retained for canonical evaluation with iterative SVD soft-impute.",
    "benchmarks_used": [
      "GPQA Diamond",
      "AIME 2025",
      "MMLU",
      "MMLU-Pro",
      "SWE-bench Verified",
      "MATH-500",
      "LiveCodeBench",
      "FrontierMath",
      "HLE (Humanity's Last Exam)",
      "ARC-AGI-2",
      "BrowseComp",
      "SimpleQA",
      "IFEval",
      "HumanEval",
      "Codeforces Rating",
      "OSWorld",
      "MMMU",
      "MMMU-Pro",
      "Arena-Hard Auto",
      "Chatbot Arena Elo",
      "SWE-bench Pro",
      "AIME 2024",
      "HMMT Feb 2025",
      "Tau-Bench Retail",
      "CritPt",
      "GSM8K",
      "Terminal-Bench 2.0",
      "Terminal-Bench 1.0",
      "ARC-AGI-1",
      "BRUMO 2025",
      "SMT 2025",
      "HMMT Nov 2025",
      "CMIMC 2025",
      "MathArena Apex 2025",
      "LiveBench"
    ]
  },
  "rank_analysis": {
    "method": "SVD on centered, min-max normalized, mean-imputed submatrix (65x35)",
    "effective_rank": 15,
    "variance_explained_by_rank": 0.9043563154665786,
    "singular_values": [
      508.47624251164325,
      350.74366937264193,
      225.43896349540904,
      203.2540064791548,
      180.84062218886564,
      171.60639532800334,
      157.40160040829264,
      149.66719794760687,
      135.53269129416518,
      128.84829116129805,
      122.61909348821946,
      120.38360619669344,
      106.27582912008023,
      101.14789081732012,
      91.95365043428588,
      89.22199615366914,
      88.02704213347207,
      82.6567733003378,
      79.43573633499908,
      75.59108373452231,
      73.90987526290385,
      72.55756932284703,
      64.72873573414924,
      58.02285610450191,
      56.916944627999236,
      51.44489652647008,
      49.84486218084499,
      45.67557641537151,
      41.95386285197325,
      40.489787256170686,
      36.70280239350497,
      35.2195794963975,
      34.20426176735957,
      26.735518197701573,
      21.792282014606425
    ],
    "justification": "90% cumulative variance threshold yields effective rank 15. First component explains 34.5% of variance, first two explain 50.9%. The spectrum decays gradually rather than sharply, reflecting both genuine structure and noise from 48.5% missing data imputed with column means. A rank of 2-3 captures the dominant factors (general capability + reasoning specialization) while higher components capture benchmark-specific variance."
  },
  "benchmark_selection": {
    "method": "greedy_forward_selection",
    "selected_benchmarks": [
      "AIME 2024",
      "ARC-AGI-2",
      "MMLU",
      "HMMT Feb 2025",
      "SWE-bench Verified"
    ],
    "n_selected": 5,
    "selection_criterion": "Minimize leave-one-model-out MAE using Ridge regression (alpha=1.0) from selected to remaining benchmarks on the cleaned normalized 65x35 matrix."
  },
  "prediction": {
    "method": "ridge_regression",
    "overall_mae": 9.055085142672697,
    "per_benchmark_mae": {
      "GPQA Diamond": 7.825122362244851,
      "AIME 2025": 12.084290770588186,
      "MMLU-Pro": 6.860793505763978,
      "MATH-500": 12.597772439744737,
      "LiveCodeBench": 9.699662838347011,
      "FrontierMath": 10.711493758949034,
      "HLE (Humanity's Last Exam)": 9.961602476798554,
      "BrowseComp": 8.541248479308344,
      "SimpleQA": 14.096054263886971,
      "IFEval": 7.627609094282048,
      "HumanEval": 8.487665409601794,
      "Codeforces Rating": 11.238837677432825,
      "OSWorld": 9.582736009845165,
      "MMMU": 13.63607836237933,
      "MMMU-Pro": 5.72269829474343,
      "Arena-Hard Auto": 11.475675036082452,
      "Chatbot Arena Elo": 10.531794431929114,
      "SWE-bench Pro": 8.996745553476902,
      "Tau-Bench Retail": 5.133337571347584,
      "CritPt": 7.286694910255489,
      "GSM8K": 6.366686315048525,
      "Terminal-Bench 2.0": 9.627987730437518,
      "Terminal-Bench 1.0": 10.219321693389626,
      "ARC-AGI-1": 10.965592356759558,
      "BRUMO 2025": 6.821332488704577,
      "SMT 2025": 7.26476046042991,
      "HMMT Nov 2025": 4.1478387222620094,
      "CMIMC 2025": 8.248013747819291,
      "MathArena Apex 2025": 8.041747858660104,
      "LiveBench": 7.851359659662013
    },
    "evaluation_protocol": "leave_one_model_out on cleaned normalized matrix (65x35)",
    "n_predictor_benchmarks": 5,
    "achieves_mae_under_5": false
  },
  "canonical_evaluation": {
    "method": "ensemble_soft_impute_clamped",
    "ranks_used": [
      3,
      5,
      8,
      12
    ],
    "description": "Ensemble of iterative SVD soft-impute at ranks 3, 5, 8, 12 on the full 83x49 normalized matrix. Predictions averaged across ranks and clamped to [0, 100].",
    "n_predictions": 196,
    "n_pairs": 196,
    "coverage": 1.0,
    "self_computed_canonical_mae": 15.135896433061296
  },
  "methodology_notes": "Pipeline: (1) Extract 83x49 raw matrix from JSON, averaging 15 duplicate entries (all identical). (2) For rank analysis and benchmark selection, filter to a denser 65x35 submatrix (benchmarks with >=12 models, models with >=10 benchmarks), min-max normalize 0-100, mean-impute missing. (3) SVD yields effective rank 15 at 90% variance; first 2 components capture ~51%. (4) Greedy forward selection of 5 benchmarks minimizing LOO Ridge MAE on the submatrix. (5) For canonical evaluation, use the full 83x49 normalized matrix with iterative SVD soft-impute (ensemble of ranks 3/5/8/12, clamped to [0,100]). Key challenges: 66% overall missingness, mixed metrics (Elo ratings vs percentages), and many sparse benchmarks with <10 observations. The canonical MAE (~15) is substantially higher than own-evaluation MAE (~9) because the canonical task tests on the full sparse matrix including benchmarks with very few observations, where matrix completion has little signal."
}