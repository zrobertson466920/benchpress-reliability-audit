{
  "data_discovery": {
    "raw_schema": "JSON with top-level keys: models (list of 83 dicts with id/name/provider/params/architecture/is_reasoning/open_weights), benchmarks (list of 49 dicts with id/name/category/metric/num_problems/source_url), scores (list of 1390 dicts with model_id/benchmark_id/score/reference_url), generated (timestamp string).",
    "extraction_decisions": "Scores extracted as model_id x benchmark_id pairs. 15 duplicate (model, benchmark) pairs found and resolved by averaging. No null scores present. Model and benchmark IDs sorted lexicographically for matrix construction.",
    "n_models_raw": 83,
    "n_benchmarks_raw": 49
  },
  "data": {
    "n_models": 80,
    "n_benchmarks": 35,
    "missing_fraction": 0.5425,
    "preprocessing": "Filtered benchmarks to those with >= 12 model scores (kept 35/49), then filtered models to those with >= 5 benchmarks in filtered set (kept 80/83). For SVD/prediction: min-max normalized each benchmark to [0,100], imputed remaining missing with column mean. Raw scores preserved in cleaned_matrix.csv.",
    "benchmarks_used": [
      "AIME 2024",
      "AIME 2025",
      "ARC-AGI-1",
      "ARC-AGI-2",
      "Arena-Hard Auto",
      "BrowseComp",
      "BRUMO 2025",
      "Chatbot Arena Elo",
      "CMIMC 2025",
      "Codeforces Rating",
      "CritPt",
      "FrontierMath",
      "GPQA Diamond",
      "GSM8K",
      "HLE (Humanity's Last Exam)",
      "HMMT Feb 2025",
      "HMMT Nov 2025",
      "HumanEval",
      "IFEval",
      "LiveBench",
      "LiveCodeBench",
      "MATH-500",
      "MathArena Apex 2025",
      "MMLU",
      "MMLU-Pro",
      "MMMU",
      "MMMU-Pro",
      "OSWorld",
      "SimpleQA",
      "SMT 2025",
      "SWE-bench Pro",
      "SWE-bench Verified",
      "Tau-Bench Retail",
      "Terminal-Bench 2.0",
      "Terminal-Bench 1.0"
    ]
  },
  "rank_analysis": {
    "method": "SVD on min-max normalized (0-100 per benchmark), column-mean imputed matrix",
    "effective_rank": 1,
    "variance_explained_by_rank": 0.9452,
    "singular_values": [
      3339.24,
      391.39,
      320.29,
      217.15,
      212.09,
      175.35,
      169.39,
      162.81,
      157.33,
      148.78
    ],
    "justification": "Rank-1 captures 94.5% of variance (strongly dominant first singular value). Effective rank = 1 at 90% threshold, 2 at 95%. The matrix is strongly low-rank: a single latent factor (general model capability) explains the vast majority of benchmark performance variation."
  },
  "benchmark_selection": {
    "method": "greedy forward selection with leave-one-model-out ridge regression",
    "selected_benchmarks": [
      "LiveCodeBench",
      "ARC-AGI-2",
      "HMMT Feb 2025",
      "Arena-Hard Auto",
      "SWE-bench Verified"
    ],
    "n_selected": 5,
    "selection_criterion": "minimize LOMO MAE on normalized 0-100 scale; stop when marginal improvement < 0.3"
  },
  "prediction": {
    "method": "iterative SVD matrix completion (soft-impute style, rank-3)",
    "overall_mae": 13.86,
    "per_benchmark_mae": {
      "MMLU": 8.069,
      "AIME 2024": 8.863,
      "AIME 2025": 8.187,
      "HLE (Humanity's Last Exam)": 16.278,
      "Codeforces Rating": 12.277,
      "ARC-AGI-1": 13.193,
      "HumanEval": 9.12,
      "GPQA Diamond": 10.966,
      "SWE-bench Verified": 13.265,
      "MATH-500": 14.159,
      "Terminal-Bench 1.0": 21.928,
      "SWE-bench Pro": 20.183,
      "SimpleQA": 19.122,
      "CMIMC 2025": 22.455,
      "Arena-Hard Auto": 22.901,
      "MMMU": 17.374,
      "LiveBench": 12.992,
      "IFEval": 10.295,
      "MMLU-Pro": 8.874,
      "LiveCodeBench": 8.659,
      "GSM8K": 16.105,
      "MMMU-Pro": 41.739,
      "ARC-AGI-2": 12.479,
      "Tau-Bench Retail": 16.221,
      "MathArena Apex 2025": 29.465,
      "HMMT Feb 2025": 20.655,
      "Terminal-Bench 2.0": 8.623,
      "OSWorld": 19.957,
      "SMT 2025": 22.39,
      "BrowseComp": 24.303,
      "BRUMO 2025": 5.333,
      "FrontierMath": 20.501,
      "CritPt": 14.633,
      "Chatbot Arena Elo": 26.092,
      "HMMT Nov 2025": 8.867
    },
    "evaluation_protocol": "random 80/20 split of observed entries (seed=42) on normalized 0-100 scale",
    "n_predictor_benchmarks": 35,
    "achieves_mae_under_5": false
  },
  "methodology_notes": "The performance matrix is strongly dominated by a single factor (~94.5% variance explained by rank 1), indicating that LLM benchmark scores are largely captured by a single 'general capability' dimension. Remaining variance likely reflects benchmark-specific skills (coding, math reasoning, instruction following). Iterative SVD completion with rank 3 was chosen to capture both the dominant factor and the most important secondary dimensions. The high missingness (~54% in filtered matrix, ~66% raw) is the main challenge; column-mean imputation introduces bias toward the center. For canonical evaluation, the full (unfiltered) matrix is used with per-model masking as specified. Mixed metrics (Elo ratings vs percentages) are handled via per-benchmark min-max normalization to a common 0-100 scale."
}