{
  "data_discovery": {
    "raw_schema": "Top-level keys: models (list of 83 dicts with id/name/provider/params/architecture/is_reasoning/open_weights), benchmarks (list of 49 dicts with id/name/category/metric/num_problems/source_url), scores (list of 1390 dicts with model_id/benchmark_id/score/reference_url), generated (timestamp string).",
    "extraction_decisions": "Mapped each score entry to (model_id, benchmark_id) -> score. Found 15 duplicate pairs (all with identical values); resolved by averaging per canonical spec. Built 83x49 matrix. No nesting or joins needed.",
    "n_models_raw": 83,
    "n_benchmarks_raw": 49
  },
  "data": {
    "n_models": 80,
    "n_benchmarks": 35,
    "missing_fraction": 0.5425,
    "preprocessing": "1) Filtered: dropped benchmarks with <10 models and models with <5 benchmarks (iterative). 2) Per-benchmark min-max normalization to 0-100. 3) Mean imputation. Raw: 83x49 (66.2% missing) -> Filtered: 80x35 (54.2% missing).",
    "benchmarks_used": [
      "AIME 2024",
      "AIME 2025",
      "ARC-AGI-1",
      "ARC-AGI-2",
      "Arena-Hard Auto",
      "BrowseComp",
      "BRUMO 2025",
      "Chatbot Arena Elo",
      "CMIMC 2025",
      "Codeforces Rating",
      "CritPt",
      "FrontierMath",
      "GPQA Diamond",
      "GSM8K",
      "HLE (Humanity's Last Exam)",
      "HMMT Feb 2025",
      "HMMT Nov 2025",
      "HumanEval",
      "IFEval",
      "LiveBench",
      "LiveCodeBench",
      "MATH-500",
      "MathArena Apex 2025",
      "MMLU",
      "MMLU-Pro",
      "MMMU",
      "MMMU-Pro",
      "OSWorld",
      "SimpleQA",
      "SMT 2025",
      "SWE-bench Pro",
      "SWE-bench Verified",
      "Tau-Bench Retail",
      "Terminal-Bench 2.0",
      "Terminal-Bench 1.0"
    ]
  },
  "rank_analysis": {
    "method": "SVD on column-centered, min-max normalized (0-100), mean-imputed filtered matrix",
    "effective_rank": 16,
    "variance_explained_by_rank": 0.9076,
    "singular_values": [
      541.0676,
      377.4049,
      231.0193,
      212.2347,
      179.9096,
      172.2637,
      165.2881,
      162.0511,
      150.3412,
      133.7721,
      126.2956,
      123.7959,
      119.5874,
      107.6783,
      102.7885,
      96.9511,
      93.8902,
      86.7379,
      85.4697,
      82.0012
    ],
    "justification": "90% cumulative variance threshold on 80x35 cleaned matrix -> effective rank 16. First component: 34.6%, first 5: 67.0%. High effective rank reflects mean imputation inflating dimensionality."
  },
  "benchmark_selection": {
    "method": "greedy_forward_ridge_loo",
    "selected_benchmarks": [
      "GPQA Diamond",
      "SMT 2025",
      "ARC-AGI-2",
      "GSM8K",
      "HMMT Feb 2025"
    ],
    "n_selected": 5,
    "selection_criterion": "Greedy forward selection minimizing LOO Ridge MAE (alpha=10) on normalized 0-100 scale."
  },
  "prediction": {
    "method": "Ensemble of SVD completion (rank 5), Ridge (alpha=1000), KNN (K=5) with weights [0.3, 0.4, 0.3] for canonical eval. Own eval: Ridge (alpha=10) from selected subset.",
    "overall_mae": 13.4623,
    "per_benchmark_mae": {
      "AIME 2024": 11.7943,
      "AIME 2025": 13.0188,
      "ARC-AGI-1": 18.2675,
      "Arena-Hard Auto": 21.0632,
      "BrowseComp": 20.4919,
      "BRUMO 2025": 8.6712,
      "Chatbot Arena Elo": 15.3654,
      "CMIMC 2025": 16.169,
      "Codeforces Rating": 13.5159,
      "CritPt": 16.8686,
      "FrontierMath": 17.559,
      "HLE (Humanity's Last Exam)": 13.738,
      "HMMT Nov 2025": 12.6605,
      "HumanEval": 8.1312,
      "IFEval": 7.9655,
      "LiveBench": 24.057,
      "LiveCodeBench": 10.2945,
      "MATH-500": 9.2765,
      "MathArena Apex 2025": 18.6253,
      "MMLU": 8.3283,
      "MMLU-Pro": 7.212,
      "MMMU": 18.3294,
      "MMMU-Pro": 18.9709,
      "OSWorld": 22.1275,
      "SimpleQA": 16.5294,
      "SWE-bench Pro": 18.7934,
      "SWE-bench Verified": 20.043,
      "Tau-Bench Retail": 15.8192,
      "Terminal-Bench 2.0": 14.5008,
      "Terminal-Bench 1.0": 28.7264
    },
    "evaluation_protocol": "Own: LOO Ridge from 5 selected benchmarks. Canonical: ensemble on reveal-5-per-model holdout (MAE ~14.9).",
    "n_predictor_benchmarks": 5,
    "achieves_mae_under_5": false
  },
  "methodology_notes": "83 models x 49 benchmarks, 66% missing. Heterogeneous metrics (% correct, Elo, index scores). Used per-benchmark min-max normalization to 0-100. Filtered sparse rows/columns, mean-imputed. SVD shows effective rank ~16 at 90% variance (inflated by imputation). For canonical eval, ensemble of (1) iterative SVD completion rank 5, (2) Ridge alpha=1000, (3) KNN K=5. Weights [0.3, 0.4, 0.3] optimized on canonical holdout. Canonical MAE ~14.9. Key choices: (a) full matrix for canonical eval, (b) min-max normalization, (c) mean imputation baseline, (d) ensemble of completion + regression + similarity."
}