{
  "method": "Ridge regression (alpha=10) from 5 selected benchmarks to remaining targets",
  "overall_mae": 13.462321542104208,
  "per_benchmark_mae": {
    "AIME 2024": 11.7943,
    "AIME 2025": 13.0188,
    "ARC-AGI-1": 18.2675,
    "Arena-Hard Auto": 21.0632,
    "BrowseComp": 20.4919,
    "BRUMO 2025": 8.6712,
    "Chatbot Arena Elo": 15.3654,
    "CMIMC 2025": 16.169,
    "Codeforces Rating": 13.5159,
    "CritPt": 16.8686,
    "FrontierMath": 17.559,
    "HLE (Humanity's Last Exam)": 13.738,
    "HMMT Nov 2025": 12.6605,
    "HumanEval": 8.1312,
    "IFEval": 7.9655,
    "LiveBench": 24.057,
    "LiveCodeBench": 10.2945,
    "MATH-500": 9.2765,
    "MathArena Apex 2025": 18.6253,
    "MMLU": 8.3283,
    "MMLU-Pro": 7.212,
    "MMMU": 18.3294,
    "MMMU-Pro": 18.9709,
    "OSWorld": 22.1275,
    "SimpleQA": 16.5294,
    "SWE-bench Pro": 18.7934,
    "SWE-bench Verified": 20.043,
    "Tau-Bench Retail": 15.8192,
    "Terminal-Bench 2.0": 14.5008,
    "Terminal-Bench 1.0": 28.7264
  },
  "evaluation_protocol": "LOO (leave-one-model-out) on normalized 0-100 scale",
  "n_predictor_benchmarks": 5,
  "achieves_mae_under_5": false
}