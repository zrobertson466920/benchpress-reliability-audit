{
  "data_discovery": {
    "raw_schema": "JSON with keys: 'models' (83 objects: id, name, provider, release_date, params_total_M, params_active_M, architecture, is_reasoning, open_weights), 'benchmarks' (49 objects: id, name, category, metric, num_problems, source_url), 'scores' (1390 entries: model_id, benchmark_id, score, reference_url), 'generated' (timestamp).",
    "extraction_decisions": "Matrix built with model_id as rows, benchmark_id as columns. 15 duplicate (model_id, benchmark_id) pairs resolved by simple averaging. All 1390 score entries used.",
    "n_models_raw": 83,
    "n_benchmarks_raw": 49
  },
  "data": {
    "n_models": 74,
    "n_benchmarks": 35,
    "missing_fraction": 0.5193,
    "preprocessing": "Filtered benchmarks with >= 12 observed models (49 -> 35) and models with >= 8 observed benchmarks (83 -> 74). Per-benchmark min-max normalization to [0,100] using full observed data. Missing values imputed via iterative rank-5 SVD (soft-impute, 300 max iterations, tol=1e-5).",
    "benchmarks_used": [
      "GPQA Diamond",
      "AIME 2025",
      "MMLU",
      "MMLU-Pro",
      "SWE-bench Verified",
      "MATH-500",
      "LiveCodeBench",
      "FrontierMath",
      "HLE (Humanity's Last Exam)",
      "ARC-AGI-2",
      "BrowseComp",
      "SimpleQA",
      "IFEval",
      "HumanEval",
      "Codeforces Rating",
      "OSWorld",
      "MMMU",
      "MMMU-Pro",
      "Arena-Hard Auto",
      "Chatbot Arena Elo",
      "SWE-bench Pro",
      "AIME 2024",
      "HMMT Feb 2025",
      "Tau-Bench Retail",
      "CritPt",
      "GSM8K",
      "Terminal-Bench 2.0",
      "Terminal-Bench 1.0",
      "ARC-AGI-1",
      "BRUMO 2025",
      "SMT 2025",
      "HMMT Nov 2025",
      "CMIMC 2025",
      "MathArena Apex 2025",
      "LiveBench"
    ]
  },
  "rank_analysis": {
    "method": "SVD on min-max normalized (0-100), soft-imputed matrix",
    "effective_rank": 3,
    "variance_explained_by_rank": 0.9111,
    "singular_values": [
      3101.47,
      1323.54,
      1093.02,
      902.74,
      578.41,
      104.6,
      95.48,
      87.09,
      76.41,
      72.25,
      71.75,
      64.71,
      62.31,
      58.67,
      54.9
    ],
    "justification": "Using 90% cumulative variance threshold, effective rank = 3. The first singular value (3101) dominates, explaining 69.7% of variance alone\u2014indicating a strong general capability factor. Rank 3 captures 91.1% of total variance."
  },
  "benchmark_selection": {
    "method": "greedy_forward_selection",
    "selected_benchmarks": [
      "BRUMO 2025",
      "BrowseComp",
      "Tau-Bench Retail",
      "GSM8K",
      "CMIMC 2025"
    ],
    "n_selected": 5,
    "selection_criterion": "Minimize LOO prediction MAE (ridge, alpha=10) on normalized 0-100 scale"
  },
  "prediction": {
    "method": "ridge_regression (own eval); ridge+KNN blend (canonical eval)",
    "overall_mae": 7.0823,
    "per_benchmark_mae": {
      "GPQA Diamond": 4.8736,
      "AIME 2025": 7.1391,
      "MMLU": 5.6298,
      "MMLU-Pro": 5.9804,
      "SWE-bench Verified": 8.0525,
      "MATH-500": 5.7288,
      "LiveCodeBench": 6.7128,
      "FrontierMath": 9.3968,
      "HLE (Humanity's Last Exam)": 9.3117,
      "ARC-AGI-2": 8.2328,
      "SimpleQA": 8.5647,
      "IFEval": 5.5351,
      "HumanEval": 6.1194,
      "Codeforces Rating": 8.4144,
      "OSWorld": 6.9765,
      "MMMU": 8.6664,
      "MMMU-Pro": 13.018,
      "Arena-Hard Auto": 4.9753,
      "Chatbot Arena Elo": 7.4308,
      "SWE-bench Pro": 12.5724,
      "AIME 2024": 6.2015,
      "HMMT Feb 2025": 5.6887,
      "CritPt": 8.551,
      "Terminal-Bench 2.0": 5.7039,
      "Terminal-Bench 1.0": 6.8084,
      "ARC-AGI-1": 8.4226,
      "SMT 2025": 8.499,
      "HMMT Nov 2025": 9.9009,
      "MathArena Apex 2025": 9.8019,
      "LiveBench": 13.1769
    },
    "evaluation_protocol": "leave-one-model-out cross-validation on filtered matrix",
    "n_predictor_benchmarks": 5,
    "achieves_mae_under_5": false
  },
  "canonical_evaluation": {
    "canonical_overall_mae": 13.3184,
    "n_predictions": 196,
    "n_held_out": 196,
    "coverage": 1.0,
    "method": "Blend of ridge (alpha=5.0) and KNN (k=7), weights ridge=0.3/knn=0.7. Fallback: column mean for benchmarks outside working set."
  },
  "methodology_notes": "Pipeline: (1) Extract 83x49 matrix from JSON, averaging 15 duplicate pairs. (2) Filter to benchmarks with >=12 observed models and models with >=8 observed benchmarks -> 74x35 matrix with 51.9% missing. (3) Per-benchmark min-max normalize to [0,100]. (4) Soft-impute (rank 5) fills missing values. (5) SVD yields effective rank 3 (90% variance). (6) Greedy forward selection picks 5 benchmarks minimizing LOO ridge MAE. (7) Ridge regression for own evaluation (MAE=7.08). (8) Canonical evaluation: blend of ridge+KNN from 5 revealed benchmarks per model (MAE=13.32). Key observations: strong low-rank structure with dominant first component; mixed metrics (Elo, %) handled via min-max normalization; 15 held-out pairs for benchmarks outside working set use column-mean fallback."
}