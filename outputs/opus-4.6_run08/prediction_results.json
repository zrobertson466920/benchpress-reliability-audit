{
  "method": "ridge_regression",
  "overall_mae": 7.0823,
  "per_benchmark_mae": {
    "GPQA Diamond": 4.8736,
    "AIME 2025": 7.1391,
    "MMLU": 5.6298,
    "MMLU-Pro": 5.9804,
    "SWE-bench Verified": 8.0525,
    "MATH-500": 5.7288,
    "LiveCodeBench": 6.7128,
    "FrontierMath": 9.3968,
    "HLE (Humanity's Last Exam)": 9.3117,
    "ARC-AGI-2": 8.2328,
    "SimpleQA": 8.5647,
    "IFEval": 5.5351,
    "HumanEval": 6.1194,
    "Codeforces Rating": 8.4144,
    "OSWorld": 6.9765,
    "MMMU": 8.6664,
    "MMMU-Pro": 13.018,
    "Arena-Hard Auto": 4.9753,
    "Chatbot Arena Elo": 7.4308,
    "SWE-bench Pro": 12.5724,
    "AIME 2024": 6.2015,
    "HMMT Feb 2025": 5.6887,
    "CritPt": 8.551,
    "Terminal-Bench 2.0": 5.7039,
    "Terminal-Bench 1.0": 6.8084,
    "ARC-AGI-1": 8.4226,
    "SMT 2025": 8.499,
    "HMMT Nov 2025": 9.9009,
    "MathArena Apex 2025": 9.8019,
    "LiveBench": 13.1769
  },
  "evaluation_protocol": "leave-one-model-out cross-validation",
  "n_predictor_benchmarks": 5,
  "predictor_benchmarks": [
    "BRUMO 2025",
    "BrowseComp",
    "Tau-Bench Retail",
    "GSM8K",
    "CMIMC 2025"
  ],
  "alpha": 10.0,
  "achieves_mae_under_5": false,
  "notes": "MAE on normalized 0-100 scale; ridge from selected 5 benchmarks to each target"
}