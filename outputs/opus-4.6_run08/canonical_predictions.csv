model_id,model_name,benchmark_id,benchmark_name,y_pred
claude-opus-4,Claude Opus 4,aime_2024,AIME 2024,84.1063
claude-opus-4,Claude Opus 4,aime_2025,AIME 2025,84.0966
claude-opus-4,Claude Opus 4,arc_agi_1,ARC-AGI-1,62.7724
claude-opus-4,Claude Opus 4,arc_agi_2,ARC-AGI-2,12.9789
claude-opus-4,Claude Opus 4,codeforces_rating,Codeforces Rating,2227.7039
claude-opus-4,Claude Opus 4,critpt,CritPt,3.2451
claude-opus-4,Claude Opus 4,frontiermath,FrontierMath,19.0674
claude-opus-4,Claude Opus 4,gpqa_diamond,GPQA Diamond,83.8039
claude-opus-4,Claude Opus 4,hle,HLE (Humanity's Last Exam),22.4153
claude-opus-4,Claude Opus 4,hmmt_2025,HMMT Feb 2025,66.0584
claude-opus-4,Claude Opus 4,ifeval,IFEval,89.9032
claude-opus-4,Claude Opus 4,livecodebench,LiveCodeBench,74.2913
claude-opus-4,Claude Opus 4,math_500,MATH-500,99.2763
claude-opus-4,Claude Opus 4,mmlu,MMLU,89.6354
claude-opus-4,Claude Opus 4,mmmu,MMMU,79.8792
claude-opus-4,Claude Opus 4,simpleqa,SimpleQA,42.9519
claude-opus-4,Claude Opus 4,swe_bench_pro,SWE-bench Pro,38.797
claude-opus-4,Claude Opus 4,tau_bench_retail,Tau-Bench Retail,81.3548
claude-opus-4,Claude Opus 4,terminal_bench,Terminal-Bench 2.0,36.2104
claude-opus-4.1,Claude Opus 4.1,frontiermath,FrontierMath,18.004
claude-opus-4.1,Claude Opus 4.1,gpqa_diamond,GPQA Diamond,76.6243
claude-opus-4.1,Claude Opus 4.1,hle,HLE (Humanity's Last Exam),16.9187
claude-opus-4.1,Claude Opus 4.1,humaneval,HumanEval,85.8681
claude-opus-4.1,Claude Opus 4.1,mmlu,MMLU,86.899
claude-opus-4.1,Claude Opus 4.1,mmlu_pro,MMLU-Pro,79.4902
claude-opus-4.1,Claude Opus 4.1,simpleqa,SimpleQA,41.8989
claude-opus-4.1,Claude Opus 4.1,swe_bench_pro,SWE-bench Pro,37.4408
claude-opus-4.1,Claude Opus 4.1,terminal_bench,Terminal-Bench 2.0,38.847
claude-opus-4.1,Claude Opus 4.1,terminal_bench_1,Terminal-Bench 1.0,46.8081
deepseek-r1-0528,DeepSeek-R1-0528,aime_2024,AIME 2024,88.0984
deepseek-r1-0528,DeepSeek-R1-0528,aime_2025,AIME 2025,84.8557
deepseek-r1-0528,DeepSeek-R1-0528,arc_agi_1,ARC-AGI-1,36.4081
deepseek-r1-0528,DeepSeek-R1-0528,brumo_2025,BRUMO 2025,89.714
deepseek-r1-0528,DeepSeek-R1-0528,codeforces_rating,Codeforces Rating,2062.6181
deepseek-r1-0528,DeepSeek-R1-0528,gpqa_diamond,GPQA Diamond,81.0918
deepseek-r1-0528,DeepSeek-R1-0528,hle,HLE (Humanity's Last Exam),20.8282
deepseek-r1-0528,DeepSeek-R1-0528,humaneval,HumanEval,88.8753
deepseek-r1-0528,DeepSeek-R1-0528,ifeval,IFEval,87.7876
deepseek-r1-0528,DeepSeek-R1-0528,imo_2025,IMO 2025,28.9514
deepseek-r1-0528,DeepSeek-R1-0528,livecodebench,LiveCodeBench,71.6374
deepseek-r1-0528,DeepSeek-R1-0528,math_500,MATH-500,95.9783
deepseek-r1-0528,DeepSeek-R1-0528,matharena_apex_2025,MathArena Apex 2025,4.3955
deepseek-r1-0528,DeepSeek-R1-0528,mmlu,MMLU,87.969
deepseek-r1-0528,DeepSeek-R1-0528,mmlu_pro,MMLU-Pro,83.9022
deepseek-r1-0528,DeepSeek-R1-0528,simpleqa,SimpleQA,36.1107
deepseek-r1-0528,DeepSeek-R1-0528,smt_2025,SMT 2025,81.3999
deepseek-r1-0528,DeepSeek-R1-0528,swe_bench_verified,SWE-bench Verified,63.0085
deepseek-r1-0528,DeepSeek-R1-0528,usamo_2025,USAMO 2025,20.9562
doubao-seed-2.0-pro,Doubao Seed 2.0 Pro,aime_2024,AIME 2024,102.4093
doubao-seed-2.0-pro,Doubao Seed 2.0 Pro,aime_2025,AIME 2025,99.3472
doubao-seed-2.0-pro,Doubao Seed 2.0 Pro,browsecomp,BrowseComp,61.6333
doubao-seed-2.0-pro,Doubao Seed 2.0 Pro,gpqa_diamond,GPQA Diamond,91.8518
doubao-seed-2.0-pro,Doubao Seed 2.0 Pro,hle,HLE (Humanity's Last Exam),36.0924
doubao-seed-2.0-pro,Doubao Seed 2.0 Pro,ifeval,IFEval,90.6821
doubao-seed-2.0-pro,Doubao Seed 2.0 Pro,mathvision,MathVision,83.275
doubao-seed-2.0-pro,Doubao Seed 2.0 Pro,mmlu,MMLU,90.9738
doubao-seed-2.0-pro,Doubao Seed 2.0 Pro,mmmu,MMMU,83.7168
doubao-seed-2.0-pro,Doubao Seed 2.0 Pro,swe_bench_verified,SWE-bench Verified,80.935
doubao-seed-2.0-pro,Doubao Seed 2.0 Pro,terminal_bench,Terminal-Bench 2.0,41.6035
gemini-2.5-pro,Gemini 2.5 Pro,aa_intelligence_index,AA Intelligence Index,62.875
gemini-2.5-pro,Gemini 2.5 Pro,aime_2024,AIME 2024,85.597
gemini-2.5-pro,Gemini 2.5 Pro,aime_2025,AIME 2025,77.8688
gemini-2.5-pro,Gemini 2.5 Pro,arc_agi_1,ARC-AGI-1,25.7228
gemini-2.5-pro,Gemini 2.5 Pro,arc_agi_2,ARC-AGI-2,1.7075
gemini-2.5-pro,Gemini 2.5 Pro,arena_hard,Arena-Hard Auto,74.0345
gemini-2.5-pro,Gemini 2.5 Pro,brumo_2025,BRUMO 2025,81.4438
gemini-2.5-pro,Gemini 2.5 Pro,chatbot_arena_elo,Chatbot Arena Elo,1395.8868
gemini-2.5-pro,Gemini 2.5 Pro,codeforces_rating,Codeforces Rating,1940.7841
gemini-2.5-pro,Gemini 2.5 Pro,critpt,CritPt,0.7221
gemini-2.5-pro,Gemini 2.5 Pro,frontiermath,FrontierMath,6.9244
gemini-2.5-pro,Gemini 2.5 Pro,gpqa_diamond,GPQA Diamond,81.3277
gemini-2.5-pro,Gemini 2.5 Pro,hle,HLE (Humanity's Last Exam),10.6372
gemini-2.5-pro,Gemini 2.5 Pro,hmmt_2025,HMMT Feb 2025,49.7237
gemini-2.5-pro,Gemini 2.5 Pro,hmmt_nov_2025,HMMT Nov 2025,78.48
gemini-2.5-pro,Gemini 2.5 Pro,imo_2025,IMO 2025,28.9514
gemini-2.5-pro,Gemini 2.5 Pro,livebench,LiveBench,48.1362
gemini-2.5-pro,Gemini 2.5 Pro,math_500,MATH-500,97.8753
gemini-2.5-pro,Gemini 2.5 Pro,matharena_apex_2025,MathArena Apex 2025,2.0998
gemini-2.5-pro,Gemini 2.5 Pro,mmlu,MMLU,88.9052
gemini-2.5-pro,Gemini 2.5 Pro,mmlu_pro,MMLU-Pro,83.3664
gemini-2.5-pro,Gemini 2.5 Pro,mmmu,MMMU,75.0772
gemini-2.5-pro,Gemini 2.5 Pro,mrcr_v2,MRCR v2,72.6
gemini-2.5-pro,Gemini 2.5 Pro,simplebench,SimpleBench,66.64
gemini-2.5-pro,Gemini 2.5 Pro,simpleqa,SimpleQA,26.1353
gemini-2.5-pro,Gemini 2.5 Pro,smt_2025,SMT 2025,71.1535
gemini-2.5-pro,Gemini 2.5 Pro,swe_bench_verified,SWE-bench Verified,59.7516
gemini-2.5-pro,Gemini 2.5 Pro,terminal_bench_1,Terminal-Bench 1.0,23.4591
gemini-2.5-pro,Gemini 2.5 Pro,usamo_2025,USAMO 2025,20.9562
gemini-2.5-pro,Gemini 2.5 Pro,video_mmu,Video-MMU,85.2375
gpt-4.1,GPT-4.1,aime_2025,AIME 2025,56.9962
gpt-4.1,GPT-4.1,arc_agi_1,ARC-AGI-1,3.4907
gpt-4.1,GPT-4.1,arena_hard,Arena-Hard Auto,64.8757
gpt-4.1,GPT-4.1,codeforces_rating,Codeforces Rating,1506.0531
gpt-4.1,GPT-4.1,hle,HLE (Humanity's Last Exam),8.5906
gpt-4.1,GPT-4.1,hmmt_2025,HMMT Feb 2025,26.742
gpt-4.1,GPT-4.1,humaneval,HumanEval,84.5426
gpt-4.1,GPT-4.1,ifeval,IFEval,85.5204
gpt-4.1,GPT-4.1,livecodebench,LiveCodeBench,46.5907
gpt-4.1,GPT-4.1,math_500,MATH-500,88.8032
gpt-4.1,GPT-4.1,mmlu,MMLU,88.7842
gpt-4.1,GPT-4.1,mmmu,MMMU,76.4025
gpt-4.1,GPT-4.1,simpleqa,SimpleQA,27.9682
gpt-4.1,GPT-4.1,swe_bench_verified,SWE-bench Verified,44.4685
gpt-4.1,GPT-4.1,terminal_bench_1,Terminal-Bench 1.0,16.6286
grok-3-beta,Grok 3 Beta,aime_2024,AIME 2024,91.5526
grok-3-beta,Grok 3 Beta,aime_2025,AIME 2025,88.5531
grok-3-beta,Grok 3 Beta,arc_agi_1,ARC-AGI-1,43.504
grok-3-beta,Grok 3 Beta,chatbot_arena_elo,Chatbot Arena Elo,1429.868
grok-3-beta,Grok 3 Beta,hle,HLE (Humanity's Last Exam),23.134
grok-3-beta,Grok 3 Beta,humaneval,HumanEval,92.3424
grok-3-beta,Grok 3 Beta,ifeval,IFEval,89.0109
grok-3-beta,Grok 3 Beta,mmlu_pro,MMLU-Pro,84.0317
grok-3-beta,Grok 3 Beta,simpleqa,SimpleQA,32.7332
grok-3-beta,Grok 3 Beta,swe_bench_verified,SWE-bench Verified,65.7672
grok-4,Grok 4,aa_intelligence_index,AA Intelligence Index,62.875
grok-4,Grok 4,aa_lcr,AA Long Context Reasoning,72.1857
grok-4,Grok 4,aime_2024,AIME 2024,94.3859
grok-4,Grok 4,arc_agi_2,ARC-AGI-2,18.0205
grok-4,Grok 4,brumo_2025,BRUMO 2025,95.2132
grok-4,Grok 4,chatbot_arena_elo,Chatbot Arena Elo,1461.1451
grok-4,Grok 4,cmimc_2025,CMIMC 2025,87.4142
grok-4,Grok 4,codeforces_rating,Codeforces Rating,2353.53
grok-4,Grok 4,frontiermath,FrontierMath,24.1584
grok-4,Grok 4,gpqa_diamond,GPQA Diamond,85.8639
grok-4,Grok 4,hle,HLE (Humanity's Last Exam),33.1022
grok-4,Grok 4,hmmt_nov_2025,HMMT Nov 2025,89.4776
grok-4,Grok 4,ifeval,IFEval,89.6388
grok-4,Grok 4,imo_2025,IMO 2025,28.9514
grok-4,Grok 4,livecodebench,LiveCodeBench,79.6392
grok-4,Grok 4,matharena_apex_2025,MathArena Apex 2025,4.78
grok-4,Grok 4,mmlu,MMLU,89.5367
grok-4,Grok 4,mmlu_pro,MMLU-Pro,85.7551
grok-4,Grok 4,mmmu,MMMU,83.7675
grok-4,Grok 4,mmmu_pro,MMMU-Pro,77.8049
grok-4,Grok 4,osworld,OSWorld,47.7068
grok-4,Grok 4,simpleqa,SimpleQA,52.0769
grok-4,Grok 4,smt_2025,SMT 2025,90.1209
grok-4,Grok 4,swe_bench_pro,SWE-bench Pro,43.7553
grok-4,Grok 4,swe_bench_verified,SWE-bench Verified,75.678
grok-4,Grok 4,terminal_bench,Terminal-Bench 2.0,42.037
grok-4,Grok 4,terminal_bench_1,Terminal-Bench 1.0,41.114
grok-4,Grok 4,usamo_2025,USAMO 2025,20.9562
kimi-k2,Kimi K2,aime_2024,AIME 2024,74.9771
kimi-k2,Kimi K2,aime_2025,AIME 2025,70.3285
kimi-k2,Kimi K2,arena_hard,Arena-Hard Auto,70.3597
kimi-k2,Kimi K2,gpqa_diamond,GPQA Diamond,74.8884
kimi-k2,Kimi K2,hle,HLE (Humanity's Last Exam),16.5044
kimi-k2,Kimi K2,hmmt_2025,HMMT Feb 2025,39.8894
kimi-k2,Kimi K2,ifeval,IFEval,86.7561
kimi-k2,Kimi K2,livecodebench,LiveCodeBench,63.7353
kimi-k2,Kimi K2,math_500,MATH-500,97.1391
kimi-k2,Kimi K2,mmlu_pro,MMLU-Pro,79.7866
kimi-k2,Kimi K2,osworld,OSWorld,29.7787
kimi-k2,Kimi K2,simpleqa,SimpleQA,32.5774
kimi-k2,Kimi K2,swe_bench_verified,SWE-bench Verified,71.6479
llama-4-maverick,Llama 4 Maverick,arc_agi_1,ARC-AGI-1,5.7975
llama-4-maverick,Llama 4 Maverick,bigcodebench,BigCodeBench,45.4857
llama-4-maverick,Llama 4 Maverick,chatbot_arena_elo,Chatbot Arena Elo,1380.5153
llama-4-maverick,Llama 4 Maverick,gpqa_diamond,GPQA Diamond,66.1522
llama-4-maverick,Llama 4 Maverick,humaneval,HumanEval,84.1172
llama-4-maverick,Llama 4 Maverick,livecodebench,LiveCodeBench,50.5252
llama-4-maverick,Llama 4 Maverick,math_500,MATH-500,91.0017
llama-4-maverick,Llama 4 Maverick,mmmu,MMMU,75.3751
llama-4-maverick,Llama 4 Maverick,simpleqa,SimpleQA,26.7387
llama-4-maverick,Llama 4 Maverick,swe_bench_verified,SWE-bench Verified,47.8278
llama-4-maverick,Llama 4 Maverick,terminal_bench_1,Terminal-Bench 1.0,15.0763
minimax-m2,MiniMax-M2,aime_2024,AIME 2024,86.4112
minimax-m2,MiniMax-M2,aime_2025,AIME 2025,75.4707
minimax-m2,MiniMax-M2,chatbot_arena_elo,Chatbot Arena Elo,1414.938
minimax-m2,MiniMax-M2,gpqa_diamond,GPQA Diamond,83.0071
minimax-m2,MiniMax-M2,hle,HLE (Humanity's Last Exam),17.2878
minimax-m2,MiniMax-M2,humaneval,HumanEval,90.732
minimax-m2,MiniMax-M2,ifeval,IFEval,88.1279
minimax-m2,MiniMax-M2,math_500,MATH-500,98.1457
minimax-m2,MiniMax-M2,mmlu,MMLU,88.9787
minimax-m2,MiniMax-M2,mmlu_pro,MMLU-Pro,85.1864
minimax-m2,MiniMax-M2,mmmu,MMMU,76.4026
minimax-m2,MiniMax-M2,simpleqa,SimpleQA,36.3292
minimax-m2,MiniMax-M2,swe_bench_pro,SWE-bench Pro,42.385
minimax-m2,MiniMax-M2,swe_bench_verified,SWE-bench Verified,65.7204
minimax-m2,MiniMax-M2,terminal_bench,Terminal-Bench 2.0,37.415
o3-mini-high,o3-mini (high),aime_2024,AIME 2024,89.1092
o3-mini-high,o3-mini (high),arc_agi_1,ARC-AGI-1,37.3853
o3-mini-high,o3-mini (high),arc_agi_2,ARC-AGI-2,7.2681
o3-mini-high,o3-mini (high),arena_hard,Arena-Hard Auto,70.8111
o3-mini-high,o3-mini (high),chatbot_arena_elo,Chatbot Arena Elo,1430.9431
o3-mini-high,o3-mini (high),frontiermath,FrontierMath,12.7525
o3-mini-high,o3-mini (high),hle,HLE (Humanity's Last Exam),25.7596
o3-mini-high,o3-mini (high),hmmt_2025,HMMT Feb 2025,79.5113
o3-mini-high,o3-mini (high),humaneval,HumanEval,88.8114
o3-mini-high,o3-mini (high),ifeval,IFEval,86.6652
o3-mini-high,o3-mini (high),livecodebench,LiveCodeBench,72.7764
o3-mini-high,o3-mini (high),math_500,MATH-500,95.26
o3-mini-high,o3-mini (high),simpleqa,SimpleQA,39.3232
o3-mini-high,o3-mini (high),swe_bench_verified,SWE-bench Verified,66.5131
o3-mini-high,o3-mini (high),usamo_2025,USAMO 2025,20.9562
