{
  "data_discovery": {
    "raw_schema": "JSON with top-level keys: 'models' (list of 83 model objects with id/name/provider/params/architecture/is_reasoning/open_weights), 'benchmarks' (list of 49 benchmark objects with id/name/category/metric/num_problems/source_url), 'scores' (list of 1390 score entries with model_id/benchmark_id/score/reference_url), 'generated' (timestamp string).",
    "extraction_decisions": "Mapped scores to (model_id, benchmark_id) pairs. Found 15 duplicate pairs (all from deepseek-r1-distill variants); resolved by averaging. All model_ids and benchmark_ids in scores matched the models/benchmarks arrays. Used model 'name' field for display, 'id' for indexing. Benchmark names used as column headers.",
    "n_models_raw": 83,
    "n_benchmarks_raw": 49
  },
  "data": {
    "n_models": 83,
    "n_benchmarks": 49,
    "missing_fraction": 0.661912957954266,
    "preprocessing": "Per-benchmark min-max normalization to [0,100] scale (to handle mixed metrics: percentages, Elo ratings ~800-3020, Codeforces ratings). Missing values imputed with per-benchmark column means after normalization. No models or benchmarks filtered. Cleaning = normalization + mean imputation.",
    "benchmarks_used": [
      "GPQA Diamond",
      "AIME 2025",
      "MMLU",
      "MMLU-Pro",
      "SWE-bench Verified",
      "MATH-500",
      "LiveCodeBench",
      "FrontierMath",
      "HLE (Humanity's Last Exam)",
      "ARC-AGI-2",
      "BrowseComp",
      "SimpleQA",
      "IFEval",
      "HumanEval",
      "Codeforces Rating",
      "OSWorld",
      "MMMU",
      "MMMU-Pro",
      "Arena-Hard Auto",
      "Chatbot Arena Elo",
      "SWE-bench Pro",
      "AIME 2024",
      "HMMT Feb 2025",
      "Tau-Bench Retail",
      "Tau-Bench Telecom",
      "Video-MMU",
      "MRCR v2",
      "AA Intelligence Index",
      "AA Long Context Reasoning",
      "CritPt",
      "SciCode",
      "MathVision",
      "GDP-Val AA",
      "GSM8K",
      "IFBench",
      "Terminal-Bench 2.0",
      "Terminal-Bench 1.0",
      "ARC-AGI-1",
      "BRUMO 2025",
      "SMT 2025",
      "USAMO 2025",
      "HMMT Nov 2025",
      "CMIMC 2025",
      "IMO 2025",
      "AIME 2026",
      "MathArena Apex 2025",
      "LiveBench",
      "SimpleBench",
      "BigCodeBench"
    ]
  },
  "rank_analysis": {
    "method": "SVD on mean-imputed, min-max normalized, column-centered matrix",
    "effective_rank": 20,
    "variance_explained_by_rank": 0.9062395034673678,
    "singular_values": [
      542.523593040778,
      384.0726761011769,
      241.94370735972382,
      219.27786720944758,
      188.74906566502904,
      177.4301143968425,
      173.7033078218942,
      171.94871853418,
      164.16473961640645,
      148.76859314817753,
      136.20438750868368,
      131.14816413291936,
      128.69945712303624,
      125.25014070094691,
      118.4829575187403,
      117.06376973280445,
      110.20496839634146,
      105.99101293199759,
      100.6149644704711,
      97.88765813215005
    ],
    "justification": "Used 90% cumulative variance threshold. First component explains 30.9% of variance, first two explain 46.4%. The spectrum decays gradually without a sharp elbow \u2014 90% requires rank 20. This is likely inflated by the high missing fraction (66.2%) since column-mean imputation flattens the true low-rank structure. A more conservative interpretation using the SV[1]/SV[2] ratio gap of 1.587 suggests an effective rank of 2-3 for the dominant signal."
  },
  "benchmark_selection": {
    "method": "Greedy forward selection",
    "selected_benchmarks": [
      "MMLU-Pro",
      "AIME 2024",
      "HMMT Feb 2025",
      "SMT 2025",
      "SWE-bench Verified",
      "FrontierMath",
      "AIME 2025"
    ],
    "n_selected": 7,
    "selection_criterion": "Minimize average leave-one-out MAE (Ridge, alpha selected by RidgeCV) across all non-selected benchmarks on min-max normalized mean-imputed matrix. Selected 7 benchmarks covering Math (AIME 2024, AIME 2025, HMMT Feb 2025, SMT 2025, FrontierMath), Knowledge (MMLU-Pro), and Coding/Agentic (SWE-bench Verified)."
  },
  "prediction": {
    "method": "Ridge regression (alpha=10) from 5 revealed benchmarks per eval model to each target benchmark. For own eval: Ridge from 7 selected benchmarks to each target.",
    "overall_mae": 6.534397118742889,
    "per_benchmark_mae": {
      "GPQA Diamond": 10.82456875989395,
      "MMLU": 7.364634025061031,
      "MATH-500": 7.304066222278789,
      "LiveCodeBench": 10.043038297747591,
      "HLE (Humanity's Last Exam)": 9.280950077153575,
      "ARC-AGI-2": 9.21610501527077,
      "BrowseComp": 6.882725053196956,
      "SimpleQA": 12.251937806486284,
      "IFEval": 8.100594717776541,
      "HumanEval": 8.802076230722891,
      "Codeforces Rating": 10.415791079330052,
      "OSWorld": 8.678187975557151,
      "MMMU": 9.98664680434317,
      "MMMU-Pro": 4.606565496720313,
      "Arena-Hard Auto": 12.601881881433114,
      "Chatbot Arena Elo": 8.377448978336325,
      "SWE-bench Pro": 8.40845811956927,
      "Tau-Bench Retail": 4.270428047027786,
      "Tau-Bench Telecom": 3.9468849188433204,
      "Video-MMU": 3.3823537942551716,
      "MRCR v2": 3.0642863139975782,
      "AA Intelligence Index": 5.750643929295792,
      "AA Long Context Reasoning": 5.679977763975141,
      "CritPt": 5.884417435040112,
      "SciCode": 2.8899134580611574,
      "MathVision": 3.098217437800008,
      "GDP-Val AA": 1.6796921974913717,
      "GSM8K": 5.6496483701524,
      "IFBench": 5.181396266571553,
      "Terminal-Bench 2.0": 8.306948784624183,
      "Terminal-Bench 1.0": 8.800689678054276,
      "ARC-AGI-1": 12.194137699881665,
      "BRUMO 2025": 2.6523644700043807,
      "USAMO 2025": 3.95247618765057,
      "HMMT Nov 2025": 3.5023634903048992,
      "CMIMC 2025": 5.8993890576540995,
      "IMO 2025": 3.511118979088483,
      "AIME 2026": 2.393055467805703,
      "MathArena Apex 2025": 6.017704811228818,
      "LiveBench": 6.107945657781361,
      "SimpleBench": 4.054789385640015,
      "BigCodeBench": 3.4281588440937276
    },
    "evaluation_protocol": "Own eval: leave-one-model-out CV on imputed normalized matrix (7 selected \u2192 42 targets). Canonical: ridge from 5 revealed \u2192 each held-out target, trained on all other models.",
    "n_predictor_benchmarks": 7,
    "achieves_mae_under_5": false,
    "canonical_mae": 16.411835569000825
  },
  "methodology_notes": "Key challenge: 66.2% missing data with only 5 revealed benchmarks per eval model in canonical task. The high missingness inflates apparent rank (90% var requires rank 20), though the dominant signal is likely rank 2-3 (first SV ratio gap). Greedy forward selection favored math benchmarks (5/7 selected), reflecting the dataset's math-heavy coverage. Canonical MAE of ~16 on 0-100 scale reflects the difficulty of predicting from only 5 benchmarks when those benchmarks have variable coverage across the training models. Column-mean imputation introduces bias but is necessary for SVD on this sparse matrix. Alternative methods tested (KNN, low-rank projection) did not improve over ridge from revealed benchmarks."
}