{
  "method": "ridge_regression_from_selected_subset",
  "overall_mae": 5.3888,
  "per_benchmark_mae": {
    "GPQA Diamond": 4.888,
    "AIME 2025": 6.6155,
    "MMLU": 2.6748,
    "SWE-bench Verified": 6.1985,
    "MATH-500": 5.0761,
    "LiveCodeBench": 6.0243,
    "FrontierMath": 3.7902,
    "HLE (Humanity's Last Exam)": 4.9777,
    "BrowseComp": 7.5891,
    "SimpleQA": 6.2321,
    "IFEval": 3.4626,
    "HumanEval": 4.4852,
    "OSWorld": 7.1521,
    "MMMU": 1.9944,
    "MMMU-Pro": 1.1858,
    "Arena-Hard Auto": 14.1794,
    "SWE-bench Pro": 6.161,
    "AIME 2024": 9.3544,
    "HMMT Feb 2025": 8.1539,
    "Tau-Bench Retail": 4.4747,
    "CritPt": 1.5717,
    "GSM8K": 3.3522,
    "Terminal-Bench 2.0": 6.2984,
    "Terminal-Bench 1.0": 6.6558,
    "ARC-AGI-1": 11.4795,
    "BRUMO 2025": 1.9341,
    "SMT 2025": 4.2985,
    "CMIMC 2025": 6.8211,
    "MathArena Apex 2025": 2.3414,
    "LiveBench": 2.24
  },
  "evaluation_protocol": "leave_one_model_out_cross_validation",
  "n_predictor_benchmarks": 5,
  "achieves_mae_under_5": false
}