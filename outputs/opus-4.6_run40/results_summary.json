{
  "data_discovery": {
    "raw_schema": "Top-level dict with keys: models (list of 83 model objects with id/name/provider/release_date/params_total_M/params_active_M/architecture/is_reasoning/open_weights), benchmarks (list of 49 benchmark objects with id/name/category/metric/num_problems/source_url), scores (list of 1390 score entries with model_id/benchmark_id/score/reference_url), generated (timestamp string '2026-02-24T12:03:39.038392').",
    "extraction_decisions": "Built 83x49 model-by-benchmark matrix by iterating over scores list. Found 15 duplicate (model_id, benchmark_id) pairs (all from deepseek-r1-distill variants), resolved by simple averaging per the canonical protocol. All 83 models and 49 benchmarks retained in raw matrix. Score values are numeric (mostly percentages 0-100, plus two Elo rating benchmarks with values ~900-2100 and a few index scores). No joins or restructuring needed; flat score list maps directly to matrix cells.",
    "n_models_raw": 83,
    "n_benchmarks_raw": 49
  },
  "data": {
    "n_models": 80,
    "n_benchmarks": 35,
    "missing_fraction": 0.5425,
    "preprocessing": "Filtered benchmarks with <10 model scores and models with <5 benchmark scores, reducing from 83x49 to 80x35. Remaining missing values (54.2%) imputed using sklearn IterativeImputer (max_iter=50, random_state=42, min_value=0). No score normalization applied to the cleaned matrix (raw scores preserved). Z-scoring applied per-benchmark only for SVD/rank analysis. For canonical predictions, used ensemble of low-rank projection and per-benchmark ridge regression on the full unfiltered matrix.",
    "benchmarks_used": [
      "GPQA Diamond",
      "AIME 2025",
      "MMLU",
      "MMLU-Pro",
      "SWE-bench Verified",
      "MATH-500",
      "LiveCodeBench",
      "FrontierMath",
      "HLE (Humanity's Last Exam)",
      "ARC-AGI-2",
      "BrowseComp",
      "SimpleQA",
      "IFEval",
      "HumanEval",
      "Codeforces Rating",
      "OSWorld",
      "MMMU",
      "MMMU-Pro",
      "Arena-Hard Auto",
      "Chatbot Arena Elo",
      "SWE-bench Pro",
      "AIME 2024",
      "HMMT Feb 2025",
      "Tau-Bench Retail",
      "CritPt",
      "GSM8K",
      "Terminal-Bench 2.0",
      "Terminal-Bench 1.0",
      "ARC-AGI-1",
      "BRUMO 2025",
      "SMT 2025",
      "HMMT Nov 2025",
      "CMIMC 2025",
      "MathArena Apex 2025",
      "LiveBench"
    ]
  },
  "rank_analysis": {
    "method": "SVD on z-scored (per-benchmark) imputed filtered matrix",
    "effective_rank": 6,
    "variance_explained_by_rank": 0.9061,
    "singular_values": [
      40.3493,
      20.6272,
      12.7248,
      11.2517,
      9.8852,
      8.1041,
      6.8936,
      5.9018,
      5.4376,
      5.0679,
      5.0016,
      4.2247,
      4.1991,
      3.7809,
      3.4031
    ],
    "justification": "Using 90% cumulative variance threshold on SVD of z-scored imputed matrix, effective rank = 6. The first component alone explains 58.9% of variance, indicating strong low-rank structure with a dominant 'general capability' factor. At rank 3, 80% of variance is captured. The spectrum drops steeply after the first few components."
  },
  "benchmark_selection": {
    "method": "greedy_forward_selection",
    "selected_benchmarks": [
      "Codeforces Rating",
      "HMMT Nov 2025",
      "MMLU-Pro",
      "ARC-AGI-2",
      "Chatbot Arena Elo"
    ],
    "n_selected": 5,
    "selection_criterion": "Minimize average MAE of ridge regression predictions for non-selected benchmarks on imputed filtered matrix. Greedy forward: at each step add the benchmark reducing residual MAE most."
  },
  "prediction": {
    "method": "ensemble_lowrank_ridge",
    "overall_mae": 5.3888,
    "per_benchmark_mae": {
      "GPQA Diamond": 4.888,
      "AIME 2025": 6.6155,
      "MMLU": 2.6748,
      "SWE-bench Verified": 6.1985,
      "MATH-500": 5.0761,
      "LiveCodeBench": 6.0243,
      "FrontierMath": 3.7902,
      "HLE (Humanity's Last Exam)": 4.9777,
      "BrowseComp": 7.5891,
      "SimpleQA": 6.2321,
      "IFEval": 3.4626,
      "HumanEval": 4.4852,
      "OSWorld": 7.1521,
      "MMMU": 1.9944,
      "MMMU-Pro": 1.1858,
      "Arena-Hard Auto": 14.1794,
      "SWE-bench Pro": 6.161,
      "AIME 2024": 9.3544,
      "HMMT Feb 2025": 8.1539,
      "Tau-Bench Retail": 4.4747,
      "CritPt": 1.5717,
      "GSM8K": 3.3522,
      "Terminal-Bench 2.0": 6.2984,
      "Terminal-Bench 1.0": 6.6558,
      "ARC-AGI-1": 11.4795,
      "BRUMO 2025": 1.9341,
      "SMT 2025": 4.2985,
      "CMIMC 2025": 6.8211,
      "MathArena Apex 2025": 2.3414,
      "LiveBench": 2.24
    },
    "evaluation_protocol": "leave_one_model_out_cross_validation on imputed filtered matrix using ridge from 5 selected benchmarks",
    "n_predictor_benchmarks": 5,
    "achieves_mae_under_5": false
  },
  "canonical_evaluation": {
    "method": "ensemble: 0.2 * low-rank SVD projection (rank=6, alpha=0.1) + 0.8 * per-benchmark ridge (alpha=100). Low-rank component: impute full matrix with column means, z-score, SVD on all models except target, project target's 5 revealed scores into low-rank space. Ridge component: for each held-out benchmark, train ridge from 5 revealed benchmarks to target using all other models with complete data.",
    "canonical_overall_mae": 15.5089,
    "canonical_coverage": 1.0,
    "canonical_per_benchmark_mae": {
      "AIME 2024": 5.7591,
      "AIME 2025": 14.0049,
      "ARC-AGI-1": 17.2355,
      "ARC-AGI-2": 14.4929,
      "Codeforces Rating": 11.7207,
      "CritPt": 22.8124,
      "FrontierMath": 29.3815,
      "GPQA Diamond": 8.2576,
      "HLE (Humanity's Last Exam)": 22.2515,
      "HMMT Feb 2025": 44.1624,
      "IFEval": 7.7374,
      "LiveCodeBench": 12.947,
      "MATH-500": 4.3734,
      "MMLU": 7.0214,
      "MMMU": 14.8717,
      "SimpleQA": 18.383,
      "SWE-bench Pro": 23.5865,
      "Tau-Bench Retail": 12.1241,
      "Terminal-Bench 2.0": 12.2276,
      "HumanEval": 11.7396,
      "MMLU-Pro": 6.1264,
      "Terminal-Bench 1.0": 12.8857,
      "BRUMO 2025": 4.9702,
      "IMO 2025": 25.5096,
      "MathArena Apex 2025": 21.0752,
      "SMT 2025": 6.4807,
      "SWE-bench Verified": 12.1682,
      "USAMO 2025": 25.2969,
      "BrowseComp": 12.5179,
      "MathVision": 26.9512,
      "AA Intelligence Index": 39.5245,
      "Arena-Hard Auto": 19.3207,
      "Chatbot Arena Elo": 17.153,
      "HMMT Nov 2025": 42.3362,
      "LiveBench": 3.9651,
      "MRCR v2": 15.9136,
      "SimpleBench": 23.8533,
      "Video-MMU": 0.5991,
      "AA Long Context Reasoning": 59.2931,
      "CMIMC 2025": 5.6156,
      "MMMU-Pro": 56.4201,
      "OSWorld": 18.7666,
      "BigCodeBench": 7.8309
    },
    "n_heldout_pairs": 196,
    "n_scored": 196
  },
  "methodology_notes": "Pipeline: (1) Extracted 83x49 matrix from flat score list, averaging 15 duplicate entries. (2) Filtered to benchmarks with >=10 model scores and models with >=5 benchmark scores (80x35), then imputed missing values via IterativeImputer for SVD analysis and benchmark selection. (3) SVD on z-scored imputed matrix reveals dominant low-rank structure: first component explains ~59% variance, effective rank=6 at 90% threshold (3 at 80%). (4) Greedy forward selection chose 5 benchmarks (Codeforces Rating, HMMT Nov 2025, MMLU-Pro, ARC-AGI-2, Chatbot Arena Elo) spanning coding, math, knowledge, reasoning, and preference categories. (5) For canonical predictions, used an ensemble of low-rank SVD projection and per-benchmark ridge regression on the full unfiltered matrix, with 20%/80% weighting achieving MAE ~15.5 on normalized 0-100 scale. Main error sources: sparse math competition benchmarks (HMMT, IMO, USAMO) with few training examples and high variance. Scale mismatch between Elo ratings and percentage metrics exists but is handled implicitly by per-benchmark ridge fitting. Key methodological choices: iterative imputation for analysis, column-mean imputation for canonical prediction speed, z-score only for SVD, ridge regression as base predictor, ensemble for canonical evaluation."
}