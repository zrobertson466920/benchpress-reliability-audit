model_id,model_name,benchmark_id,benchmark_name,y_pred
claude-opus-4,Claude Opus 4,aime_2024,AIME 2024,90.37
claude-opus-4,Claude Opus 4,aime_2025,AIME 2025,82.8474
claude-opus-4,Claude Opus 4,arc_agi_1,ARC-AGI-1,40.3616
claude-opus-4,Claude Opus 4,arc_agi_2,ARC-AGI-2,3.4983
claude-opus-4,Claude Opus 4,codeforces_rating,Codeforces Rating,2101.9313
claude-opus-4,Claude Opus 4,critpt,CritPt,3.8234
claude-opus-4,Claude Opus 4,frontiermath,FrontierMath,15.7786
claude-opus-4,Claude Opus 4,gpqa_diamond,GPQA Diamond,79.3944
claude-opus-4,Claude Opus 4,hle,HLE (Humanity's Last Exam),27.3508
claude-opus-4,Claude Opus 4,hmmt_2025,HMMT Feb 2025,84.2009
claude-opus-4,Claude Opus 4,ifeval,IFEval,83.3698
claude-opus-4,Claude Opus 4,livecodebench,LiveCodeBench,75.1346
claude-opus-4,Claude Opus 4,math_500,MATH-500,95.6605
claude-opus-4,Claude Opus 4,mmlu,MMLU,85.6361
claude-opus-4,Claude Opus 4,mmmu,MMMU,78.5551
claude-opus-4,Claude Opus 4,simpleqa,SimpleQA,37.2309
claude-opus-4,Claude Opus 4,swe_bench_pro,SWE-bench Pro,38.2001
claude-opus-4,Claude Opus 4,tau_bench_retail,Tau-Bench Retail,83.0318
claude-opus-4,Claude Opus 4,terminal_bench,Terminal-Bench 2.0,28.7374
claude-opus-4.1,Claude Opus 4.1,frontiermath,FrontierMath,19.1475
claude-opus-4.1,Claude Opus 4.1,gpqa_diamond,GPQA Diamond,72.4587
claude-opus-4.1,Claude Opus 4.1,hle,HLE (Humanity's Last Exam),21.6011
claude-opus-4.1,Claude Opus 4.1,humaneval,HumanEval,78.1012
claude-opus-4.1,Claude Opus 4.1,mmlu,MMLU,79.1725
claude-opus-4.1,Claude Opus 4.1,mmlu_pro,MMLU-Pro,74.7587
claude-opus-4.1,Claude Opus 4.1,simpleqa,SimpleQA,39.3841
claude-opus-4.1,Claude Opus 4.1,swe_bench_pro,SWE-bench Pro,39.44
claude-opus-4.1,Claude Opus 4.1,terminal_bench,Terminal-Bench 2.0,34.7419
claude-opus-4.1,Claude Opus 4.1,terminal_bench_1,Terminal-Bench 1.0,30.8558
deepseek-r1-0528,DeepSeek-R1-0528,aime_2024,AIME 2024,81.0803
deepseek-r1-0528,DeepSeek-R1-0528,aime_2025,AIME 2025,81.1515
deepseek-r1-0528,DeepSeek-R1-0528,arc_agi_1,ARC-AGI-1,35.8882
deepseek-r1-0528,DeepSeek-R1-0528,brumo_2025,BRUMO 2025,91.0452
deepseek-r1-0528,DeepSeek-R1-0528,codeforces_rating,Codeforces Rating,2077.3493
deepseek-r1-0528,DeepSeek-R1-0528,gpqa_diamond,GPQA Diamond,70.8469
deepseek-r1-0528,DeepSeek-R1-0528,hle,HLE (Humanity's Last Exam),22.4413
deepseek-r1-0528,DeepSeek-R1-0528,humaneval,HumanEval,80.5282
deepseek-r1-0528,DeepSeek-R1-0528,ifeval,IFEval,88.4081
deepseek-r1-0528,DeepSeek-R1-0528,imo_2025,IMO 2025,31.8868
deepseek-r1-0528,DeepSeek-R1-0528,livecodebench,LiveCodeBench,63.2041
deepseek-r1-0528,DeepSeek-R1-0528,math_500,MATH-500,92.1831
deepseek-r1-0528,DeepSeek-R1-0528,matharena_apex_2025,MathArena Apex 2025,3.8465
deepseek-r1-0528,DeepSeek-R1-0528,mmlu,MMLU,80.4079
deepseek-r1-0528,DeepSeek-R1-0528,mmlu_pro,MMLU-Pro,72.9046
deepseek-r1-0528,DeepSeek-R1-0528,simpleqa,SimpleQA,30.4903
deepseek-r1-0528,DeepSeek-R1-0528,smt_2025,SMT 2025,84.5322
deepseek-r1-0528,DeepSeek-R1-0528,swe_bench_verified,SWE-bench Verified,64.4075
deepseek-r1-0528,DeepSeek-R1-0528,usamo_2025,USAMO 2025,21.5917
doubao-seed-2.0-pro,Doubao Seed 2.0 Pro,aime_2024,AIME 2024,87.778
doubao-seed-2.0-pro,Doubao Seed 2.0 Pro,aime_2025,AIME 2025,113.9447
doubao-seed-2.0-pro,Doubao Seed 2.0 Pro,browsecomp,BrowseComp,64.0475
doubao-seed-2.0-pro,Doubao Seed 2.0 Pro,gpqa_diamond,GPQA Diamond,84.1063
doubao-seed-2.0-pro,Doubao Seed 2.0 Pro,hle,HLE (Humanity's Last Exam),31.754
doubao-seed-2.0-pro,Doubao Seed 2.0 Pro,ifeval,IFEval,92.4817
doubao-seed-2.0-pro,Doubao Seed 2.0 Pro,mathvision,MathVision,82.3655
doubao-seed-2.0-pro,Doubao Seed 2.0 Pro,mmlu,MMLU,90.3787
doubao-seed-2.0-pro,Doubao Seed 2.0 Pro,mmmu,MMMU,79.3327
doubao-seed-2.0-pro,Doubao Seed 2.0 Pro,swe_bench_verified,SWE-bench Verified,71.4376
doubao-seed-2.0-pro,Doubao Seed 2.0 Pro,terminal_bench,Terminal-Bench 2.0,35.4181
gemini-2.5-pro,Gemini 2.5 Pro,aa_intelligence_index,AA Intelligence Index,62.3267
gemini-2.5-pro,Gemini 2.5 Pro,aime_2024,AIME 2024,84.2991
gemini-2.5-pro,Gemini 2.5 Pro,aime_2025,AIME 2025,72.1192
gemini-2.5-pro,Gemini 2.5 Pro,arc_agi_1,ARC-AGI-1,32.8812
gemini-2.5-pro,Gemini 2.5 Pro,arc_agi_2,ARC-AGI-2,5.9951
gemini-2.5-pro,Gemini 2.5 Pro,arena_hard,Arena-Hard Auto,72.7092
gemini-2.5-pro,Gemini 2.5 Pro,brumo_2025,BRUMO 2025,88.9228
gemini-2.5-pro,Gemini 2.5 Pro,chatbot_arena_elo,Chatbot Arena Elo,1410.8386
gemini-2.5-pro,Gemini 2.5 Pro,codeforces_rating,Codeforces Rating,1896.1123
gemini-2.5-pro,Gemini 2.5 Pro,critpt,CritPt,4.1081
gemini-2.5-pro,Gemini 2.5 Pro,frontiermath,FrontierMath,18.4803
gemini-2.5-pro,Gemini 2.5 Pro,gpqa_diamond,GPQA Diamond,75.8458
gemini-2.5-pro,Gemini 2.5 Pro,hle,HLE (Humanity's Last Exam),17.7503
gemini-2.5-pro,Gemini 2.5 Pro,hmmt_2025,HMMT Feb 2025,57.0858
gemini-2.5-pro,Gemini 2.5 Pro,hmmt_nov_2025,HMMT Nov 2025,91.4529
gemini-2.5-pro,Gemini 2.5 Pro,imo_2025,IMO 2025,27.7402
gemini-2.5-pro,Gemini 2.5 Pro,livebench,LiveBench,59.5053
gemini-2.5-pro,Gemini 2.5 Pro,math_500,MATH-500,99.7325
gemini-2.5-pro,Gemini 2.5 Pro,matharena_apex_2025,MathArena Apex 2025,6.8238
gemini-2.5-pro,Gemini 2.5 Pro,mmlu,MMLU,83.431
gemini-2.5-pro,Gemini 2.5 Pro,mmlu_pro,MMLU-Pro,77.5576
gemini-2.5-pro,Gemini 2.5 Pro,mmmu,MMMU,76.7217
gemini-2.5-pro,Gemini 2.5 Pro,mrcr_v2,MRCR v2,68.9918
gemini-2.5-pro,Gemini 2.5 Pro,simplebench,SimpleBench,67.5843
gemini-2.5-pro,Gemini 2.5 Pro,simpleqa,SimpleQA,28.0668
gemini-2.5-pro,Gemini 2.5 Pro,smt_2025,SMT 2025,81.5193
gemini-2.5-pro,Gemini 2.5 Pro,swe_bench_verified,SWE-bench Verified,57.5709
gemini-2.5-pro,Gemini 2.5 Pro,terminal_bench_1,Terminal-Bench 1.0,27.1101
gemini-2.5-pro,Gemini 2.5 Pro,usamo_2025,USAMO 2025,18.3601
gemini-2.5-pro,Gemini 2.5 Pro,video_mmu,Video-MMU,85.6658
gpt-4.1,GPT-4.1,aime_2025,AIME 2025,82.109
gpt-4.1,GPT-4.1,arc_agi_1,ARC-AGI-1,32.6021
gpt-4.1,GPT-4.1,arena_hard,Arena-Hard Auto,59.6106
gpt-4.1,GPT-4.1,codeforces_rating,Codeforces Rating,1955.9569
gpt-4.1,GPT-4.1,hle,HLE (Humanity's Last Exam),24.003
gpt-4.1,GPT-4.1,hmmt_2025,HMMT Feb 2025,67.3368
gpt-4.1,GPT-4.1,humaneval,HumanEval,90.116
gpt-4.1,GPT-4.1,ifeval,IFEval,79.4417
gpt-4.1,GPT-4.1,livecodebench,LiveCodeBench,54.4701
gpt-4.1,GPT-4.1,math_500,MATH-500,86.1327
gpt-4.1,GPT-4.1,mmlu,MMLU,89.8135
gpt-4.1,GPT-4.1,mmmu,MMMU,76.9672
gpt-4.1,GPT-4.1,simpleqa,SimpleQA,29.4343
gpt-4.1,GPT-4.1,swe_bench_verified,SWE-bench Verified,54.8258
gpt-4.1,GPT-4.1,terminal_bench_1,Terminal-Bench 1.0,27.4847
grok-3-beta,Grok 3 Beta,aime_2024,AIME 2024,85.8075
grok-3-beta,Grok 3 Beta,aime_2025,AIME 2025,84.6112
grok-3-beta,Grok 3 Beta,arc_agi_1,ARC-AGI-1,48.03
grok-3-beta,Grok 3 Beta,chatbot_arena_elo,Chatbot Arena Elo,1430.1504
grok-3-beta,Grok 3 Beta,hle,HLE (Humanity's Last Exam),24.4238
grok-3-beta,Grok 3 Beta,humaneval,HumanEval,90.3276
grok-3-beta,Grok 3 Beta,ifeval,IFEval,93.1701
grok-3-beta,Grok 3 Beta,mmlu_pro,MMLU-Pro,82.0623
grok-3-beta,Grok 3 Beta,simpleqa,SimpleQA,39.6226
grok-3-beta,Grok 3 Beta,swe_bench_verified,SWE-bench Verified,61.6859
grok-4,Grok 4,aa_intelligence_index,AA Intelligence Index,61.9266
grok-4,Grok 4,aa_lcr,AA Long Context Reasoning,73.1476
grok-4,Grok 4,aime_2024,AIME 2024,87.7138
grok-4,Grok 4,arc_agi_2,ARC-AGI-2,23.4443
grok-4,Grok 4,brumo_2025,BRUMO 2025,94.059
grok-4,Grok 4,chatbot_arena_elo,Chatbot Arena Elo,1445.634
grok-4,Grok 4,cmimc_2025,CMIMC 2025,83.659
grok-4,Grok 4,codeforces_rating,Codeforces Rating,2208.6641
grok-4,Grok 4,frontiermath,FrontierMath,24.5249
grok-4,Grok 4,gpqa_diamond,GPQA Diamond,81.2478
grok-4,Grok 4,hle,HLE (Humanity's Last Exam),32.4136
grok-4,Grok 4,hmmt_nov_2025,HMMT Nov 2025,89.8735
grok-4,Grok 4,ifeval,IFEval,87.426
grok-4,Grok 4,imo_2025,IMO 2025,33.0311
grok-4,Grok 4,livecodebench,LiveCodeBench,75.4006
grok-4,Grok 4,matharena_apex_2025,MathArena Apex 2025,8.2102
grok-4,Grok 4,mmlu,MMLU,85.4724
grok-4,Grok 4,mmlu_pro,MMLU-Pro,81.1982
grok-4,Grok 4,mmmu,MMMU,81.1428
grok-4,Grok 4,mmmu_pro,MMMU-Pro,79.0666
grok-4,Grok 4,osworld,OSWorld,53.3584
grok-4,Grok 4,simpleqa,SimpleQA,47.8265
grok-4,Grok 4,smt_2025,SMT 2025,87.8753
grok-4,Grok 4,swe_bench_pro,SWE-bench Pro,42.4638
grok-4,Grok 4,swe_bench_verified,SWE-bench Verified,74.9707
grok-4,Grok 4,terminal_bench,Terminal-Bench 2.0,42.2172
grok-4,Grok 4,terminal_bench_1,Terminal-Bench 1.0,29.7667
grok-4,Grok 4,usamo_2025,USAMO 2025,16.1525
kimi-k2,Kimi K2,aime_2024,AIME 2024,75.505
kimi-k2,Kimi K2,aime_2025,AIME 2025,71.9587
kimi-k2,Kimi K2,arena_hard,Arena-Hard Auto,47.2687
kimi-k2,Kimi K2,gpqa_diamond,GPQA Diamond,71.328
kimi-k2,Kimi K2,hle,HLE (Humanity's Last Exam),21.9111
kimi-k2,Kimi K2,hmmt_2025,HMMT Feb 2025,70.4235
kimi-k2,Kimi K2,ifeval,IFEval,78.1699
kimi-k2,Kimi K2,livecodebench,LiveCodeBench,63.4035
kimi-k2,Kimi K2,math_500,MATH-500,94.0129
kimi-k2,Kimi K2,mmlu_pro,MMLU-Pro,83.5386
kimi-k2,Kimi K2,osworld,OSWorld,46.6894
kimi-k2,Kimi K2,simpleqa,SimpleQA,29.8021
kimi-k2,Kimi K2,swe_bench_verified,SWE-bench Verified,65.5506
llama-4-maverick,Llama 4 Maverick,arc_agi_1,ARC-AGI-1,40.7903
llama-4-maverick,Llama 4 Maverick,bigcodebench,BigCodeBench,44.4922
llama-4-maverick,Llama 4 Maverick,chatbot_arena_elo,Chatbot Arena Elo,1421.9913
llama-4-maverick,Llama 4 Maverick,gpqa_diamond,GPQA Diamond,65.6164
llama-4-maverick,Llama 4 Maverick,humaneval,HumanEval,90.2025
llama-4-maverick,Llama 4 Maverick,livecodebench,LiveCodeBench,55.6312
llama-4-maverick,Llama 4 Maverick,math_500,MATH-500,87.0445
llama-4-maverick,Llama 4 Maverick,mmmu,MMMU,77.9263
llama-4-maverick,Llama 4 Maverick,simpleqa,SimpleQA,34.7124
llama-4-maverick,Llama 4 Maverick,swe_bench_verified,SWE-bench Verified,54.5893
llama-4-maverick,Llama 4 Maverick,terminal_bench_1,Terminal-Bench 1.0,27.2924
minimax-m2,MiniMax-M2,aime_2024,AIME 2024,93.0471
minimax-m2,MiniMax-M2,aime_2025,AIME 2025,70.5629
minimax-m2,MiniMax-M2,chatbot_arena_elo,Chatbot Arena Elo,1433.2771
minimax-m2,MiniMax-M2,gpqa_diamond,GPQA Diamond,79.5975
minimax-m2,MiniMax-M2,hle,HLE (Humanity's Last Exam),25.4806
minimax-m2,MiniMax-M2,humaneval,HumanEval,84.0429
minimax-m2,MiniMax-M2,ifeval,IFEval,78.0845
minimax-m2,MiniMax-M2,math_500,MATH-500,99.4227
minimax-m2,MiniMax-M2,mmlu,MMLU,85.908
minimax-m2,MiniMax-M2,mmlu_pro,MMLU-Pro,86.5912
minimax-m2,MiniMax-M2,mmmu,MMMU,78.6491
minimax-m2,MiniMax-M2,simpleqa,SimpleQA,36.0026
minimax-m2,MiniMax-M2,swe_bench_pro,SWE-bench Pro,36.6991
minimax-m2,MiniMax-M2,swe_bench_verified,SWE-bench Verified,77.683
minimax-m2,MiniMax-M2,terminal_bench,Terminal-Bench 2.0,26.7279
o3-mini-high,o3-mini (high),aime_2024,AIME 2024,78.0683
o3-mini-high,o3-mini (high),arc_agi_1,ARC-AGI-1,45.7414
o3-mini-high,o3-mini (high),arc_agi_2,ARC-AGI-2,17.1723
o3-mini-high,o3-mini (high),arena_hard,Arena-Hard Auto,67.161
o3-mini-high,o3-mini (high),chatbot_arena_elo,Chatbot Arena Elo,1434.1183
o3-mini-high,o3-mini (high),frontiermath,FrontierMath,22.4712
o3-mini-high,o3-mini (high),hle,HLE (Humanity's Last Exam),26.855
o3-mini-high,o3-mini (high),hmmt_2025,HMMT Feb 2025,70.6395
o3-mini-high,o3-mini (high),humaneval,HumanEval,87.785
o3-mini-high,o3-mini (high),ifeval,IFEval,87.3402
o3-mini-high,o3-mini (high),livecodebench,LiveCodeBench,67.1868
o3-mini-high,o3-mini (high),math_500,MATH-500,93.4929
o3-mini-high,o3-mini (high),simpleqa,SimpleQA,39.8615
o3-mini-high,o3-mini (high),swe_bench_verified,SWE-bench Verified,64.8352
o3-mini-high,o3-mini (high),usamo_2025,USAMO 2025,24.077
