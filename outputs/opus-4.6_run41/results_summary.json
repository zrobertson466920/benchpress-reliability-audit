{
  "data_discovery": {
    "raw_schema": "Dict with keys: models (list of 83 model objects with id/name/provider/etc), benchmarks (list of 49 benchmark objects with id/name/category/metric/etc), scores (list of 1390 score entries with model_id/benchmark_id/score/reference_url), generated (timestamp string).",
    "extraction_decisions": "Built 83x49 matrix from scores list. 15 duplicate (model_id, benchmark_id) pairs were averaged. Model and benchmark IDs used as canonical identifiers, sorted lexicographically for matrix row/column ordering. All numeric scores used as-is (mixed scales: percentages, Elo ratings, index scores).",
    "n_models_raw": 83,
    "n_benchmarks_raw": 49
  },
  "data": {
    "n_models": 74,
    "n_benchmarks": 35,
    "missing_fraction": 0.5193,
    "preprocessing": "Filtered to benchmarks with >=10 observations (35/49 kept) and models with >=8 observations in kept benchmarks (74/83 kept). Per-benchmark min-max normalization to 0-100 scale for prediction. Z-score normalization (per benchmark, zero imputation for missing) used for SVD rank analysis.",
    "benchmarks_used": [
      "AIME 2024",
      "AIME 2025",
      "ARC-AGI-1",
      "ARC-AGI-2",
      "Arena-Hard Auto",
      "BrowseComp",
      "BRUMO 2025",
      "Chatbot Arena Elo",
      "CMIMC 2025",
      "Codeforces Rating",
      "CritPt",
      "FrontierMath",
      "GPQA Diamond",
      "GSM8K",
      "HLE (Humanity's Last Exam)",
      "HMMT Feb 2025",
      "HMMT Nov 2025",
      "HumanEval",
      "IFEval",
      "LiveBench",
      "LiveCodeBench",
      "MATH-500",
      "MathArena Apex 2025",
      "MMLU",
      "MMLU-Pro",
      "MMMU",
      "MMMU-Pro",
      "OSWorld",
      "SimpleQA",
      "SMT 2025",
      "SWE-bench Pro",
      "SWE-bench Verified",
      "Tau-Bench Retail",
      "Terminal-Bench 2.0",
      "Terminal-Bench 1.0"
    ]
  },
  "rank_analysis": {
    "method": "SVD on z-score-normalized, zero-imputed cleaned matrix (74x35)",
    "effective_rank": 15,
    "variance_explained_by_rank": 0.9057,
    "singular_values": [
      21.9491,
      13.8376,
      8.7353,
      8.1545,
      7.0741,
      6.5417,
      6.2252,
      5.7842,
      5.4428,
      5.0649,
      4.707,
      4.3857,
      4.2976,
      3.9836,
      3.9308
    ],
    "justification": "Using 90% cumulative variance threshold, effective rank is 15. The first 2 components explain 54% of variance (SV ratio drop of 1.59 and 1.58 for first two gaps), suggesting a dominant low-rank signal but with a long tail of smaller components. The matrix is strongly low-rank in the sense that 2-3 components capture the majority of model differentiation, but 52% missingness and mixed metric scales inflate the apparent dimensionality."
  },
  "benchmark_selection": {
    "method": "Greedy forward selection (correlation-based)",
    "selected_benchmarks": [
      "GSM8K",
      "CritPt",
      "Chatbot Arena Elo",
      "BRUMO 2025",
      "GPQA Diamond",
      "SMT 2025",
      "AIME 2025"
    ],
    "n_selected": 7,
    "selection_criterion": "At each step, add the benchmark maximizing average |pairwise correlation| with remaining non-selected benchmarks (pairwise-complete observations, min 5 shared models)."
  },
  "prediction": {
    "method": "Ridge regression (alpha=1.0), top-8 correlated predictors per target benchmark",
    "overall_mae": 14.7223,
    "per_benchmark_mae": {
      "AIME 2024": 2.2454,
      "ARC-AGI-2": 72.0791,
      "BrowseComp": 30.5578,
      "BRUMO 2025": 7.8335,
      "Chatbot Arena Elo": 70.6928,
      "CMIMC 2025": 7.3621,
      "Codeforces Rating": 9.1469,
      "CritPt": 12.6527,
      "FrontierMath": 3.2533,
      "GPQA Diamond": 7.1422,
      "GSM8K": 16.665,
      "HMMT Nov 2025": 14.2406,
      "IFEval": 9.8289,
      "LiveBench": 9.1765,
      "LiveCodeBench": 3.2566,
      "MATH-500": 7.6039,
      "MathArena Apex 2025": 55.139,
      "MMMU": 35.5526,
      "SMT 2025": 3.5505,
      "SWE-bench Pro": 39.3713
    },
    "evaluation_protocol": "LOO per model. For each target benchmark, select 8 most-correlated other benchmarks as predictors. Use only rows where all predictors and target are observed. Refit Ridge per LOO fold. Scale: 0-100 per-benchmark min-max normalized.",
    "n_predictor_benchmarks": 8,
    "achieves_mae_under_5": false
  },
  "canonical_evaluation": {
    "method": "SVD projection (rank-5). For each eval model, mask held-out entries, z-score normalize training data, compute rank-5 SVD, project eval model via revealed entries using regularized least squares, reconstruct all benchmarks.",
    "n_predicted": 196,
    "n_total_pairs": 196,
    "coverage": 1.0,
    "self_check_mae": 17.2709
  },
  "methodology_notes": "Key degrees of freedom: (1) Filtered sparse benchmarks/models rather than imputing the full matrix \u2014 this loses 14 benchmarks and 9 models but improves data density from 34% to 48%. (2) Used z-score normalization for rank analysis to put all benchmarks on comparable scales despite mixed metrics (%, Elo, index). (3) Used 0-100 min-max normalization for prediction to bound the output space. (4) SVD with zero imputation is a known approximation \u2014 iterative imputation or matrix completion methods could improve rank estimates. (5) Correlation-based subset selection is fast but doesn't directly optimize predictive MAE. (6) The high missingness (52%) means LOO evaluation only covers benchmarks/models with sufficient joint observations (20/35 benchmarks evaluated). (7) For canonical evaluation, used SVD rank-5 projection which is fast but may underfit for some benchmarks."
}