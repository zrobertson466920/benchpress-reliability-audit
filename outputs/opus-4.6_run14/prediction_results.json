{
  "method": "ALS_matrix_completion",
  "als_rank": 3,
  "als_regularization": 0.5,
  "overall_mae": 11.776,
  "per_benchmark_mae": {
    "GPQA Diamond": 6.235,
    "AIME 2025": 9.348,
    "MMLU": 3.199,
    "MMLU-Pro": 4.547,
    "SWE-bench Verified": 7.401,
    "MATH-500": 4.765,
    "LiveCodeBench": 5.681,
    "FrontierMath": 7.211,
    "HLE (Humanity's Last Exam)": 3.516,
    "ARC-AGI-2": 8.659,
    "BrowseComp": 4.68,
    "SimpleQA": 9.549,
    "IFEval": 2.077,
    "HumanEval": 2.488,
    "Codeforces Rating": 150.278,
    "OSWorld": 7.984,
    "MMMU": 1.73,
    "MMMU-Pro": 13.57,
    "Arena-Hard Auto": 17.656,
    "Chatbot Arena Elo": 18.571,
    "SWE-bench Pro": 6.736,
    "AIME 2024": 7.842,
    "HMMT Feb 2025": 7.32,
    "Tau-Bench Retail": 6.077,
    "Tau-Bench Telecom": 10.49,
    "Video-MMU": 0.035,
    "AA Long Context Reasoning": 2.079,
    "CritPt": 5.435,
    "SciCode": 3.279,
    "MathVision": 4.306,
    "GSM8K": 7.272,
    "IFBench": 5.254,
    "Terminal-Bench 2.0": 10.115,
    "Terminal-Bench 1.0": 4.338,
    "ARC-AGI-1": 8.197,
    "BRUMO 2025": 3.625,
    "SMT 2025": 5.056,
    "USAMO 2025": 9.819,
    "HMMT Nov 2025": 0.624,
    "CMIMC 2025": 5.33,
    "IMO 2025": 14.516,
    "MathArena Apex 2025": 9.806,
    "LiveBench": 7.15,
    "BigCodeBench": 6.035
  },
  "evaluation_protocol": "LOO on 200 random observed cells",
  "n_predictor_benchmarks": 8,
  "achieves_mae_under_5": false
}