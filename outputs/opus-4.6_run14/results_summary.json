{
  "data_discovery": {
    "raw_schema": "Dict with keys: models (list of 83 dicts with id/name/provider/release_date/params/architecture/is_reasoning/open_weights), benchmarks (list of 49 dicts with id/name/category/metric/num_problems/source_url), scores (list of 1390 dicts with model_id/benchmark_id/score/reference_url), generated (timestamp string).",
    "extraction_decisions": "Built 83x49 matrix from scores list. 15 duplicate (model_id, benchmark_id) pairs found and resolved by averaging. Model IDs and benchmark IDs used as matrix indices. All 83 models and 49 benchmarks retained (no filtering). Scores taken as-is from the score field.",
    "n_models_raw": 83,
    "n_benchmarks_raw": 49
  },
  "data": {
    "n_models": 83,
    "n_benchmarks": 49,
    "missing_fraction": 0.6619,
    "preprocessing": "Z-score normalization per benchmark (subtract mean, divide by std). Missing values imputed with 0 in z-space (equivalent to benchmark mean). No models or benchmarks dropped. This handles mixed scales (% correct, Elo ratings, index scores) for decomposition.",
    "benchmarks_used": [
      "GPQA Diamond",
      "AIME 2025",
      "MMLU",
      "MMLU-Pro",
      "SWE-bench Verified",
      "MATH-500",
      "LiveCodeBench",
      "FrontierMath",
      "HLE (Humanity's Last Exam)",
      "ARC-AGI-2",
      "BrowseComp",
      "SimpleQA",
      "IFEval",
      "HumanEval",
      "Codeforces Rating",
      "OSWorld",
      "MMMU",
      "MMMU-Pro",
      "Arena-Hard Auto",
      "Chatbot Arena Elo",
      "SWE-bench Pro",
      "AIME 2024",
      "HMMT Feb 2025",
      "Tau-Bench Retail",
      "Tau-Bench Telecom",
      "Video-MMU",
      "MRCR v2",
      "AA Intelligence Index",
      "AA Long Context Reasoning",
      "CritPt",
      "SciCode",
      "MathVision",
      "GDP-Val AA",
      "GSM8K",
      "IFBench",
      "Terminal-Bench 2.0",
      "Terminal-Bench 1.0",
      "ARC-AGI-1",
      "BRUMO 2025",
      "SMT 2025",
      "USAMO 2025",
      "HMMT Nov 2025",
      "CMIMC 2025",
      "IMO 2025",
      "AIME 2026",
      "MathArena Apex 2025",
      "LiveBench",
      "SimpleBench",
      "BigCodeBench"
    ]
  },
  "rank_analysis": {
    "method": "SVD on z-scored mean-imputed 83x49 matrix",
    "effective_rank": 18,
    "variance_explained_by_rank": 0.9067,
    "singular_values": [
      22.2338,
      13.7194,
      8.7581,
      8.3317,
      7.2703,
      6.6081,
      6.2402,
      6.1208,
      5.6595,
      5.1396,
      4.8794,
      4.687,
      4.3922,
      4.2809,
      4.0892,
      3.9031,
      3.7461,
      3.5833,
      3.4692,
      3.3073,
      3.2328,
      3.0951,
      2.9596,
      2.8091,
      2.7034,
      2.66,
      2.4676,
      2.3046,
      2.2267,
      2.167,
      2.1128,
      1.9499,
      1.7587,
      1.7092,
      1.6365,
      1.5241,
      1.4033,
      1.2946,
      1.1961,
      1.0643,
      0.9585,
      0.7713,
      0.7094,
      0.6884,
      0.4766,
      0.4066,
      0.3677,
      0.2536,
      0.2295
    ],
    "justification": "Effective rank = 18 by 90% cumulative variance threshold. The first component explains 37.3% and the first 2 explain 51.5%, indicating a strong dominant factor (general capability) but substantial secondary structure. The high effective rank (18) reflects both genuine multi-dimensional variation and noise amplified by 66% missingness with mean imputation."
  },
  "benchmark_selection": {
    "method": "greedy_forward_selection",
    "selected_benchmarks": [
      "MathVision",
      "Terminal-Bench 2.0",
      "Codeforces Rating",
      "HLE (Humanity's Last Exam)",
      "BrowseComp",
      "MMMU",
      "HumanEval",
      "MMLU"
    ],
    "n_selected": 8,
    "selection_criterion": "Minimize in-sample ridge regression MAE from selected benchmarks to all remaining benchmarks, evaluated on observed entries only. At each step, the benchmark minimizing the training MAE when added to the current set is selected."
  },
  "prediction": {
    "method": "ALS (Alternating Least Squares) matrix completion with rank 3. Z-score normalization per benchmark, then ALS factorizes into W(83x3) and H(3x49) using only observed entries. Regularization lambda=0.5, 100 iterations. Predictions converted back to raw scale.",
    "overall_mae": 11.776,
    "per_benchmark_mae": {
      "GPQA Diamond": 6.235,
      "AIME 2025": 9.348,
      "MMLU": 3.199,
      "MMLU-Pro": 4.547,
      "SWE-bench Verified": 7.401,
      "MATH-500": 4.765,
      "LiveCodeBench": 5.681,
      "FrontierMath": 7.211,
      "HLE (Humanity's Last Exam)": 3.516,
      "ARC-AGI-2": 8.659,
      "BrowseComp": 4.68,
      "SimpleQA": 9.549,
      "IFEval": 2.077,
      "HumanEval": 2.488,
      "Codeforces Rating": 150.278,
      "OSWorld": 7.984,
      "MMMU": 1.73,
      "MMMU-Pro": 13.57,
      "Arena-Hard Auto": 17.656,
      "Chatbot Arena Elo": 18.571,
      "SWE-bench Pro": 6.736,
      "AIME 2024": 7.842,
      "HMMT Feb 2025": 7.32,
      "Tau-Bench Retail": 6.077,
      "Tau-Bench Telecom": 10.49,
      "Video-MMU": 0.035,
      "AA Long Context Reasoning": 2.079,
      "CritPt": 5.435,
      "SciCode": 3.279,
      "MathVision": 4.306,
      "GSM8K": 7.272,
      "IFBench": 5.254,
      "Terminal-Bench 2.0": 10.115,
      "Terminal-Bench 1.0": 4.338,
      "ARC-AGI-1": 8.197,
      "BRUMO 2025": 3.625,
      "SMT 2025": 5.056,
      "USAMO 2025": 9.819,
      "HMMT Nov 2025": 0.624,
      "CMIMC 2025": 5.33,
      "IMO 2025": 14.516,
      "MathArena Apex 2025": 9.806,
      "LiveBench": 7.15,
      "BigCodeBench": 6.035
    },
    "evaluation_protocol": "Leave-one-out on 200 randomly sampled observed cells. For each cell, remove it from training, refit ALS, predict. Overall MAE computed in raw score units (mixed scales).",
    "n_predictor_benchmarks": 8,
    "achieves_mae_under_5": false,
    "canonical_overall_mae_normalized": 17.1,
    "canonical_coverage": 1.0
  },
  "methodology_notes": "Key decisions: (1) Kept all 83 models and 49 benchmarks without filtering \u2014 sparse entries handled by ALS which only fits to observed cells. (2) Z-score normalization chosen over min-max because Elo-scale benchmarks (Codeforces ~800-3020, Chatbot Arena ~1338-1502) would dominate otherwise. (3) ALS rank 3 selected by cross-validation (10% held-out observed cells), outperforming ranks 2, 5, 7, 10. (4) The 66% missingness is the dominant challenge; mean imputation for SVD rank analysis inflates effective rank. (5) For canonical eval, ALS is refit per eval model with that model's held-out entries removed. (6) Self-evaluation LOO MAE (11.8 raw) is inflated by Elo-scale benchmarks; canonical normalized MAE is more meaningful for cross-agent comparison."
}