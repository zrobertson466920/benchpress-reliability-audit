{
  "data_discovery": {
    "raw_schema": "JSON with top-level keys: models (list of 83 dicts with id/name/provider/etc), benchmarks (list of 49 dicts with id/name/category/metric), scores (list of 1390 entries with model_id/benchmark_id/score/reference_url), generated (timestamp string)",
    "extraction_decisions": "Mapped scores to (model_id, benchmark_id) pairs. 15 duplicate pairs found (all deepseek-r1-distill variants); resolved by averaging. No null scores. Score range [0, 3020] due to mix of percentage metrics and Elo ratings.",
    "n_models_raw": 83,
    "n_benchmarks_raw": 49
  },
  "data": {
    "n_models": 80,
    "n_benchmarks": 35,
    "missing_fraction": 0.5425,
    "preprocessing": "Filtered benchmarks with >= 10 model coverage (49 -> 35), then filtered models with >= 5 benchmarks (83 -> 80). Applied per-benchmark min-max normalization to 0-100 scale to handle mixed metrics (Elo ratings vs percentages). Imputed remaining missing values with per-benchmark column mean.",
    "benchmarks_used": [
      "AIME 2024",
      "AIME 2025",
      "ARC-AGI-1",
      "ARC-AGI-2",
      "Arena-Hard Auto",
      "BrowseComp",
      "BRUMO 2025",
      "Chatbot Arena Elo",
      "CMIMC 2025",
      "Codeforces Rating",
      "CritPt",
      "FrontierMath",
      "GPQA Diamond",
      "GSM8K",
      "HLE (Humanity's Last Exam)",
      "HMMT Feb 2025",
      "HMMT Nov 2025",
      "HumanEval",
      "IFEval",
      "LiveBench",
      "LiveCodeBench",
      "MATH-500",
      "MathArena Apex 2025",
      "MMLU",
      "MMLU-Pro",
      "MMMU",
      "MMMU-Pro",
      "OSWorld",
      "SimpleQA",
      "SMT 2025",
      "SWE-bench Pro",
      "SWE-bench Verified",
      "Tau-Bench Retail",
      "Terminal-Bench 2.0",
      "Terminal-Bench 1.0"
    ]
  },
  "rank_analysis": {
    "method": "SVD on centered min-max normalized (0-100) imputed matrix",
    "effective_rank": 16,
    "variance_explained_by_rank": 0.90759831755574,
    "singular_values": [
      541.0675617394056,
      377.40494779195495,
      231.01925795040526,
      212.23471751782031,
      179.90960408612693,
      172.26366088579408,
      165.28805578091706,
      162.05114529775454,
      150.34122325479592,
      133.7721463139821,
      126.29561854737918,
      123.79586690941535,
      119.58735098206445,
      107.67832139503719,
      102.78848412832713,
      96.95109035130425,
      93.89018844061991,
      86.73792704362073,
      85.46968670118584,
      82.00115191637825,
      81.10375168069268,
      77.74828008087364,
      72.80707941649358,
      69.16077542061532,
      65.84490209448653,
      60.117507472898275,
      59.117603371179975,
      52.1071579987439,
      49.626415921603304,
      44.325634682261374,
      43.963474425845426,
      38.63820831350081,
      38.31319156755235,
      30.719431682880643,
      23.955150880574514
    ],
    "justification": "Full filtered matrix (80x35, 54% missing, mean-imputed) yields effective rank 16 at 90% variance, but this is inflated by imputation noise. A denser submatrix (29x20, 12.8% missing) gives effective rank 9 at 90% or 6 at 80%, more representative of true structure. First component captures ~50% of variance on the dense submatrix, confirming meaningful low-rank structure.",
    "dense_submatrix_effective_rank": 9,
    "dense_submatrix_effective_rank_80pct": 6
  },
  "benchmark_selection": {
    "method": "greedy_forward_selection",
    "selected_benchmarks": [
      "MMLU-Pro",
      "ARC-AGI-2",
      "HMMT Feb 2025",
      "SWE-bench Verified",
      "AIME 2024"
    ],
    "n_selected": 5,
    "selection_criterion": "Minimize leave-one-out Ridge regression MAE when predicting all non-selected benchmarks from the selected subset on the normalized 0-100 scale"
  },
  "prediction": {
    "method": "Ridge regression (alpha=1.0) from selected benchmark subset to each target benchmark",
    "overall_mae": 8.170071015659966,
    "per_benchmark_mae": {
      "AIME 2025": 13.149509607571801,
      "ARC-AGI-1": 9.79551982855185,
      "Arena-Hard Auto": 13.709308016954463,
      "BrowseComp": 7.168167702178496,
      "BRUMO 2025": 5.768541106220837,
      "Chatbot Arena Elo": 8.646929399907927,
      "CMIMC 2025": 6.723135925159168,
      "Codeforces Rating": 12.610624796975609,
      "CritPt": 6.047215142647583,
      "FrontierMath": 9.043218597932446,
      "GPQA Diamond": 10.688292774194409,
      "GSM8K": 5.503355259697751,
      "HLE (Humanity's Last Exam)": 8.922194834587065,
      "HMMT Nov 2025": 3.3259619107518836,
      "HumanEval": 9.539112511975624,
      "IFEval": 8.023963420927977,
      "LiveBench": 6.2281940775036615,
      "LiveCodeBench": 10.50213615287492,
      "MATH-500": 6.701874580431644,
      "MathArena Apex 2025": 6.766079045583103,
      "MMLU": 7.502659440753149,
      "MMMU": 10.537823980428056,
      "MMMU-Pro": 4.544586160477456,
      "OSWorld": 7.939552306388839,
      "SimpleQA": 11.70835021067314,
      "SMT 2025": 6.023260229534115,
      "SWE-bench Pro": 7.507696432527132,
      "Tau-Bench Retail": 4.300726253091061,
      "Terminal-Bench 2.0": 7.546946035245677,
      "Terminal-Bench 1.0": 8.627194728052128
    },
    "evaluation_protocol": "leave-one-out cross-validation on cleaned/filtered normalized matrix",
    "n_predictor_benchmarks": 5,
    "achieves_mae_under_5": false
  },
  "canonical_evaluation": {
    "method": "Ensemble: KNN (inverse-RMSE weighted, up to 10 neighbors) + Ridge regression with LOO-tuned alpha + per-revealed-benchmark linear regression (correlation-weighted) + column mean baseline",
    "canonical_overall_mae_self_check": 15.556494921367117,
    "n_predictions": 196,
    "n_heldout_pairs": 196,
    "coverage": 1.0,
    "effective_rank_used": 16
  },
  "methodology_notes": "Pipeline: (1) Extracted 83x49 matrix from JSON, averaged 15 duplicate entries. (2) Filtered to benchmarks with >=10 model coverage and models with >=5 benchmarks, yielding 80x35 cleaned matrix. (3) Min-max normalized each benchmark to 0-100 to handle mixed scales (Elo vs percentage). (4) Imputed missing values with column means for SVD analysis. (5) SVD rank analysis on both full imputed matrix (effective rank 16@90%) and dense 29x20 submatrix (effective rank 9@90%, 6@80%); dense estimate is more trustworthy. (6) Greedy forward selection of 5 benchmarks minimizing LOO Ridge MAE. (7) Ridge regression for own evaluation (LOO MAE ~8.2 on normalized scale). (8) Canonical eval: ensemble of (a) KNN with inverse-RMSE distance weights from revealed benchmarks, (b) Ridge regression with LOO-tuned regularization, (c) per-benchmark linear regression from each revealed benchmark to target, weighted by correlation, (d) column mean baseline. All predictions in per-benchmark normalized 0-100 space, converted back to raw for output."
}