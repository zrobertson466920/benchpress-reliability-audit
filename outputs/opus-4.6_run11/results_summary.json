{
  "data_discovery": {
    "raw_schema": "Dict with keys: models (83 dicts: id/name/provider/release_date/params/architecture/is_reasoning/open_weights), benchmarks (49 dicts: id/name/category/metric/num_problems/source_url), scores (1390 entries: model_id/benchmark_id/score/reference_url), generated (timestamp).",
    "extraction_decisions": "Mapped scores to (model_id, benchmark_id) pairs. Averaged 15 exact duplicates (all identical). No null scores. Score scales vary: most 0-100 percentage, Codeforces Rating ~1000-3000, Elo ~1100-1400.",
    "n_models_raw": 83,
    "n_benchmarks_raw": 49
  },
  "data": {
    "n_models": 73,
    "n_benchmarks": 29,
    "missing_fraction": 0.45063769485120453,
    "preprocessing": "Filtered to benchmarks with >= 15 models and models with >= 8 benchmarks. Per-benchmark min-max normalization to 0-100. Iterative SVD imputation at rank 5.",
    "benchmarks_used": [
      "GPQA Diamond",
      "AIME 2025",
      "MMLU",
      "MMLU-Pro",
      "SWE-bench Verified",
      "MATH-500",
      "LiveCodeBench",
      "FrontierMath",
      "HLE (Humanity's Last Exam)",
      "ARC-AGI-2",
      "BrowseComp",
      "SimpleQA",
      "IFEval",
      "HumanEval",
      "Codeforces Rating",
      "OSWorld",
      "MMMU",
      "Arena-Hard Auto",
      "Chatbot Arena Elo",
      "SWE-bench Pro",
      "AIME 2024",
      "HMMT Feb 2025",
      "Terminal-Bench 2.0",
      "Terminal-Bench 1.0",
      "ARC-AGI-1",
      "BRUMO 2025",
      "SMT 2025",
      "CMIMC 2025",
      "MathArena Apex 2025"
    ]
  },
  "rank_analysis": {
    "method": "SVD on min-max normalized (0-100), iterative-SVD-imputed, column-centered matrix",
    "effective_rank": 5,
    "variance_explained_by_rank": 0.9524191083427785,
    "singular_values": [
      739.7860351312129,
      613.1386736108298,
      474.89720070102675,
      440.461166075315,
      376.518043718148,
      97.48854186784563,
      86.95979602521436,
      85.84369808148944,
      78.96295854710198,
      75.2970481690107,
      74.12908434496953,
      67.75524911210238,
      63.352078746954064,
      62.150002420702776,
      53.4945823349829,
      51.45556388602502,
      48.777538296441115,
      44.37907328314495,
      42.34134575720636,
      38.46929486576779,
      36.79240596641068,
      34.15022799939757,
      31.225113924350364,
      30.154250505201407,
      27.743157842011367,
      24.43986558442743,
      23.673020779082428,
      15.23179556898184,
      12.93397427211395
    ],
    "justification": "90% cumulative variance threshold on SVD of 73x29 matrix yields rank 5. First SV dominates (SV1/SV2=1.2), indicating strong general capability factor."
  },
  "benchmark_selection": {
    "method": "Greedy forward selection",
    "selected_benchmarks": [
      "LiveCodeBench",
      "BRUMO 2025",
      "Terminal-Bench 1.0",
      "BrowseComp",
      "SWE-bench Pro",
      "MMMU",
      "Codeforces Rating",
      "Arena-Hard Auto"
    ],
    "n_selected": 8,
    "selection_criterion": "Minimize Ridge regression MAE from selected to remaining benchmarks"
  },
  "prediction": {
    "method": "Ridge regression (rank=5)",
    "overall_mae": 12.451886590639443,
    "per_benchmark_mae": {
      "MMLU": 19.22333105939065,
      "MMLU-Pro": 13.36828592076422,
      "SWE-bench Verified": 90.14272189175095,
      "LiveCodeBench": 34.012961158304094,
      "FrontierMath": 89.32971104133892,
      "ARC-AGI-2": 33.736264614075736,
      "SimpleQA": 99.21923513935901,
      "HumanEval": 17.249744209320344,
      "Codeforces Rating": 17.920911648664088,
      "Arena-Hard Auto": 118.14684585455849,
      "Chatbot Arena Elo": 21.930528039672176,
      "AIME 2024": 17.800490449971377,
      "HMMT Feb 2025": 40.49256099420462,
      "ARC-AGI-1": 84.82110382675128,
      "GPQA Diamond": 39.053683319319546,
      "AIME 2025": 51.30582331601942,
      "HLE (Humanity's Last Exam)": 41.70510198048297,
      "IFEval": 13.43499929968411,
      "MMMU": 133.3159574657914,
      "Terminal-Bench 1.0": 169.9384133771771,
      "MATH-500": 51.23406754466985,
      "BrowseComp": 27.246484257332973,
      "BRUMO 2025": 25.76911425651097,
      "SMT 2025": 22.144798526738775,
      "CMIMC 2025": 25.095369210150754,
      "SWE-bench Pro": 181.28145809001825,
      "Terminal-Bench 2.0": 20.799812477025085,
      "MathArena Apex 2025": 34.29994392465238,
      "OSWorld": 109.55760871025471
    },
    "evaluation_protocol": "Leave-one-model-out with reveal_k=5",
    "n_predictor_benchmarks": 8,
    "achieves_mae_under_5": false
  },
  "canonical_evaluation": {
    "canonical_overall_mae": 13.464614801747782,
    "canonical_coverage": 1.0,
    "n_predictions": 196,
    "n_pairs": 196,
    "rank_used": 5,
    "method_used": "Ridge",
    "svd_mae": 29.127292479155074,
    "ridge_mae": 13.464614801747782,
    "ensemble_mae": 19.39602413768326
  },
  "methodology_notes": "Pipeline: (1) Extract all 83x49 scores from flat JSON, average 15 exact duplicates. (2) Filter to denser submatrix (73x29) using thresholds (bench>=15 models, model>=8 benchmarks). (3) Per-benchmark min-max normalize to 0-100. (4) Iterative SVD imputation (rank-5, converged). (5) SVD for rank analysis on imputed matrix. (6) Greedy forward benchmark selection minimizing Ridge MAE. (7) Low-rank SVD projection + Ridge regression for prediction (both evaluated). (8) Canonical eval uses full 83x49 matrix with iterative SVD imputation (rank-4) and best of SVD/Ridge/Ensemble. Key choices: min-max normalization handles mixed metrics; iterative SVD imputation recovers latent structure; filtering trades coverage for density."
}