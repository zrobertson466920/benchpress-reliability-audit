{
  "data_discovery": {
    "raw_schema": "JSON with keys: models (list of 83 dicts with id, name, provider, release_date, params_total_M, params_active_M, architecture, is_reasoning, open_weights), benchmarks (list of 49 dicts with id, name, category, metric, num_problems, source_url), scores (list of 1390 dicts with model_id, benchmark_id, score, reference_url), generated (timestamp string).",
    "extraction_decisions": "Mapped each score entry to (model_id, benchmark_id) -> score. Found 15 duplicate pairs (e.g. deepseek-r1-distill-qwen-32b on gpqa_diamond); resolved by averaging per canonical spec. All 1390 scores were numeric (no non-numeric). Built 83x49 raw matrix with 1375 unique observed cells. Score range [0.0, 3020.0] spans different metrics: mostly percentage-correct but includes Elo ratings, index scores, and other scales.",
    "n_models_raw": 83,
    "n_benchmarks_raw": 49
  },
  "data": {
    "n_models": 80,
    "n_benchmarks": 35,
    "missing_fraction": 0.5425,
    "preprocessing": "Two-stage: (1) Filtered matrix: dropped benchmarks with <10 observations and models with <5 observations (iterative, converged to 80 models x 35 benchmarks). (2) Per-benchmark min-max normalization to 0-100 scale to handle mixed metrics. (3) Iterative rank-5 SVD imputation for missing values (column-mean init, converged). For canonical evaluation: used full 83x49 matrix (all benchmarks/models) with same normalization and imputation, since predictions are needed for benchmarks outside the filtered set.",
    "benchmarks_used": [
      "AIME 2024",
      "AIME 2025",
      "ARC-AGI-1",
      "ARC-AGI-2",
      "Arena-Hard Auto",
      "BrowseComp",
      "BRUMO 2025",
      "Chatbot Arena Elo",
      "CMIMC 2025",
      "Codeforces Rating",
      "CritPt",
      "FrontierMath",
      "GPQA Diamond",
      "GSM8K",
      "HLE (Humanity's Last Exam)",
      "HMMT Feb 2025",
      "HMMT Nov 2025",
      "HumanEval",
      "IFEval",
      "LiveBench",
      "LiveCodeBench",
      "MATH-500",
      "MathArena Apex 2025",
      "MMLU",
      "MMLU-Pro",
      "MMMU",
      "MMMU-Pro",
      "OSWorld",
      "SimpleQA",
      "SMT 2025",
      "SWE-bench Pro",
      "SWE-bench Verified",
      "Tau-Bench Retail",
      "Terminal-Bench 2.0",
      "Terminal-Bench 1.0"
    ]
  },
  "rank_analysis": {
    "method": "SVD on min-max normalized (0-100) + iteratively imputed matrix (80 models x 35 benchmarks)",
    "effective_rank": 2,
    "variance_explained_by_rank": 0.9121198563110372,
    "singular_values": [
      3239.8790841852256,
      769.5512014632573,
      661.7080993436527,
      529.3343239784036,
      501.2308801688576,
      116.58993798560574,
      114.3453267662913,
      92.13828003842977,
      82.08343381281708,
      80.69020698731121,
      78.1171770398085,
      73.53167274278158,
      68.51189049178657,
      66.2334508658589,
      63.16349172450704
    ],
    "justification": "Using 90% cumulative variance threshold: effective rank = 2. The first singular value alone captures 86.3% of variance, indicating a very strong dominant factor (likely overall model capability). First 2 components: 91.2%, first 4: 97.1%, first 5: 99.2%. The matrix is strongly low-rank. The effective rank of 2 means the 80x35 benchmark matrix can be well-approximated by a rank-2 matrix, though rank 5 gives better predictions in the canonical evaluation likely due to capturing category-specific factors."
  },
  "benchmark_selection": {
    "method": "Greedy forward selection minimizing LOO ridge regression MAE on observed data",
    "selected_benchmarks": [
      "BrowseComp",
      "Terminal-Bench 2.0",
      "MMMU",
      "AIME 2025",
      "BRUMO 2025",
      "MMLU",
      "HMMT Nov 2025",
      "SWE-bench Pro"
    ],
    "n_selected": 8,
    "selection_criterion": "At each step, add the benchmark that minimizes average leave-one-out MAE (ridge regression, alpha=1.0) when predicting all remaining benchmarks from the selected set, using only rows with complete observations for both predictor and target benchmarks."
  },
  "prediction": {
    "method": "Ensemble: 90% latent projection (rank-5 SVD, ridge alpha=0.01) + 10% per-target ridge regression. Latent projection: fit global iterative SVD on all observed data, project eval model into latent space using only revealed benchmarks, predict held-out benchmarks via V_k @ z + col_means. Per-target ridge: for each (model, benchmark) pair, train ridge on other models using revealed benchmarks as features.",
    "overall_mae": 8.686785229046661,
    "per_benchmark_mae": {
      "AIME 2024": 1.0760530429815347,
      "ARC-AGI-1": 14.238547254597055,
      "ARC-AGI-2": 22.1187383149253,
      "Arena-Hard Auto": null,
      "Chatbot Arena Elo": 21.329759834541193,
      "CMIMC 2025": 2.316688697218615,
      "Codeforces Rating": 12.095944656336192,
      "CritPt": 19.41902223333663,
      "FrontierMath": 6.048388086633966,
      "GPQA Diamond": 2.579338880716338,
      "GSM8K": null,
      "HLE (Humanity's Last Exam)": 5.168848200145216,
      "HMMT Feb 2025": 6.032137698553352,
      "HumanEval": 2.5914999026416723,
      "IFEval": 3.299630999669489,
      "LiveBench": 2.3707900921418363,
      "LiveCodeBench": 7.875921055421122,
      "MATH-500": 1.7875925333126723,
      "MathArena Apex 2025": 8.180286682916403,
      "MMLU-Pro": 2.4294728483229306,
      "MMMU-Pro": 7.079621129676891,
      "OSWorld": 24.309232578747835,
      "SimpleQA": 12.983497977363346,
      "SMT 2025": 1.6366636624807227,
      "SWE-bench Verified": 5.174997442265939,
      "Tau-Bench Retail": 17.729089863397714,
      "Terminal-Bench 1.0": null
    },
    "evaluation_protocol": "Own evaluation: LOO CV per target benchmark on normalized 0-100 scale (ridge from 8 selected benchmarks). Canonical evaluation: reveal-k-per-model protocol with k=5, using blended latent projection + per-target ridge.",
    "n_predictor_benchmarks": 8,
    "achieves_mae_under_5": false
  },
  "canonical_evaluation": {
    "overall_mae": 12.752059129414004,
    "per_benchmark_mae": {
      "AIME 2024": 7.134253592551943,
      "AIME 2025": 7.481624727454516,
      "ARC-AGI-1": 10.90405383549248,
      "ARC-AGI-2": 8.834777355207397,
      "Codeforces Rating": 12.433185745949483,
      "CritPt": 8.576755173205273,
      "FrontierMath": 20.51891647280386,
      "GPQA Diamond": 5.078110689831794,
      "HLE (Humanity's Last Exam)": 18.752635724155542,
      "HMMT Feb 2025": 26.87178479634906,
      "IFEval": 6.859540955942726,
      "LiveCodeBench": 9.908195921254416,
      "MATH-500": 4.152755914457488,
      "MMLU": 6.972106861902526,
      "MMMU": 14.819122777519548,
      "SimpleQA": 17.480274087612738,
      "SWE-bench Pro": 11.528517476537942,
      "Tau-Bench Retail": 5.754323359686097,
      "Terminal-Bench 2.0": 15.715730827317836,
      "HumanEval": 12.213171416295964,
      "MMLU-Pro": 6.769952427511096,
      "Terminal-Bench 1.0": 7.428047106417682,
      "BRUMO 2025": 6.694007680547216,
      "IMO 2025": 30.636345517280322,
      "MathArena Apex 2025": 9.628768424186175,
      "SMT 2025": 10.247531011499852,
      "SWE-bench Verified": 9.496201278164365,
      "USAMO 2025": 28.622539856790354,
      "BrowseComp": 40.777477173131636,
      "MathVision": 12.284339827319172,
      "AA Intelligence Index": 21.05358501813292,
      "Arena-Hard Auto": 11.199217811182473,
      "Chatbot Arena Elo": 17.752473376536106,
      "HMMT Nov 2025": 7.688845644431343,
      "LiveBench": 9.569097665483895,
      "MRCR v2": 26.62332460582057,
      "SimpleBench": 25.9452587710934,
      "Video-MMU": 8.87050432522642,
      "AA Long Context Reasoning": 58.86714499827812,
      "CMIMC 2025": 0.17190388875506812,
      "MMMU-Pro": 66.67174177654947,
      "OSWorld": 8.617620263649876,
      "BigCodeBench": 21.95698533907901
    },
    "coverage": 1.0,
    "n_predictions": 196,
    "n_held_out_pairs": 196,
    "method_detail": "Blended ensemble: 0.90 * latent_projection(rank=5, ridge_alpha=0.01) + 0.10 * per_target_ridge. Global iterative SVD computed once on all data. For each eval model, revealed benchmark scores are used to solve for latent factors via ridge regression on the benchmark loading matrix V_k, then predictions are generated as V_k @ z + column_means. Per-target ridge provides a complementary signal by directly regressing each target benchmark from the 5 revealed benchmarks using other models as training data."
  },
  "methodology_notes": "Pipeline: (1) Extracted 83x49 matrix from JSON (averaged 15 duplicates). (2) Filtered to 80x35 for rank analysis (>=10 obs/bench, >=5 obs/model). (3) Min-max normalized per benchmark to 0-100 (handles Elo ratings, index scores vs percentages). (4) Iterative rank-5 SVD imputation. (5) SVD for rank analysis: effective rank=2 at 90% variance. (6) Greedy forward selection of 8 benchmarks for subset prediction. (7) For canonical eval: used full 83x49 matrix, latent projection approach projects each eval model into rank-5 space using only its 5 revealed benchmarks, then predicts all held-out benchmarks. Blended with per-target ridge (10% weight). Canonical MAE=12.75. Key challenge: high missingness (66%), mixed metric scales, and some benchmarks with very few observations (e.g., AIME 2026: 2 obs, SciCode: 3 obs) that are hard to predict via low-rank structure. Worst predictions are for benchmarks with atypical distributions or very sparse data (MMMU-Pro, AA Long Context Reasoning, BrowseComp)."
}