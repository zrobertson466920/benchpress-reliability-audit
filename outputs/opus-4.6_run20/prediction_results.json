{
  "method": "Ridge regression (selected -> target)",
  "overall_mae": 7.674,
  "per_benchmark_mae": {
    "GPQA Diamond": 1.749,
    "AIME 2025": 4.775,
    "MMLU": 1.873,
    "MMLU-Pro": 7.443,
    "SWE-bench Verified": 6.034,
    "MATH-500": null,
    "LiveCodeBench": 17.964,
    "FrontierMath": 9.61,
    "HLE (Humanity's Last Exam)": 24.816,
    "ARC-AGI-2": null,
    "BrowseComp": 1.038,
    "SimpleQA": 12.533,
    "IFEval": 4.196,
    "HumanEval": null,
    "Codeforces Rating": 17.859,
    "OSWorld": null,
    "MMMU": null,
    "MMMU-Pro": null,
    "Arena-Hard Auto": null,
    "Chatbot Arena Elo": 9.545,
    "SWE-bench Pro": null,
    "AIME 2024": 2.391,
    "HMMT Feb 2025": 1.88,
    "Tau-Bench Retail": null,
    "CritPt": null,
    "GSM8K": null,
    "Terminal-Bench 2.0": 3.924,
    "Terminal-Bench 1.0": null,
    "ARC-AGI-1": null,
    "BRUMO 2025": null,
    "SMT 2025": null,
    "HMMT Nov 2025": null,
    "CMIMC 2025": null,
    "MathArena Apex 2025": null,
    "LiveBench": 2.41
  },
  "evaluation_protocol": "leave-one-out by model, on normalized 0-100 scale",
  "n_predictor_benchmarks": 7,
  "predictor_benchmarks": [
    "BrowseComp",
    "LiveBench",
    "GPQA Diamond",
    "AIME 2025",
    "MMLU",
    "SWE-bench Verified",
    "Terminal-Bench 2.0"
  ],
  "alpha": 10.0,
  "achieves_mae_under_5": false
}