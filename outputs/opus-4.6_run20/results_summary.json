{
  "data_discovery": {
    "raw_schema": "JSON with top-level keys: models (list of 83 dicts with id/name/provider/etc), benchmarks (list of 49 dicts with id/name/category/metric), scores (list of 1390 dicts with model_id/benchmark_id/score/reference_url), generated (timestamp string).",
    "extraction_decisions": "Duplicates (15 pairs) resolved by simple average per canonical_evaluation.md. All model_ids and benchmark_ids in scores matched the models/benchmarks lists (no orphans). Scores extracted as-is (numeric). No joins needed \u2014 flat relational structure.",
    "n_models_raw": 83,
    "n_benchmarks_raw": 49
  },
  "data": {
    "n_models": 83,
    "n_benchmarks": 35,
    "missing_fraction": 0.556,
    "preprocessing": "Filtered to benchmarks with >= 12 model observations (35/49 kept), models with >= 3 observed benchmarks in filtered set (83/83 kept). Min-max normalized per benchmark to 0-100 scale. Missing values imputed via iterative SVD (soft-impute, rank=5, tol=1e-5).",
    "benchmarks_used": [
      "GPQA Diamond",
      "AIME 2025",
      "MMLU",
      "MMLU-Pro",
      "SWE-bench Verified",
      "MATH-500",
      "LiveCodeBench",
      "FrontierMath",
      "HLE (Humanity's Last Exam)",
      "ARC-AGI-2",
      "BrowseComp",
      "SimpleQA",
      "IFEval",
      "HumanEval",
      "Codeforces Rating",
      "OSWorld",
      "MMMU",
      "MMMU-Pro",
      "Arena-Hard Auto",
      "Chatbot Arena Elo",
      "SWE-bench Pro",
      "AIME 2024",
      "HMMT Feb 2025",
      "Tau-Bench Retail",
      "CritPt",
      "GSM8K",
      "Terminal-Bench 2.0",
      "Terminal-Bench 1.0",
      "ARC-AGI-1",
      "BRUMO 2025",
      "SMT 2025",
      "HMMT Nov 2025",
      "CMIMC 2025",
      "MathArena Apex 2025",
      "LiveBench"
    ]
  },
  "rank_analysis": {
    "method": "SVD on min-max normalized (0-100), soft-imputed (rank-5) filtered matrix",
    "effective_rank": 3,
    "variance_explained_by_rank": 0.9285,
    "singular_values": [
      3249.1339,
      1032.9411,
      892.0335,
      738.6242,
      570.912,
      110.0759,
      98.764,
      92.6925,
      79.7502,
      75.6427,
      73.9681,
      66.1213,
      63.5449,
      59.331,
      57.2459
    ],
    "justification": "Using 90% cumulative variance threshold on SVD of the imputed normalized matrix, the effective rank is 3. The first singular value dominates (78.9% variance), indicating a strong general-ability factor. Subsequent components capture progressively smaller specialization effects."
  },
  "benchmark_selection": {
    "method": "greedy_forward_selection",
    "selected_benchmarks": [
      "BrowseComp",
      "LiveBench",
      "GPQA Diamond",
      "AIME 2025",
      "MMLU",
      "SWE-bench Verified",
      "Terminal-Bench 2.0"
    ],
    "n_selected": 7,
    "selection_criterion": "Minimize LOO Ridge regression MAE (alpha=10) on remaining benchmarks, greedy forward, normalized 0-100 scale"
  },
  "prediction": {
    "method": "Ridge regression from selected benchmarks to each target (alpha=10.0)",
    "overall_mae": 7.674,
    "per_benchmark_mae": {
      "GPQA Diamond": 1.749,
      "AIME 2025": 4.775,
      "MMLU": 1.873,
      "MMLU-Pro": 7.443,
      "SWE-bench Verified": 6.034,
      "MATH-500": null,
      "LiveCodeBench": 17.964,
      "FrontierMath": 9.61,
      "HLE (Humanity's Last Exam)": 24.816,
      "ARC-AGI-2": null,
      "BrowseComp": 1.038,
      "SimpleQA": 12.533,
      "IFEval": 4.196,
      "HumanEval": null,
      "Codeforces Rating": 17.859,
      "OSWorld": null,
      "MMMU": null,
      "MMMU-Pro": null,
      "Arena-Hard Auto": null,
      "Chatbot Arena Elo": 9.545,
      "SWE-bench Pro": null,
      "AIME 2024": 2.391,
      "HMMT Feb 2025": 1.88,
      "Tau-Bench Retail": null,
      "CritPt": null,
      "GSM8K": null,
      "Terminal-Bench 2.0": 3.924,
      "Terminal-Bench 1.0": null,
      "ARC-AGI-1": null,
      "BRUMO 2025": null,
      "SMT 2025": null,
      "HMMT Nov 2025": null,
      "CMIMC 2025": null,
      "MathArena Apex 2025": null,
      "LiveBench": 2.41
    },
    "evaluation_protocol": "Leave-one-out by model on normalized 0-100 scale, using only rows where target and all selected benchmarks are observed",
    "n_predictor_benchmarks": 7,
    "achieves_mae_under_5": false
  },
  "canonical_evaluation": {
    "canonical_overall_mae": 17.085,
    "canonical_coverage": 1.0,
    "n_predictions": 196,
    "n_heldout_pairs": 196,
    "method": "Hybrid: Ridge regression from 5 revealed benchmarks (alpha=10) blended 60/40 with iterative SVD completion (rank-5). Ridge used where >=5 training models available; pure SVD as fallback. All on canonically normalized 0-100 scale, converted back to raw."
  },
  "methodology_notes": "Pipeline: (1) Extract flat relational scores with duplicate averaging. (2) Filter sparse benchmarks (<12 obs) and sparse models (<3 obs). (3) Min-max normalize to 0-100 per benchmark. (4) Soft-impute (iterative rank-5 SVD) for missing values. (5) Full SVD for rank analysis. (6) Greedy forward benchmark selection minimizing LOO Ridge MAE. (7) Ridge regression for own evaluation. (8) Iterative SVD completion for canonical predictions. Scale mixing (Elo ratings vs percentages) handled by per-benchmark normalization. The dominant first singular value suggests a strong general-ability factor. Filtering was chosen over working with the full sparse matrix to improve decomposition stability."
}