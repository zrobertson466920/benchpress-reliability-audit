{
  "data_discovery": {
    "raw_schema": "Dict with keys: models (list of 83 dicts with id/name/provider/etc), benchmarks (list of 49 dicts with id/name/category/metric/etc), scores (list of 1390 dicts with model_id/benchmark_id/score/reference_url), generated (timestamp). Models identified by string 'id', benchmarks by string 'id', scores link them. Some (model,benchmark) pairs appear more than once (15 duplicates).",
    "extraction_decisions": "1) Duplicates resolved by simple average (per canonical spec). 2) Model/benchmark IDs used as canonical identifiers; names from model/benchmark metadata. 3) All score entries used (no filtering at extraction). 4) No nesting to resolve beyond top-level lists.",
    "n_models_raw": 83,
    "n_benchmarks_raw": 49
  },
  "data": {
    "n_models": 80,
    "n_benchmarks": 35,
    "missing_fraction": 0.5425,
    "preprocessing": "Filtered to benchmarks with >= 10 model observations and models with >= 5 observations in filtered benchmarks. No imputation in cleaned_matrix.csv (NaN preserved). For rank analysis: per-benchmark min-max normalization to [0,1] then column-mean imputation. Elo-scale benchmarks (Codeforces Rating, Chatbot Arena Elo) retained but normalized like others.",
    "benchmarks_used": [
      "AIME 2024",
      "AIME 2025",
      "ARC-AGI-1",
      "ARC-AGI-2",
      "Arena-Hard Auto",
      "BrowseComp",
      "BRUMO 2025",
      "Chatbot Arena Elo",
      "CMIMC 2025",
      "Codeforces Rating",
      "CritPt",
      "FrontierMath",
      "GPQA Diamond",
      "GSM8K",
      "HLE (Humanity's Last Exam)",
      "HMMT Feb 2025",
      "HMMT Nov 2025",
      "HumanEval",
      "IFEval",
      "LiveBench",
      "LiveCodeBench",
      "MATH-500",
      "MathArena Apex 2025",
      "MMLU",
      "MMLU-Pro",
      "MMMU",
      "MMMU-Pro",
      "OSWorld",
      "SimpleQA",
      "SMT 2025",
      "SWE-bench Pro",
      "SWE-bench Verified",
      "Tau-Bench Retail",
      "Terminal-Bench 2.0",
      "Terminal-Bench 1.0"
    ]
  },
  "rank_analysis": {
    "method": "SVD on min-max normalized ([0,1] per benchmark), column-mean imputed cleaned matrix",
    "effective_rank": 1,
    "variance_explained_by_rank": 0.9452,
    "singular_values": [
      33.3924,
      3.9139,
      3.2029,
      2.1715,
      2.1209,
      1.7535,
      1.6939,
      1.6281,
      1.5733,
      1.4878,
      1.3302,
      1.2624,
      1.236,
      1.1722,
      1.052
    ],
    "justification": "Using 90% cumulative variance threshold on SVD of the normalized imputed matrix (80x35), effective rank = 1. The first component alone explains 94.5% of variance, indicating a dominant general-capability factor. Rank 1 captures 94.5% of total variance."
  },
  "benchmark_selection": {
    "method": "Greedy forward selection",
    "selected_benchmarks": [
      "MMLU-Pro",
      "ARC-AGI-2",
      "AIME 2024",
      "SWE-bench Verified",
      "HMMT Feb 2025",
      "AIME 2025",
      "Arena-Hard Auto"
    ],
    "n_selected": 7,
    "selection_criterion": "Minimize Ridge regression MAE from selected to remaining benchmarks on min-max normalized mean-imputed matrix"
  },
  "prediction": {
    "method": "Ridge regression (alpha=1.0) from selected benchmark subset to remaining benchmarks, trained per-target-benchmark",
    "overall_mae": 20.5094,
    "per_benchmark_mae": {
      "ARC-AGI-1": 17.6261,
      "BRUMO 2025": 4.7572,
      "BrowseComp": 9.8915,
      "CMIMC 2025": 9.1722,
      "Chatbot Arena Elo": 28.2221,
      "Codeforces Rating": 250.787,
      "CritPt": 3.5,
      "FrontierMath": 8.2375,
      "GPQA Diamond": 5.6145,
      "GSM8K": 5.7403,
      "HLE (Humanity's Last Exam)": 6.5122,
      "HMMT Nov 2025": 4.0189,
      "HumanEval": 5.0452,
      "IFEval": 4.0799,
      "LiveBench": 8.2925,
      "LiveCodeBench": 6.685,
      "MATH-500": 3.6454,
      "MMLU": 3.1501,
      "MMMU": 4.3206,
      "MMMU-Pro": 5.8204,
      "MathArena Apex 2025": 6.5552,
      "OSWorld": 9.8641,
      "SMT 2025": 5.5542,
      "SWE-bench Pro": 6.5092,
      "SimpleQA": 9.5061,
      "Tau-Bench Retail": 3.9233,
      "Terminal-Bench 1.0": 11.2145,
      "Terminal-Bench 2.0": 10.8976
    },
    "evaluation_protocol": "Leave-one-model-out CV on observed entries only (raw score scale)",
    "n_predictor_benchmarks": 7,
    "achieves_mae_under_5": false
  },
  "canonical_evaluation": {
    "canonical_overall_mae": 15.305825206979529,
    "n_predictions": 196,
    "n_heldout_pairs": 196,
    "coverage": 1.0,
    "per_benchmark_mae": {
      "AA Intelligence Index": 52.43522727272727,
      "AA Long Context Reasoning": 37.6091954022989,
      "AIME 2024": 8.23164912280702,
      "AIME 2025": 9.687544169611314,
      "ARC-AGI-1": 21.103069908814586,
      "ARC-AGI-2": 13.192315175097278,
      "Arena-Hard Auto": 19.39284557235421,
      "BigCodeBench": 2.498148148148175,
      "BrowseComp": 29.547971360381858,
      "BRUMO 2025": 7.768217734855149,
      "Chatbot Arena Elo": 16.611463414634162,
      "CMIMC 2025": 11.863802559414978,
      "Codeforces Rating": 11.325999099099098,
      "CritPt": 26.35056497175141,
      "FrontierMath": 23.38702549575071,
      "GPQA Diamond": 5.685938752605418,
      "HLE (Humanity's Last Exam)": 20.783597262952103,
      "HMMT Feb 2025": 26.383640718562873,
      "HMMT Nov 2025": 46.15329218106994,
      "HumanEval": 10.094339118825092,
      "IFEval": 6.396370235934672,
      "IMO 2025": 31.837715282319596,
      "LiveBench": 1.149511400651484,
      "LiveCodeBench": 12.144240218380343,
      "MATH-500": 5.300264657980463,
      "MathArena Apex 2025": 6.646766169154229,
      "MathVision": 26.95121951219508,
      "MMLU": 8.893541666666662,
      "MMLU-Pro": 9.892824307166942,
      "MMMU": 17.19752540874947,
      "MMMU-Pro": 66.35311355311357,
      "MRCR v2": 58.66678832116788,
      "OSWorld": 2.3974272930648777,
      "SimpleBench": 23.55555555555557,
      "SimpleQA": 20.81015588825436,
      "SMT 2025": 4.896739130434781,
      "SWE-bench Pro": 11.69127565982405,
      "SWE-bench Verified": 11.139035916824197,
      "Tau-Bench Retail": 22.45975103734434,
      "Terminal-Bench 2.0": 14.934678899082565,
      "Terminal-Bench 1.0": 14.967196467991169,
      "USAMO 2025": 32.03326646606486,
      "Video-MMU": 15.442986425339335
    }
  },
  "methodology_notes": "Pipeline: (1) Extract full 83x49 matrix from JSON, average 15 duplicate entries. (2) Filter to benchmarks with >=10 models and models with >=5 benchmarks to reduce extreme sparsity. (3) Min-max normalize per benchmark to [0,1], then column-mean impute for SVD/selection. (4) SVD for rank analysis. (5) Greedy forward selection of 7 benchmarks minimizing Ridge regression MAE. (6) Ridge regression for prediction. (7) Canonical eval uses Ridge from 5 revealed benchmarks per eval model to predict held-out entries, trained on all other models. Key degrees of freedom: filtering thresholds (10 models/5 benchmarks), min-max normalization choice, mean imputation, Ridge alpha=1.0, greedy forward selection, 7 benchmarks selected. Canonical eval v2: Ridge alpha=10, blend with benchmark mean (weight = min(n_train/20, 1)), clip predictions to [0,100] normalized range."
}