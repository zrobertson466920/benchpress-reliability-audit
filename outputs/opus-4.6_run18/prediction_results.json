{
  "method": "Ridge regression from selected benchmark subset",
  "overall_mae": 20.509424160131548,
  "per_benchmark_mae": {
    "ARC-AGI-1": 17.626066712571344,
    "BRUMO 2025": 4.7572257122003,
    "BrowseComp": 9.89154008717301,
    "CMIMC 2025": 9.17221665079248,
    "Chatbot Arena Elo": 28.222074653784887,
    "Codeforces Rating": 250.78695974697266,
    "CritPt": 3.4999806852457556,
    "FrontierMath": 8.237536211215435,
    "GPQA Diamond": 5.614534990074867,
    "GSM8K": 5.740338557280781,
    "HLE (Humanity's Last Exam)": 6.5121604044077985,
    "HMMT Nov 2025": 4.018929431614317,
    "HumanEval": 5.045198315284897,
    "IFEval": 4.079868326553003,
    "LiveBench": 8.292516062008863,
    "LiveCodeBench": 6.685043873577533,
    "MATH-500": 3.6453806680194365,
    "MMLU": 3.1501072090504993,
    "MMMU": 4.320609970160119,
    "MMMU-Pro": 5.820357248783311,
    "MathArena Apex 2025": 6.55522055448947,
    "OSWorld": 9.864078597290094,
    "SMT 2025": 5.5541652486689115,
    "SWE-bench Pro": 6.509202604048767,
    "SimpleQA": 9.50612346447365,
    "Tau-Bench Retail": 3.9232784554006757,
    "Terminal-Bench 1.0": 11.214502446539042,
    "Terminal-Bench 2.0": 10.897621042098669
  },
  "evaluation_protocol": "Leave-one-model-out CV on observed entries, Ridge alpha=1.0",
  "n_predictor_benchmarks": 7,
  "n_predictions": 907,
  "achieves_mae_under_5": false
}