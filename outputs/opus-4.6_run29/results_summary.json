{
  "data_discovery": {
    "raw_schema": "JSON: models(83), benchmarks(49), scores(1390), generated. Models have id/name/provider/params/architecture/is_reasoning/open_weights. Benchmarks have id/name/category/metric/num_problems/source_url. Scores have model_id/benchmark_id/score/reference_url.",
    "extraction_decisions": "15 duplicate (model,benchmark) pairs found (DeepSeek distill models, all identical scores); resolved by averaging. Built 83x49 matrix indexed by sorted IDs.",
    "n_models_raw": 83,
    "n_benchmarks_raw": 49
  },
  "data": {
    "n_models": 80,
    "n_benchmarks": 35,
    "missing_fraction": 0.5425,
    "preprocessing": "Iteratively filtered to benchmarks with >=10 models and models with >=5 benchmarks (83x49 -> 80x35). Per-benchmark min-max to [0,100]. Column-mean imputation for SVD only.",
    "benchmarks_used": [
      "AIME 2024",
      "AIME 2025",
      "ARC-AGI-1",
      "ARC-AGI-2",
      "Arena-Hard Auto",
      "BrowseComp",
      "BRUMO 2025",
      "Chatbot Arena Elo",
      "CMIMC 2025",
      "Codeforces Rating",
      "CritPt",
      "FrontierMath",
      "GPQA Diamond",
      "GSM8K",
      "HLE (Humanity's Last Exam)",
      "HMMT Feb 2025",
      "HMMT Nov 2025",
      "HumanEval",
      "IFEval",
      "LiveBench",
      "LiveCodeBench",
      "MATH-500",
      "MathArena Apex 2025",
      "MMLU",
      "MMLU-Pro",
      "MMMU",
      "MMMU-Pro",
      "OSWorld",
      "SimpleQA",
      "SMT 2025",
      "SWE-bench Pro",
      "SWE-bench Verified",
      "Tau-Bench Retail",
      "Terminal-Bench 2.0",
      "Terminal-Bench 1.0"
    ]
  },
  "rank_analysis": {
    "method": "SVD on mean-imputed min-max-normalized [0,100] column-centered matrix",
    "effective_rank": 2,
    "variance_explained_by_rank": 0.5151,
    "singular_values": [
      541.0676,
      377.4049,
      231.0193,
      212.2347,
      179.9096,
      172.2637,
      165.2881,
      162.0511,
      150.3412,
      133.7721,
      126.2956,
      123.7959,
      119.5874,
      107.6783,
      102.7885
    ],
    "justification": "Elbow criterion: largest SV ratio gap at SV2/SV3=1.63. Top 2 components explain 51.5% variance. 90%-threshold rank=16 is inflated by mean-imputation noise (54% missing). First component alone explains 34.6%, suggesting a dominant general-capability factor."
  },
  "benchmark_selection": {
    "method": "Greedy forward selection",
    "selected_benchmarks": [
      "GPQA Diamond",
      "LiveCodeBench",
      "HumanEval",
      "AIME 2025",
      "SimpleQA"
    ],
    "n_selected": 5,
    "selection_criterion": "0.7*avg|correlation_with_remaining| + 0.3*coverage_fraction on imputed normalized matrix"
  },
  "prediction": {
    "method": "Ridge regression (alpha=10) from selected benchmarks to each target",
    "overall_mae": 14.488,
    "per_benchmark_mae": {
      "AIME 2024": 7.3649,
      "ARC-AGI-1": 17.0034,
      "ARC-AGI-2": 19.7994,
      "Arena-Hard Auto": 26.2678,
      "BrowseComp": 29.5978,
      "BRUMO 2025": 9.9569,
      "Chatbot Arena Elo": 12.2994,
      "CMIMC 2025": 17.3784,
      "Codeforces Rating": 11.9616,
      "CritPt": 22.1159,
      "FrontierMath": 24.5911,
      "HLE (Humanity's Last Exam)": 15.4244,
      "HMMT Feb 2025": 8.4897,
      "HMMT Nov 2025": 8.095,
      "IFEval": 7.7274,
      "LiveBench": 12.1711,
      "MATH-500": 8.6609,
      "MathArena Apex 2025": 21.642,
      "MMLU": 9.1158,
      "MMLU-Pro": 8.4642,
      "MMMU": 21.174,
      "MMMU-Pro": 35.6682,
      "OSWorld": 20.0611,
      "SMT 2025": 10.3292,
      "SWE-bench Pro": 11.0081,
      "SWE-bench Verified": 16.3842,
      "Tau-Bench Retail": 12.3801,
      "Terminal-Bench 2.0": 11.6802,
      "Terminal-Bench 1.0": 24.1021
    },
    "evaluation_protocol": "Leave-one-model-out CV via hat-matrix on min-max normalized [0,100] scale",
    "n_predictor_benchmarks": 5,
    "achieves_mae_under_5": false
  },
  "canonical_evaluation": {
    "canonical_overall_mae": 14.7954,
    "canonical_coverage": 1.0,
    "n_predictions": 196,
    "n_heldout_pairs": 196
  },
  "methodology_notes": "Pipeline: (1) Extract 83x49 matrix from flat scores; 15 duplicate pairs averaged. (2) Filter to well-covered rows/cols (>=10/>=5), yielding 80x35. (3) Min-max normalize per benchmark to [0,100]. (4) SVD on mean-imputed centered matrix; effective rank ~2 by elbow (SV2/SV3 largest gap). (5) Greedy benchmark selection by correlation+coverage heuristic, top 5 chosen. (6) Ridge (alpha=10) for prediction, LOOCV via hat matrix. (7) Canonical eval: Ridge from 5 revealed benchmarks, rank-5 low-rank fallback. Scale heterogeneity (Elo ~1300-3000 vs percentages 0-100) handled by per-benchmark normalization. Key DOF: filtering thresholds (10/5), normalization (min-max), imputation (column mean), rank criterion (elbow vs 90%), Ridge alpha (10), selection heuristic weights (0.7/0.3)."
}