{
  "method": "Ridge regression (alpha=1.0) from 5 selected benchmarks to targets",
  "overall_mae": 10.27025491649533,
  "per_benchmark_mae": {
    "AA Intelligence Index": 10.568554271320577,
    "AIME 2024": 11.226542185016621,
    "AIME 2025": 8.12579793786493,
    "ARC-AGI-1": 16.610387709708547,
    "ARC-AGI-2": 10.879796679863565,
    "Arena-Hard Auto": 13.93823220137244,
    "BigCodeBench": 6.359696443582078,
    "BrowseComp": 6.220187839664073,
    "Chatbot Arena Elo": 17.2439707085731,
    "CMIMC 2025": 8.782993691380659,
    "Codeforces Rating": 67.47546288052379,
    "FrontierMath": 6.028457639591394,
    "GPQA Diamond": 5.284966025733283,
    "HLE (Humanity's Last Exam)": 5.5075913846769415,
    "HMMT Feb 2025": 10.404355079463269,
    "HMMT Nov 2025": 6.028581546959408,
    "HumanEval": 4.916943303071732,
    "IFBench": 11.971384128646482,
    "IFEval": 3.757980050530655,
    "IMO 2025": 38.980473561934865,
    "LiveBench": 4.289486580934144,
    "LiveCodeBench": 7.368288379943622,
    "MATH-500": 4.676886438142139,
    "MathArena Apex 2025": 8.261768179510915,
    "MMLU": 4.080488590243042,
    "MMLU-Pro": 5.605563741852189,
    "MMMU": 2.8255713484841443,
    "MMMU-Pro": 6.579851714705002,
    "OSWorld": 10.519498491572287,
    "SimpleBench": 14.627668748605256,
    "SimpleQA": 9.760420743542674,
    "SMT 2025": 3.6304198221820507,
    "SWE-bench Pro": 7.9941638807393165,
    "SWE-bench Verified": 7.975187127743313,
    "Tau-Bench Retail": 7.9515505244612745,
    "Tau-Bench Telecom": 14.861028469784348,
    "Terminal-Bench 2.0": 10.086918777903403,
    "Terminal-Bench 1.0": 12.70640199457471,
    "USAMO 2025": 10.26790918335702,
    "Video-MMU": 15.430481453987197
  },
  "evaluation_protocol": "5-fold cross-validation (LOO for benchmarks with <10 observed)",
  "n_predictor_benchmarks": 5,
  "achieves_mae_under_5": false
}