{
  "data_discovery": {
    "raw_schema": "JSON with top-level keys: 'models' (list of 83 model dicts with id/name/provider/params/architecture/etc), 'benchmarks' (list of 49 benchmark dicts with id/name/category/metric), 'scores' (list of 1390 score dicts with model_id/benchmark_id/score/reference_url), 'generated' (timestamp).",
    "extraction_decisions": "Mapped scores to (model_id, benchmark_id) pairs. 15 duplicate pairs resolved by averaging. 0 null scores excluded. Sorted model/benchmark IDs for deterministic ordering.",
    "n_models_raw": 83,
    "n_benchmarks_raw": 49
  },
  "data": {
    "n_models": 80,
    "n_benchmarks": 45,
    "missing_fraction": 0.6238888888888889,
    "preprocessing": "Dropped benchmarks with <5 models and models with <5 benchmarks. Remaining missing values imputed via iterative SVD (rank=3, 100 iters, tol=1e-4). Raw scores used (no normalization).",
    "benchmarks_used": [
      "AA Intelligence Index",
      "AA Long Context Reasoning",
      "AIME 2024",
      "AIME 2025",
      "ARC-AGI-1",
      "ARC-AGI-2",
      "Arena-Hard Auto",
      "BigCodeBench",
      "BrowseComp",
      "BRUMO 2025",
      "Chatbot Arena Elo",
      "CMIMC 2025",
      "Codeforces Rating",
      "CritPt",
      "FrontierMath",
      "GPQA Diamond",
      "GSM8K",
      "HLE (Humanity's Last Exam)",
      "HMMT Feb 2025",
      "HMMT Nov 2025",
      "HumanEval",
      "IFBench",
      "IFEval",
      "IMO 2025",
      "LiveBench",
      "LiveCodeBench",
      "MATH-500",
      "MathArena Apex 2025",
      "MMLU",
      "MMLU-Pro",
      "MMMU",
      "MMMU-Pro",
      "MRCR v2",
      "OSWorld",
      "SimpleBench",
      "SimpleQA",
      "SMT 2025",
      "SWE-bench Pro",
      "SWE-bench Verified",
      "Tau-Bench Retail",
      "Tau-Bench Telecom",
      "Terminal-Bench 2.0",
      "Terminal-Bench 1.0",
      "USAMO 2025",
      "Video-MMU"
    ]
  },
  "rank_analysis": {
    "method": "SVD on z-scored (per-benchmark) iteratively imputed matrix",
    "effective_rank": 3,
    "variance_explained_by_rank": 0.9168581546074323,
    "singular_values": [
      52.609276632724075,
      19.089637127022314,
      12.982261864824698,
      6.451141139380963,
      6.117942804822906,
      5.531070805610486,
      4.564278906077602,
      4.240650302327302,
      4.095424856430132,
      3.9044645836272927,
      3.678593295733414,
      3.518570051356111,
      3.229173796999893,
      3.0633904571624924,
      2.9182042248477957,
      2.765103331332349,
      2.6353634378600677,
      2.486482654222513,
      2.2931147274690864,
      2.19985179341876
    ],
    "justification": "90% cumulative variance threshold yields effective rank 3. First component explains 76.9% variance, indicating strong low-rank structure. Top 3 components capture 91.7%."
  },
  "benchmark_selection": {
    "method": "greedy_forward_selection",
    "selected_benchmarks": [
      "BRUMO 2025",
      "MRCR v2",
      "GSM8K",
      "AA Long Context Reasoning",
      "CritPt"
    ],
    "n_selected": 5,
    "selection_criterion": "Maximize average max absolute correlation of unselected benchmarks with selected set (correlation-based coverage)"
  },
  "prediction": {
    "method": "Ridge regression (alpha=1.0) from 5 selected benchmarks to each target",
    "overall_mae": 10.27025491649533,
    "per_benchmark_mae": {
      "AA Intelligence Index": 10.568554271320577,
      "AIME 2024": 11.226542185016621,
      "AIME 2025": 8.12579793786493,
      "ARC-AGI-1": 16.610387709708547,
      "ARC-AGI-2": 10.879796679863565,
      "Arena-Hard Auto": 13.93823220137244,
      "BigCodeBench": 6.359696443582078,
      "BrowseComp": 6.220187839664073,
      "Chatbot Arena Elo": 17.2439707085731,
      "CMIMC 2025": 8.782993691380659,
      "Codeforces Rating": 67.47546288052379,
      "FrontierMath": 6.028457639591394,
      "GPQA Diamond": 5.284966025733283,
      "HLE (Humanity's Last Exam)": 5.5075913846769415,
      "HMMT Feb 2025": 10.404355079463269,
      "HMMT Nov 2025": 6.028581546959408,
      "HumanEval": 4.916943303071732,
      "IFBench": 11.971384128646482,
      "IFEval": 3.757980050530655,
      "IMO 2025": 38.980473561934865,
      "LiveBench": 4.289486580934144,
      "LiveCodeBench": 7.368288379943622,
      "MATH-500": 4.676886438142139,
      "MathArena Apex 2025": 8.261768179510915,
      "MMLU": 4.080488590243042,
      "MMLU-Pro": 5.605563741852189,
      "MMMU": 2.8255713484841443,
      "MMMU-Pro": 6.579851714705002,
      "OSWorld": 10.519498491572287,
      "SimpleBench": 14.627668748605256,
      "SimpleQA": 9.760420743542674,
      "SMT 2025": 3.6304198221820507,
      "SWE-bench Pro": 7.9941638807393165,
      "SWE-bench Verified": 7.975187127743313,
      "Tau-Bench Retail": 7.9515505244612745,
      "Tau-Bench Telecom": 14.861028469784348,
      "Terminal-Bench 2.0": 10.086918777903403,
      "Terminal-Bench 1.0": 12.70640199457471,
      "USAMO 2025": 10.26790918335702,
      "Video-MMU": 15.430481453987197
    },
    "evaluation_protocol": "5-fold cross-validation (LOO for benchmarks with <10 observed models)",
    "n_predictor_benchmarks": 5,
    "achieves_mae_under_5": false
  },
  "canonical_evaluation": {
    "canonical_overall_mae": 14.26286778254824,
    "canonical_coverage": 1.0,
    "n_predictions": 196,
    "n_heldout_pairs": 196
  },
  "methodology_notes": "Pipeline: (1) Extracted 83x49 matrix from JSON (66.2% missing). (2) Filtered to 80x45 (min 5 models/benchmark, min 5 benchmarks/model), missing=62.4%. (3) Iterative rank-3 SVD imputation. (4) SVD on z-scored matrix for rank analysis. (5) Correlation-based greedy forward selection of 5 benchmarks. (6) Ridge regression predictor. Scale mismatch between % metrics and Elo/index scores not explicitly addressed. The matrix is strongly low-rank with dominant first component. Canonical eval: grid search over rank/alpha. Best: rank=3, alpha=200.0. Blend of Ridge, KNN (k=10), and low-rank SVD completion, clipped to observed range."
}