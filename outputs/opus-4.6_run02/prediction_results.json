{
  "method": "Ridge regression from selected subset to each target benchmark",
  "overall_mae": 10.19,
  "per_benchmark_mae": {
    "AIME 2024": 8.31,
    "AIME 2025": 8.79,
    "ARC-AGI-1": 15.48,
    "ARC-AGI-2": 9.99,
    "Arena-Hard Auto": 6.74,
    "BrowseComp": 18.96,
    "BRUMO 2025": 12.34,
    "Chatbot Arena Elo": 9.62,
    "CMIMC 2025": 10.65,
    "FrontierMath": 13.87,
    "GPQA Diamond": 7.47,
    "GSM8K": 4.69,
    "HLE (Humanity's Last Exam)": 9.51,
    "HMMT Feb 2025": 6.6,
    "HumanEval": 7.07,
    "IFEval": 6.57,
    "LiveBench": 53.67,
    "LiveCodeBench": 8.31,
    "MATH-500": 6.33,
    "MathArena Apex 2025": 28.45,
    "MMLU": 7.36,
    "MMLU-Pro": 7.06,
    "MMMU-Pro": 41.24,
    "MRCR v2": 38.35,
    "OSWorld": 7.39,
    "SimpleQA": 10.44,
    "SMT 2025": 9.81,
    "SWE-bench Pro": 23.83,
    "SWE-bench Verified": 8.33,
    "Tau-Bench Retail": 15.05,
    "Terminal-Bench 2.0": 7.56,
    "Terminal-Bench 1.0": 9.9,
    "Video-MMU": 16.25
  },
  "evaluation_protocol": "5-fold CV on observed entries of non-selected target benchmarks",
  "n_predictor_benchmarks": 7,
  "achieves_mae_under_5": false,
  "scale": "normalized 0-100 per benchmark"
}