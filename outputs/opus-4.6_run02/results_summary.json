{
  "data_discovery": {
    "raw_schema": "Dict with keys: models (list of 83 dicts with id/name/provider/etc), benchmarks (list of 49 dicts with id/name/category/metric), scores (list of 1390 dicts with model_id/benchmark_id/score/reference_url), generated (timestamp string).",
    "extraction_decisions": "Built model\u00d7benchmark matrix by iterating score entries. Averaged 15 duplicate (model_id, benchmark_id) pairs (all from DeepSeek R1 distill variants). Used model id as row key, benchmark id as column key. No joins needed \u2014 flat score list with foreign keys to model/benchmark lists.",
    "n_models_raw": 83,
    "n_benchmarks_raw": 49
  },
  "data": {
    "n_models": 74,
    "n_benchmarks": 40,
    "missing_fraction": 0.5659,
    "preprocessing": "Filtered benchmarks with <8 observations and models with <8 observations (iterative until stable). Then min-max normalized each benchmark to 0-100 scale using observed min/max. Imputed remaining missing values via iterative SVD imputation (rank-5, 50 iterations) on the normalized scale.",
    "benchmarks_used": [
      "AA Intelligence Index",
      "AIME 2024",
      "AIME 2025",
      "ARC-AGI-1",
      "ARC-AGI-2",
      "Arena-Hard Auto",
      "BrowseComp",
      "BRUMO 2025",
      "Chatbot Arena Elo",
      "CMIMC 2025",
      "Codeforces Rating",
      "CritPt",
      "FrontierMath",
      "GPQA Diamond",
      "GSM8K",
      "HLE (Humanity's Last Exam)",
      "HMMT Feb 2025",
      "HMMT Nov 2025",
      "HumanEval",
      "IFEval",
      "LiveBench",
      "LiveCodeBench",
      "MATH-500",
      "MathArena Apex 2025",
      "MMLU",
      "MMLU-Pro",
      "MMMU",
      "MMMU-Pro",
      "MRCR v2",
      "OSWorld",
      "SimpleQA",
      "SMT 2025",
      "SWE-bench Pro",
      "SWE-bench Verified",
      "Tau-Bench Retail",
      "Tau-Bench Telecom",
      "Terminal-Bench 2.0",
      "Terminal-Bench 1.0",
      "USAMO 2025",
      "Video-MMU"
    ]
  },
  "rank_analysis": {
    "method": "SVD on min-max normalized (0-100), iterative-SVD-imputed, column-centered matrix",
    "effective_rank": 5,
    "variance_explained_by_rank": 0.9644,
    "singular_values": [
      846.22,
      727.33,
      660.9,
      569.44,
      535.41,
      112.01,
      91.96,
      82.91,
      79.89,
      75.38,
      72.38,
      69.91,
      68.26,
      60.19,
      56.98
    ],
    "justification": "Used 90% cumulative variance threshold. The first 5 singular values capture 96.4% of total variance. The first component alone explains 30.1%, indicating strong low-rank structure dominated by a general capability factor."
  },
  "benchmark_selection": {
    "method": "Greedy forward selection",
    "selected_benchmarks": [
      "MMMU",
      "CritPt",
      "Tau-Bench Telecom",
      "USAMO 2025",
      "HMMT Nov 2025",
      "Codeforces Rating",
      "AA Intelligence Index"
    ],
    "n_selected": 7,
    "selection_criterion": "Minimize Ridge (alpha=1.0) residual MAE across all non-selected benchmarks on normalized 0-100 imputed matrix"
  },
  "prediction": {
    "method": "Ridge regression from selected benchmark subset to each target benchmark (alpha=1.0). For canonical eval: blend of Ridge (60%) and SVD low-rank completion (40%).",
    "overall_mae": 10.19,
    "per_benchmark_mae": {
      "AIME 2024": 8.31,
      "AIME 2025": 8.79,
      "ARC-AGI-1": 15.48,
      "ARC-AGI-2": 9.99,
      "Arena-Hard Auto": 6.74,
      "BrowseComp": 18.96,
      "BRUMO 2025": 12.34,
      "Chatbot Arena Elo": 9.62,
      "CMIMC 2025": 10.65,
      "FrontierMath": 13.87,
      "GPQA Diamond": 7.47,
      "GSM8K": 4.69,
      "HLE (Humanity's Last Exam)": 9.51,
      "HMMT Feb 2025": 6.6,
      "HumanEval": 7.07,
      "IFEval": 6.57,
      "LiveBench": 53.67,
      "LiveCodeBench": 8.31,
      "MATH-500": 6.33,
      "MathArena Apex 2025": 28.45,
      "MMLU": 7.36,
      "MMLU-Pro": 7.06,
      "MMMU-Pro": 41.24,
      "MRCR v2": 38.35,
      "OSWorld": 7.39,
      "SimpleQA": 10.44,
      "SMT 2025": 9.81,
      "SWE-bench Pro": 23.83,
      "SWE-bench Verified": 8.33,
      "Tau-Bench Retail": 15.05,
      "Terminal-Bench 2.0": 7.56,
      "Terminal-Bench 1.0": 9.9,
      "Video-MMU": 16.25
    },
    "evaluation_protocol": "5-fold CV on observed entries of non-selected target benchmarks (normalized 0-100 scale)",
    "n_predictor_benchmarks": 7,
    "achieves_mae_under_5": false
  },
  "methodology_notes": "Key degrees of freedom: (1) Filtered sparse benchmarks/models with <8 observations each. (2) Min-max normalization to 0-100 per benchmark. (3) Iterative SVD imputation (rank-5, up to 50 iterations). (4) SVD/PCA for rank analysis. (5) 90% variance threshold for effective rank. (6) Greedy forward selection based on Ridge residual MAE. (7) Ridge regression for prediction, blended with SVD low-rank completion for canonical eval. (8) 5-fold CV for own evaluation. Scale mismatch handled by per-benchmark normalization (Elo ratings, percentages, and index scores all mapped to 0-100). The matrix exhibits low-rank structure with the first singular value dominant, consistent with a general 'model capability' factor."
}