{
  "method": "Ridge regression from selected benchmark subset",
  "overall_mae": 7.943689174976547,
  "per_benchmark_mae": {
    "ARC-AGI-1": 10.1968696102298,
    "BrowseComp": 7.2917397375594195,
    "BRUMO 2025": 6.2491098010617305,
    "Chatbot Arena Elo": 8.8151411701181,
    "CMIMC 2025": 7.385233998811046,
    "Codeforces Rating": 10.806419269281516,
    "CritPt": 6.445953531789911,
    "FrontierMath": 9.360872924916924,
    "GPQA Diamond": 10.346079419907184,
    "GSM8K": 5.785282135054056,
    "HLE (Humanity's Last Exam)": 8.95016787758492,
    "HMMT Nov 2025": 3.904231633967709,
    "HumanEval": 9.393271112740678,
    "IFEval": 7.503202777250694,
    "LiveBench": 6.449497398465903,
    "LiveCodeBench": 9.949988273237519,
    "MATH-500": 7.050900211700565,
    "MathArena Apex 2025": 6.87781791962104,
    "MMLU": 7.218883949457284,
    "MMMU": 10.794101481447594,
    "MMMU-Pro": 4.651524103881418,
    "OSWorld": 8.281243462753242,
    "SimpleQA": 12.066007833654734,
    "SMT 2025": 6.800057625451418,
    "SWE-bench Pro": 8.4248549989742,
    "Tau-Bench Retail": 4.522582278632226,
    "Terminal-Bench 2.0": 7.737495853185001,
    "Terminal-Bench 1.0": 9.164766508607508
  },
  "evaluation_protocol": "Leave-one-model-out cross-validation on normalized (0-100) imputed matrix",
  "n_predictor_benchmarks": 7,
  "achieves_mae_under_5": false,
  "alpha": 1.0,
  "n_models_evaluated": 80,
  "n_target_benchmarks": 28,
  "scale": "0-100 per-benchmark min-max normalized"
}