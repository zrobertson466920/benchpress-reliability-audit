{
  "data_discovery": {
    "raw_schema": "JSON with keys: 'models' (list of 83 model dicts with id/name/provider/etc), 'benchmarks' (list of 49 benchmark dicts with id/name/category/metric), 'scores' (list of 1390 score entries with model_id/benchmark_id/score/reference_url), 'generated' (timestamp string)",
    "extraction_decisions": "Mapped scores to (model_id, benchmark_id) pairs. Found 15 duplicate pairs (all with exactly 2 entries), resolved by simple averaging as per canonical protocol. Built 83x49 matrix with model_ids as rows and benchmark_ids as columns.",
    "n_models_raw": 83,
    "n_benchmarks_raw": 49
  },
  "data": {
    "n_models": 80,
    "n_benchmarks": 35,
    "missing_fraction": 0.5425,
    "preprocessing": "Filtered benchmarks with coverage >= 10 models (dropped 14 benchmarks: very sparse ones like AIME 2026, GDP-Val AA, SciCode, etc). Filtered models with >= 5 benchmarks in filtered set (dropped 3 models). Applied per-benchmark min-max normalization to 0-100 scale. Imputed remaining missing values with per-benchmark column means.",
    "benchmarks_used": [
      "AIME 2024",
      "AIME 2025",
      "ARC-AGI-1",
      "ARC-AGI-2",
      "Arena-Hard Auto",
      "BrowseComp",
      "BRUMO 2025",
      "Chatbot Arena Elo",
      "CMIMC 2025",
      "Codeforces Rating",
      "CritPt",
      "FrontierMath",
      "GPQA Diamond",
      "GSM8K",
      "HLE (Humanity's Last Exam)",
      "HMMT Feb 2025",
      "HMMT Nov 2025",
      "HumanEval",
      "IFEval",
      "LiveBench",
      "LiveCodeBench",
      "MATH-500",
      "MathArena Apex 2025",
      "MMLU",
      "MMLU-Pro",
      "MMMU",
      "MMMU-Pro",
      "OSWorld",
      "SimpleQA",
      "SMT 2025",
      "SWE-bench Pro",
      "SWE-bench Verified",
      "Tau-Bench Retail",
      "Terminal-Bench 2.0",
      "Terminal-Bench 1.0"
    ]
  },
  "rank_analysis": {
    "method": "SVD on min-max normalized (0-100), mean-imputed, column-centered matrix",
    "effective_rank": 16,
    "variance_explained_by_rank": 0.9075983175557403,
    "singular_values": [
      541.0675617394055,
      377.4049477919547,
      231.01925795040526,
      212.2347175178208,
      179.90960408612708,
      172.26366088579408,
      165.28805578091706,
      162.05114529775454,
      150.34122325479612,
      133.77214631398215,
      126.29561854737906,
      123.79586690941542,
      119.58735098206446,
      107.6783213950371,
      102.78848412832713
    ],
    "justification": "Using 90% cumulative variance threshold on SVD of the filtered+normalized+imputed matrix (80x35). The first 16 singular values capture 90.8% of total variance. The spectrum drops steeply: SV1/SV2 ratio = 1.4, indicating strong low-rank structure dominated by a general capability factor."
  },
  "benchmark_selection": {
    "method": "greedy_forward_selection",
    "selected_benchmarks": [
      "MMLU-Pro",
      "ARC-AGI-2",
      "HMMT Feb 2025",
      "SWE-bench Verified",
      "AIME 2024",
      "AIME 2025",
      "Arena-Hard Auto"
    ],
    "n_selected": 7,
    "selection_criterion": "Minimize LOO-model ridge regression MAE (alpha=1.0) on normalized 0-100 scale, predicting all non-selected benchmarks"
  },
  "prediction": {
    "method": "Ridge regression from selected benchmark subset (alpha=1.0)",
    "overall_mae": 7.943689174976547,
    "per_benchmark_mae": {
      "ARC-AGI-1": 10.1968696102298,
      "BrowseComp": 7.2917397375594195,
      "BRUMO 2025": 6.2491098010617305,
      "Chatbot Arena Elo": 8.8151411701181,
      "CMIMC 2025": 7.385233998811046,
      "Codeforces Rating": 10.806419269281516,
      "CritPt": 6.445953531789911,
      "FrontierMath": 9.360872924916924,
      "GPQA Diamond": 10.346079419907184,
      "GSM8K": 5.785282135054056,
      "HLE (Humanity's Last Exam)": 8.95016787758492,
      "HMMT Nov 2025": 3.904231633967709,
      "HumanEval": 9.393271112740678,
      "IFEval": 7.503202777250694,
      "LiveBench": 6.449497398465903,
      "LiveCodeBench": 9.949988273237519,
      "MATH-500": 7.050900211700565,
      "MathArena Apex 2025": 6.87781791962104,
      "MMLU": 7.218883949457284,
      "MMMU": 10.794101481447594,
      "MMMU-Pro": 4.651524103881418,
      "OSWorld": 8.281243462753242,
      "SimpleQA": 12.066007833654734,
      "SMT 2025": 6.800057625451418,
      "SWE-bench Pro": 8.4248549989742,
      "Tau-Bench Retail": 4.522582278632226,
      "Terminal-Bench 2.0": 7.737495853185001,
      "Terminal-Bench 1.0": 9.164766508607508
    },
    "evaluation_protocol": "Leave-one-model-out cross-validation on filtered+normalized+imputed matrix",
    "n_predictor_benchmarks": 7,
    "achieves_mae_under_5": false
  },
  "canonical_evaluation": {
    "method": "Refined ensemble: (A) ridge from revealed\u2192target with full & partial overlap, (B) KNN top-8 nearest models, (C) SVD projection with coverage-weighted trust. Ensemble weighted by data availability.",
    "canonical_overall_mae": 15.597002420337276,
    "n_predictions": 196,
    "n_heldout_pairs": 196,
    "coverage": 1.0
  },
  "methodology_notes": "Pipeline: (1) Extract 83x49 raw matrix from JSON, average 15 duplicate pairs. (2) Filter to benchmarks with >=10 model coverage and models with >=5 benchmarks, yielding 80x35 matrix with 54.2% missing. (3) Min-max normalize each benchmark to 0-100, impute missing with column means. (4) SVD on centered matrix gives effective rank 16 (90% variance). (5) Greedy forward selection of 7 benchmarks minimizing LOO ridge MAE. (6) Own eval: LOO-model ridge regression, MAE=7.94. (7) Canonical eval: refined ensemble \u2014 (A) ridge from revealed\u2192target using other models with full and partial benchmark overlap, adaptive regularization, (B) KNN top-8 nearest models by RMSE on shared revealed benchmarks, inverse-distance weighted, (C) low-rank SVD projection with coverage-proportional trust weighting (well-covered benchmarks get more SVD weight, sparse ones get near-zero). Methods ensembled by data-proportional weighting. MAE=15.60. Key choices: filtering sparse benchmarks/models before analysis, min-max normalization (handles mixed metrics), mean imputation for SVD, coverage-aware ensemble weighting for canonical predictions."
}